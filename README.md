# AI-Builder-with-n8n-Create-Agents-Voice-Agents-Notes
This Repository contains my "AI Builder with n8n: Create Agents &amp; Voice Agents" Course Notes from Udemy

**I) Week 1 - Automate with Workflows in n8n Cloud**

**A) Day 1 - Build Your First AI Agent with n8n and OpenRouter (No-Code Tutorial)**

**B) Day 1 - Build AI Agents with n8n: Agentic AI Workflow Automation Framework**

**C) Day 1 - Build AI Agents with n8n: Complete Learning Roadmap & Workflow Setup**

**D) Day 1 - What is an AI Agent? Understanding Agentic Workflows in n8n**

**E) Day 1 - OpenAI API Setup: Cost Optimization and Free Alternatives Guide**

**F) Day 1 - How to Build an AI Agent with n8n and OpenAI API Integration**

**G) Day 2 - Understanding Agentic AI: How AI Agents Work with LLMs and Prompts**

**H) Day 2 - How LLMs Create Illusion of Memory and Reasoning Capabilities in AI**

**I) Day 2 - How Tool Calling Works in Agentic AI Systems and LLM Workflows**

**J) Day 2 - How to Evaluate AI Agents: Stop Anthropomorphizing LLMs in Workflows**

**K) Day 2 - How to Navigate n8n Cloud: Admin Panel, Instance, and Canvas Tutorial**

**L) Day 2 - How to Build AI Workflows with n8n: Nodes, Triggers, and Automation**

**M) Day 3 - How to Integrate Google Sheets and Google Drive with n8n Workflows**

**N) Day 3 - How to Build an AI Workflow in n8n with Google Drive Integration**

**O) Day 3 - How to Automate Stock Portfolio Tracker with n8n and Google Sheets**

**P) Day 3 - How to Build an AI Agent to Automatically Draft Gmail Replies in n8n**

**Q) Day 4 - Understanding JSON in n8n: Key-Value Pairs, Objects, and Arrays**

**R) Day 4 - n8n Authentication Methods: API Keys, OAuth2, and Workflow Integration**

**S) Day 4 - How to Integrate Pushover Notifications with n8n Workflows**

**T) Day 4 - Create Telegram Bot Using n8n: Complete AI Chatbot Integration Tutorial**

**U) Day 4 - How to Integrate Telegram Bot with n8n AI Agent Workflow Automation**

**V) Day 4 - How to Build a Slack Bot with n8n OAuth Integration and Automation**

**W) Day 4 - Connect Slack to n8n Using OAuth2 and Webhook Triggers Step-by-Step**

**X) Day 5 - n8n JSON Workflow Tutorial: Webhooks, Authentication & Integration**

**Y) Day 5 - n8n Node Types Explained: Core Nodes, Subnodes, and Cluster Nodes**

**Z) Day 5 - Build an AI-Powered Portfolio Rebalancer Using N8N and Google Sheets**

**AA) Day 5 - Agentic AI Workflow Automation: Balance Autonomy and Instructions**

**AB) Day 5 - How to Integrate Gmail and Pushover Notifications with n8n AI Agent**

**AC) Day 5 - n8n Workflow Automation: Using If Nodes for Conditional Logic**

**II) Section 2: Week 2: Accelerate With Voice Agents And RAG**

**A) Day 1 - How to Build AI Voice Agents with ElevenLabs Agent Platform**

**B) Day 1 - Build Your First AI Voice Agent with ElevenLabs Conversational AI**

**C) Day 1 - ElevenLabs Voice Agent Tools: Deploy Conversational AI with Widgets**

**D) Day 1 - Building Multi-Agent Workflows with ElevenLabs Voice AI Agents**

**E) Day 2 - ElevenLabs n8n Integration: Building Conversational AI Voice Agents**

**F) Day 2 - Automate Text-to-Speech Using ElevenLabs API in n8n Workflows**

**G) Day 2 - How to Build a Voice AI Agent with n8n Webhook and ElevenLabs API**

**H) Day 2 - Build an AI Voice Agent with n8n, ElevenLabs, and Gemini Workflow**

**I) Day 2 - How to Connect ElevenLabs AI Voice Agent to n8n Workflow with Webhooks**

**J) Day 2 - How to Connect ElevenLabs AI Voice Agent to n8n Using Webhooks**

**K) Day 3 - What is RAG in AI: Retrieval-Augmented Generation Explained**

**L) Day 3 - How Embedding Models Enable Semantic Search in RAG Systems**

**M) Day 3 - How RAG Works: Vector Databases and Semantic Search Explained**

**N) Day 3 - Agentic RAG vs Traditional RAG: Building Smarter AI Retrieval Systems**

**O) Day 4 - Building RAG Pipelines with Supabase Vector Database and Embeddings**

**P) Day 4 - How to Use n8n and Supabase Integration for AI-Powered RAG System**

**Q) Day 4 - Building RAG Pipeline with n8n and Supabase Vector Store Setup**

**R) Day 4 - How to Set Up Supabase Vector Database for RAG with n8n**

**S) Day 4 - Supabase Vector Store Integration with n8n Using OpenAI Embeddings**

**T) Day 4 - Build a RAG Pipeline Using n8n and Supabase Vector Database**



# **I) Week 1 - Automate with Workflows in n8n Cloud**

# **A) Day 1 - Build Your First AI Agent with n8n and OpenRouter (No-Code Tutorial)**

Each of my courses has something different going for it, and for this one, it’s all about being satisfying. This is the single most satisfying course I’ve ever made. Whether you’re technical or completely non-technical, in a very short amount of time I’m going to have you delivering real, tangible business value. Welcome to Gentle Co-Builder: Low-Code AI Agents and Voice Agents Using n8n. In just three weeks, you’ll be building AI agents that actually make an impact.

But first—stop right there. Normally we’d talk about objectives, introductions, logistics, and the curriculum. No, we won’t. You’ve taken my course before. You know I hate doing that at the beginning. Let’s get straight into action. Let’s build something cool. Let’s get some instant gratification. We can come back later to all the formalities. Let’s go.

First up, open your favorite web browser and go to openrouter.ai. If you already have something like an OpenAI API key, you can skip this step if you like. Otherwise, this is a great place to get access to free or paid AI models and route between different providers. If you don’t already have an OpenRouter account, click the Sign Up button and create one. You can sign up with email and password or use Google authentication, which is what I usually do.

Once you’re logged in, you may be asked a few onboarding questions. You’ll also be prompted to create an API key. Say yes, give it a name, don’t worry about usage limits, and then copy the key to your clipboard. Paste it somewhere safe so you can access it later. If that didn’t happen automatically, you can always click your avatar in the top right, go to Keys, and create a new API key there. You can make as many as you want. Just make sure you copy it correctly—if anything looks off, you can always generate another one. This key is what we’ll use to connect to AI from within n8n.

That’s step one. We’re halfway to instant gratification.

Now open a new browser window and go to the platform where we’ll be spending a lot of time together: n8n. If you don’t already have an account, click Get Started to begin the setup. Enter your name, email, password, and choose a screen name. You can opt in or out of the newsletter, then click Start 14-day free trial. If you’ve already used a free trial before, you may need the paid version, which is currently about $24 a month. Otherwise, you should be able to start the free trial.

When you first log in, you’ll answer a few basic questions about yourself and your company. You can say it’s just you, select a team like engineering, describe your company however you want, or skip anything you prefer. You might be asked if you want to invite teammates—feel free to skip that. Once your workspace is ready, you’ll see a screen that says Welcome… Create your first workflow. This is our home for the next three weeks.

Click Start from scratch or New workflow. If you already had an n8n account, you might click Create workflow instead. Either way, you’ll arrive at a blank canvas. This is the workflow editor where we’ll build automations visually, without writing code. Click Add first step. On the right, you’ll see a list of triggers—these are what start a workflow. Choose On Chat Message. Then either click Back to Canvas or simply press the Escape key to return.

Next, click the plus button to the right of the trigger. Select AI, then AI Agent. Again, return to the canvas. You’ll now see a large AI Agent node that you can drag and reposition. This is your first AI agent.

Inside the AI Agent node, click the plus under Chat Model. Search for OpenRouter and select OpenRouter Chat Model. Then choose Create New Credential. Paste in the API key you copied earlier from OpenRouter and save it. If it tests successfully, you’re good to go. If not, go back to OpenRouter, generate a new key, and try again.

Once the credential is saved, close that window. Now go to the Model dropdown. You’ll see many available models. Most cost money, but we’re going to choose a free one. Back on OpenRouter’s website, you can search for “free” to see available options. There are many, including versions of DeepSeek, Gemini, and OpenAI’s open models. We’ll use OpenAI’s open-source GPT model.

Back in n8n, type openai/gpt-20b:free into the model selector and choose it. Then return to the canvas.

Now click Open Chat. Type something simple like “Hi there.” You’ll see the workflow execute—spinning indicators, activity in the background—and then a response appears. You just connected to an AI model in the cloud using a visual workflow, without writing a single line of code.

Try another prompt, such as: “Please tell me a joke about AI automations.” Watch it run, and there’s your answer. Maybe not the best joke ever—but that’s not the point. You’ve just built your first AI-powered workflow.

This is the beginning. This is the first time you’ve created a workflow that calls an AI agent, connects to a model through OpenRouter, and runs entirely in the cloud—completely low-code and completely free.

Congratulations. That’s a little taste of instant gratification.

Now that we’ve built something, we can step back and start answering the big questions: what exactly is this course about, and what is n8n? Let’s go do that next.

# **B) Day 1 - Build AI Agents with n8n: Agentic AI Workflow Automation Framework**

Now, you might be thinking, “Ed, that wasn’t the biggest instant gratification, because I can do all that with ChatGPT. I can just ask it to tell me a joke.” And that’s true. But what we actually just did was build our own little version of ChatGPT in a couple of minutes. We designed a workflow with a chat interface, made a call to an AI, and you can already see how we could switch out different models. More importantly, you can see how we’re planting a seed that can grow into something much bigger. We’ll be able to add more AI, more services, and more integrations, allowing us to orchestrate increasingly complex workflows—all through this very simple interface. And it only takes minutes. This is just the beginning of something much larger.

So with that, let me welcome you properly again to the Agentic AI Builder. Prepare for building. This course is hands-on. It’s about creating. We’ve got so much to build and so much impact to prepare for. What we build is going to be valuable: high-impact, meaty deliverables that you could use in your own business, in products you already build, or directly for your clients. These are real ways to deliver impact. Whether you are technical or non-technical, this course will equip you to deliver value—and it’s also going to be a lot of fun. I can say that with confidence because I’ve already built every project we’re about to go through in the next three weeks, and I had a blast. You’re going to have a blast too.

Let me explain who this course is for. There are two main personas. First, if you’re primarily a business or product person—someone who delivers products—this course allows you to work at the frontier of what’s possible with generative AI, and especially agentic AI, which is where most of the excitement is right now. You’ll be able to build products with business impact without writing a line of code. If that’s you, this course is for you.

The second persona is the AI engineer. Maybe you already know how to write software. So why would this course be for you? Because it allows you to deliver substantial business functionality in minutes, and in a way that stays aligned with your business partners or clients—so much so that they can even take over what you build. If you’re doing a project or a gig for a client who needs automation, you can build it quickly and hand it off. The fact that you’re already technical only makes this more powerful. This course empowers both technical and non-technical people. It works on both levels.

And of course, most of you probably sit somewhere in between. Maybe you’ve never written a line of code in your life. Maybe you’ve done some coding years ago. Maybe you’re comfortable with complex Excel formulas—which is basically coding. Wherever you fall on that continuum, this course is for you. I’ve designed it so that there is something valuable for everyone, regardless of where you are on the technical spectrum.

So where will you be in three weeks? What do you get out of this course? By the end, you will be able to create AI agents and voice agents that solve real business problems. I’m not a believer in building agents just for the sake of it. There should always be a business problem you’re solving and a measurable outcome you’re driving. That’s what we will focus on. You’ll be able to apply for jobs that involve building AI agents, or submit proposals for projects that require agent-based solutions. You’ll also be equipped to use agents for automation, acceleration, and amplification—either within your own business or for your clients. That is what we will achieve by the end of three weeks.

Now, just to introduce myself and show you that I’m actually qualified to teach you this—apologies to anyone who has heard this before, because I do make the same jokes every time. I’m an LLM guy, not a comedian. My name is Ed Donner. I’m the co-founder and CTO of an AI startup called Nebula. I spent most of my career at JP Morgan, where I started as a software engineer and eventually became a managing director, running an organization of about 300 engineers and data scientists. I began in London, which is where I’m from originally, worked in Tokyo for a while, and now I’m based in New York City.

Before Nebula, I was the co-founder and CEO of an AI startup called Untapped, which was acquired a few years ago. This picture you see is from the Times Square billboard announcing the acquisition—a magical moment for me. And if you squint carefully enough, you can actually see me on that billboard. I don’t just show this to brag about my Times Square moment, but also because I live right there. I live about a block away, right behind the Hard Rock Café guitar. In fact, that’s where I’m coming to you from right now.

It’s also customary to show a hobby photo, so here’s one of me after flying a plane. You might think I’m showing it to say how skilled I am—but it’s the opposite. My greatest strength in artificial intelligence is only surpassed by my complete inability to do anything involving hand–eye coordination. So if you ever walk onto a plane and see me in the cockpit, look for a parachute immediately. But if you’re on a course about using AI to deliver business impact and I’m teaching it, then you’re in exactly the right place. You’re in safe hands.

As I’ve mentioned before, I tend to talk about AI and large language models to anyone who will listen. A few years ago, my friends and family convinced me to stop giving them impromptu lectures and instead make courses and videos online. I gave it a try—and it’s been an absolute joy. People seem to enjoy learning from someone who actually works with these systems in the real world, as a practitioner. Today, around 300,000 people from 190 countries have taken my AI courses, and I’m incredibly grateful that you’re part of that.

You might be wondering how this course fits in with the others I teach. Let me explain. My first set of courses is for aspiring AI engineers: the Core Track, the Agentic Track, and the MLOps Track. These are designed for people with at least some technical background, though many non-technical learners have succeeded with them as well. Then I have a course for AI leaders—founders, managers, and executives who want to understand how to build durable, differentiated AI businesses. I actually recommend that one for technical people too, because commercial thinking is a powerful skill for engineers.

And then there is this course. This one is for AI builders. You don’t need any technical experience. It’s low-code and no-code. It’s about using n8n to build agents and voice agents that deliver immediate business impact in a very short time. This isn’t primarily for AI engineers or AI leaders—it’s for people who want to build and deliver results.

If more than one of these courses appeals to you, that’s great. They’re designed to be taken in any order and to complement one another. Yes, there’s some repetition, but repetition is actually a good thing when you’re learning. So if something else sparks your interest, go for it. But for now—for today and for the next three weeks—we’re focused on one thing: building AI with n8n.

# **C) Day 1 - Build AI Agents with n8n: Complete Learning Roadmap & Workflow Setup**

Over the next three weeks, this is what I have in store for you. This is the curriculum ahead.

There are going to be three different types of sessions. First, there will be core sessions that focus on Agentic AI, on n8n, and on the core capabilities. Second, there will be sessions purely dedicated to integrations, because integrations are such a huge part of working with AI and delivering real business impact. That is a whole topic in itself. And third, there will be real-world projects, which will form the final focus area.

So how does this fit into the three weeks?

Each of the three weeks has a punchy title that describes what we are trying to achieve. The first week is called Automate. It is about using n8n to build workflows for your business. Automate is week one.

Week two is called Accelerate. This is next-level work. Here we start adding more power to your business or your clients’ businesses. We will work with voice agents and use RAG. You may not yet know what RAG is, or you may have heard of it because it is one of the most hyped ideas right now. We will properly dig into it in week two as we focus on acceleration.

That brings us to week three, which I call Amplify. This is where we really dial up your business, your clients’ businesses, or your product using multi-agent systems. We will use MCP—another popular buzzword—and we will look at what actually delivers impact and what may just be hype. The goal is to focus on real business value. By the end of week three, you will have automated, accelerated, and amplified your business.
Each of these three weeks has five days. In week one, the first two days focus on core skills. Then we move into integrations, and we finish with a business project that you are going to love.

In week two, we again start with a core skills day, followed by an integrations day, then another core day where we go deep into RAG. After that comes another integrations day, and we finish the week by delivering a voice agent.

Week three starts with core topics again, where we self-host n8n and use local models. Then we move into advanced integrations, followed by MCP and context engineering with sub-agents—hot topics right now in Agentic AI. Finally, we conclude with a capstone project designed to amplify your business.
After all of this, you will reach a celebration moment. You will get your certificate, your cup, your chalice, and you will be able to say: I am now an Agentic AI builder.
That covers the objectives and the course structure. Now it is time to actually get started.
We will begin with a little bit of theory, and then we will move back into action—back to n8n and back to building workflows. First, though, I want to make sure we are on the same page with some basic definitions and foundational concepts. Many of you may already be comfortable with this, but it is useful to align and perhaps add a few extra insights along the way.

We should start with a definition of an LLM. A large language model is the core data-science model at the heart of generative AI. It is an AI program that generates text. It produces output based on an input. That output might be an answer to a question, or it might represent an action that should be taken. If the input asks, “What should I do next?”, the model generates the most likely next text as its response.
We are all sometimes guilty of treating LLMs in a human-like way—attributing motivations, intentions, or even relationships to them. But it is important to stay grounded in reality. LLMs are extremely powerful statistical programs. They are large-scale pattern-matching systems trained on enormous amounts of data. During training, they learn patterns in language so that when given an input sequence of text, they can predict the most likely text that should come next.
If the input is a question, the model predicts a good answer. If it is a request for what to do next, it predicts actions. It can even predict which tools to call or continue a conversation. But behind all of this is statistics and pattern matching. That is the fundamental nature of a large language model.
There are also many techniques layered on top of LLMs to make them feel conversational. For example, what we often call “memory” is usually just a clever software trick that feeds previous messages back into the model so it appears to remember the conversation. In reality, the model itself is stateless. It does not retain memory between calls. It simply receives input and produces output each time.

What is astonishing is that, at the massive scale these models operate—trillions of parameters—this statistical process creates the appearance of intelligence. People often refer to this as emergent intelligence: complex behavior that arises purely from scale.
It is also important to understand the difference between GPT the model and ChatGPT the product. GPT is the large language model itself—the statistical engine that takes input and generates output. It is stateless and has no awareness of previous interactions.
ChatGPT, on the other hand, is a software product built by OpenAI that uses GPT behind the scenes. The product adds features such as memory, tools, web browsing, and a user interface. All of that functionality is implemented in software around the model. The model provides the intelligence, while the product delivers the experience.
Earlier, when we built something using OpenRouter, we were doing the same thing: creating a product on top of a model and adding chat-like functionality. Keeping this distinction in mind—the product versus the model—is essential as you start building your own systems.

If all of this already feels familiar, that is great. If not, don’t worry. We will cover these ideas repeatedly, and they will soon become second nature.
One last concept I want to touch on is the idea of an API. People talk about APIs all the time, and many are unsure what they actually are. At its core, an API is simply a way for different software applications to communicate with each other. It allows one system to send a request to another system and receive a response, using standard formats and protocols.

Most modern APIs work over the web using HTTP—the same technology used to load web pages. Instead of retrieving a web page, you call a specific URL, known as an endpoint, to tell another piece of software to perform an action or return data. The information exchanged is usually in a format called JSON (JavaScript Object Notation). You will become very familiar with JSON in this course—it is about as “low-code” as technical work gets.

Finally, most APIs require authentication. You usually identify yourself using an API key, which acts like a password. This key tells the service who you are and allows it to track usage or apply billing. When you make a request to a service like OpenRouter, you include your API key so it knows it is you and can process your request.
That is the foundation. From here, we move into building, integrating, and creating real-world agentic systems.

# **D) Day 1 - What is an AI Agent? Understanding Agentic Workflows in n8n**

Okay, we’ve got that out of the way. I know you probably already knew what an LM and an API were, but hopefully there was still something interesting in there.

But now let’s talk about AI agents. Even if you think you know what an AI agent is, there is so much confusion, disconnect, and disagreement about what the term actually means. There are even memes about how confusing it is.

So let me clear that up. An AI agent can honestly be whatever you want it to be. It’s one of those terms that people use very loosely these days. As long as it involves AI, you could probably call it an AI agent. However, most practitioners today do have something more specific in mind when they use the term. That’s what I want to explain now.

First, let’s talk about what people don’t necessarily mean by an AI agent. For a while, OpenAI and others used a definition along the lines of “AI systems that can do work for you independently.” This included tools like the OpenAI GPT agent (formerly the Operator agent), where you could ask something like, “Find me a Chinese restaurant in New York with availability for three people at 9 p.m. tonight,” and then watch it open a browser and carry out the task on your behalf. For a long time, that was considered an AI agent.

Then, towards the start of 2025, the industry began to converge on a slightly tighter definition, popularized by Anthropic in their blog post Building Effective Agents, along with work from Hugging Face and others. This view defined agents as systems where an LLM controls the workflow. In other words, instead of hard-coding “do A, then B, then C,” you allow the LLM to decide what happens next. You might prompt it with something like: “There are three possible next steps. Please tell me which one to take.” At that point, the LLM is orchestrating the workflow. This became the evolving definition of an AI agent.

More recently, towards the end of 2025, an even clearer definition has taken hold: an AI agent is an LLM that runs tools in a loop to achieve a goal. That means the model repeatedly decides what action to take, executes a tool, observes the result, and continues until the objective is reached. We will go much deeper into this terminology throughout the course.

You have already seen an example of this in n8n, where we used an AI agent node with tools connected to it. In practice, we can be a bit flexible. On this course, any workflow that includes that AI agent node can reasonably be called an AI agent. But strictly speaking, in the practitioner’s sense, it becomes truly “agentic” only when the agent is running tools in a loop to achieve a goal. And don’t worry—our main projects will absolutely meet that definition. We will be building true AI agents.

Now, there is one more definition I should give you: what exactly is n8n, which is what this whole course is built around? You probably already have a good idea, but let’s go a little deeper.

At its core, n8n is a workflow automation startup. It is similar in spirit to tools like Zapier, but it is newer and more flexible. It was founded in Berlin in 2019 and has grown rapidly into a multi-billion-dollar business with millions of users. What really sets it apart is how simple and streamlined it is, both for working with AI and for building integrations. It is incredibly easy to deliver real business value using n8n, whether you are technical or non-technical.

There are two main ways you can use n8n. The first is the cloud version, which is what we used earlier. In this case, you are using their hosted installation of n8n. It is free for a limited time, and after that you pay a subscription—around $24 per month in my case. The second option is to host it yourself. You can download the software and run it on your own computer or on your own server, and in that case it is free.

This brings us to the license. n8n is not exactly open source. It uses something called a “fair code” license, which is important to understand. Fair code means that you can use the software for free, download it, run it, and even modify it. You can use n8n for your own business without paying anything if you host it yourself. You can also build projects for clients and use n8n as part of those projects without owing anything to n8n.

So what can’t you do? You cannot use n8n to directly compete with n8n itself. You cannot resell the product, host a competing platform, or take the software, modify it slightly, and sell your own version. You also cannot sell the product to your clients. What you can sell are your services—your automation work, your workflows, and the solutions you build using n8n. That distinction is at the heart of the fair code license, and it is a very reasonable model.

To summarize: you can download n8n and run it for free, build solutions for your own business, and build and sell solutions for clients. What you cannot do is sell the n8n product itself. You can either self-host for free or use the cloud version with a subscription, starting at roughly $20–$24 per month depending on the plan.

There is also a third option called white-labeling. This is for companies that want to embed n8n inside their own product. That option comes with commercial terms, as you would expect, and requires contacting the n8n team directly through their website.

So hopefully now you not only know what n8n is, but also how you are allowed to use it and how it fits into real business use cases.

# **E) Day 1 - OpenAI API Setup: Cost Optimization and Free Alternatives Guide**

Okay, we’re about to go and do some final tinkering for today with N810, but I do want to set your expectations about APIs, costs, integrations, and all that good stuff that will shape the time we’re going to spend together.

I want to say that we’re going to do a ton of different integrations, and some of them come with costs. They are all optional, and you are always in control. First and foremost, you should decide what you want to spend money on and what you do not.

There’s no need to spend anything at all, but it is up to you to make that call as you go. I will give you alternatives, and you can do some research to understand what you would rather do.

If you want to spend a couple of dollars on something like OpenAI, then you should do so. If you don’t, you don’t need to do it. There will be plenty of alternatives, and you can always look for other integrations that are free. That can always be an alternative to what we do.

So please do keep that in mind.

N810’s cloud version, which we’re going to use in weeks one and two for simplicity because it’s so quick to get up and running, has a two-week free trial, at least for me right now. After that, it becomes $24 a month.

In week three, we’re going to move to self-hosting so that it’s free and runs on your own computer. It’s just slightly more technical, which is why I’m waiting until week three to get there.

If, however, you want to do that from the get-go, then you absolutely can. If you already know what you’re doing and you’re a bit technical, then just use the self-hosted version instead. You can check the documentation, which is perfectly clear.

I also want to say again that all of the integrations we use are optional. You can always switch things up for a different integration, and almost everything we do has a free tier. One of the exceptions is OpenAI.

OpenAI, which we’ll use in a second, is extremely cheap for most of what we’ll be doing. However, they do require a minimum upfront balance of $5, which you then use on a pay-as-you-go basis.

For some people, that’s annoying, and you don’t have to use it. You can always use OpenRouter as a drop-in replacement throughout this course. As you saw earlier, OpenRouter is free.

However, with OpenRouter’s free models, there are rate limits. That means there may be times when it throws an error and you’ll have to wait before trying again. That’s simply part of life on a free plan.

If that becomes too frustrating, then you might decide to spend the $5 and get an OpenAI balance.

If you can, I would suggest OpenAI because it is so widely used. Many clients, future clients, or employers will value the fact that you have OpenAI experience. It’s a good one to try out.

That said, any of the major providers are great. If you’d rather use Anthropic, which is my personal favorite, go for it. If you like Gemini, which also has a strong free tier when you go directly through Google AI Studio, you can absolutely use that too.

All of the models work in basically the same way in N810, and you can pick whichever one you feel most comfortable with. We’ll be setting up OpenAI next for those who want it, but you can also stick with OpenRouter.

So yes, keep OpenRouter in mind as your free alternative. You can use it again, pick any free model, or even try paid ones if you prefer. It’s also an easy way to experiment with different models.

And please don’t feel like you need to stick only to the integrations I’ll be covering in this course. One of the great things about N810 is how many integrations there are to choose from.

You should focus on learning the techniques: how to set up integrations in general, not just how to use one specific tool. Once you understand the pattern, you can apply it to different integrations, experiment on your own, and try new things.

The documentation for N810 is excellent. Every integration is well documented, so you should be able to take what we do and go in a different direction if you want.

That is the best way to learn. Don’t just do exactly what I do. First, make it work the same way. Then try taking it in a slightly different direction. Experiment with a different integration. Solve a different problem.

There are so many integrations available that you’ll be able to use this to solve all kinds of different problems. That’s the joy of it, and that’s also where real expertise is built.

Okay.

With that, let’s go back to it. Let’s go and do some experimenting.

First up, we’re going to set up an OpenAI account if you don’t already have one. This is completely optional if you’d rather stick with OpenRouter.

The website to go to is platform.openai.com.

This is not the same as ChatGPT. ChatGPT is a product with a free tier and a subscription option. That’s not what we’re doing here.

We want API access so that we can connect directly to the underlying model, the LLM that sits behind ChatGPT. We’re not using the product. We’re accessing the models themselves. That’s why we go to the OpenAI platform.

If you don’t already have an account, click Sign Up. You can continue using Google, Apple, Microsoft, or just an email address. I’m signing in with Google.

After that, it will ask you a few questions, like your date of birth. Once that’s done, you’ll be prompted to enter an organization name.

If you’re doing this just to build your skills, you can simply use your name or something like “Education.” Then choose whatever role fits you best and create the organization. You can invite your team later if you want.

You may see an option to set up your API key immediately. You can skip it for now by clicking “I’ll do this later.”

You’ll then arrive inside the OpenAI platform with your account set up.

I know I keep repeating this, but it’s important: we are now in the OpenAI platform, not in ChatGPT. This is where professionals connect to the underlying AI to build their own products. There is no monthly subscription here.

Instead, you pay a very small amount each time you make a request to the AI, usually fractions of a cent per call.

There are two things we need to do now: create an API key and add the $5 minimum balance. You may have done this during the welcome flow, but if not, we’ll do it now.

First, let’s set up the API key. Go to Settings, then click on API Keys.

You’ll see a button to create a new secret key. That is your API key—the credential N810 will use to identify you when it connects to OpenAI.

Click Create New Secret Key. Leave it owned by you, give it any name you like, choose the default project, and keep all permissions as they are. Then click Create Secret Key.

You’ll see your key appear. Copy it to your clipboard and store it somewhere safe, such as a simple text file or password manager. Avoid fancy editors that might change characters.

Keep it copied for now so that you can paste it into N810 shortly. Then click Done.

If you ever have trouble connecting to OpenAI, you can always come back here, revoke the old key, and create a new one. You can create as many keys as you need.

That’s your API key. Keep it safe.

If you want to use OpenAI, you also need to add the $5 minimum balance. To do this, go back to Settings, then click on Billing.

Click Add Payment Details. This does not affect your ChatGPT account—this is only for the API.

Here, you’ll enter your credit card and add the $5 balance. Make sure auto-reload is turned off. Auto-reload would automatically add more money when your balance runs out, which you don’t want while you’re just getting started.

Once you’ve added the balance, you can check Billing History to confirm that the funds are there. In some countries, it can take a little time for the payment to clear, so be patient if it doesn’t work immediately.

You can also visit the Usage page to track how much you’re spending. You’ll likely see that you’re spending very slowly, because API usage is extremely cheap unless you scale to many users.

For good API hygiene, keep this usage page bookmarked. Check it occasionally, and only top up your account with an amount you’re comfortable with.

The worst-case scenario is that you spend the amount you’ve added, so keep that amount small and always keep an eye on your usage.

Okay.

At this point, you’ve set up your OpenAI account, created your API key, and added your $5 minimum balance.

We are now ready to go back to N810 and build an AI agent that connects to OpenAI and does real work.

# **F) Day 1 - How to Build an AI Agent with n8n and OpenAI API Integration**

All right. We’re back here in NH and I’m going to press sign in to come back in as the account that we just created a few moments ago. This comes up. That was the name that I gave my instance. I click open instance and here it is. This is looking now at the home screen. If you’re still on the previous screen we were on before with that workflow, you can just press this home button right here and you get to this screen, the home screen for this instance. And you can see here that it has my workflow, the thing we created earlier, sitting there.

We’re going to start again and create one from scratch like we did before and use OpenAI this time. So follow along with me here, and tomorrow we’ll explain what all the different terminology means. We first press Create Workflow right here and we get again this fresh screen, a new screen for us. We’ll add a first step, and again we are going to add this thing here on chat message on the right. Click there and then remember what you do next: you press escape, or you click that back button at the top left.

So here we are. Now we are going to press this plus button. You remember what we do next. We click on AI, and next we do AI agent. And here is our AI agent. And next it’s either back to canvas or it’s escape. Here we are back on this canvas. The chat model is the first thing for us to select. You may remember last time we did Open Router at this point, and you may do that again should you wish. But I’m going to this time do Chat OpenAI by typing OpenAI, and there we see the OpenAI chat model.

If you’re starting for the first time, you may see an option to get 100 free OpenAI API credits. I don’t know if you see that too, but I’m going to ignore that for now and just use the credits that we just set up. Now I’m going to click here and say Create New Credential, just as we did with Open Router. I paste in the API key that I still have in my clipboard, press save, and it works: credential successfully applied, testing successful. If it didn’t work for you, just go back, create a new API key, copy it, and paste it straight in. Don’t go through another editor — they can cause trouble.

We are now set up to be using OpenAI. In this box right here we’re going to keep it as “from list.” This is the list of all the models available on OpenAI. As of right now, the fanciest one is GPT-5.1, but we’re going to stick with GPT-4.1 Mini because it’s really fast, really cheap, and great to start with. There’s an even faster, cheaper one called GPT-4.1 nano, but we’ll use mini for now.

Now we go back and try it out. We click in the chat section and say “hi there.” Off it goes, it’s thinking, processing, and we get an answer back: “Hello, how can I assist you today?” We just connected to OpenAI. Now I say, “Hi, my name is Ed.” It replies nicely. Then I ask, “Hey, what’s my name?” And it says it doesn’t know. This is the key learning moment. The LLM itself is stateless — every call is new. The illusion of memory in products like ChatGPT comes from software built around the model.

So we add memory. We click just below Memory, choose Simple Memory Stores, no credentials required, and go back. The yellow box means something changed, so we reset the chat session. Now we repeat: “Hi there.” “My name’s Ed.” Then we ask, “What’s my name?” And it answers correctly. That’s because we’ve given it conversational memory. We’re building pieces of what ChatGPT does by connecting boxes and lines instead of writing code.

Now it’s time to add a tool. Open a browser and go to MarketStack. Sign up for a free account and copy your API key. Back in NAS, clear the chat, press the plus button under tools, search for MarketStack, create new credentials, paste in the API key, and save. Then configure the tool: leave resource as end of day data, operation as get many, set the ticker to be defined automatically by the model, leave limit at 50, and add the filter latest. Press escape, and the red warning symbol disappears.

Now we have a model, memory, and a tool. Let’s test it. I say, “Please tell me the end of day equity price for Google.” Watch what happens — the agent uses the tool. And it replies: “The end of day market price for Google on the 8th of December was 313.72.” We just had a conversation with a model that could look up stock prices using a tool. The agent connected to OpenAI, used memory to maintain the conversation, and queried MarketStack.

This is our staging ground. From this simple beginning, we’ll build more sophisticated agents and real business workflows in the coming weeks. Congratulations on building your first workflow in N810 — simple, but a stepping stone to much greater things.

You also get something extra with this course: you get me. I’m available to help you if you get stuck. You can reach me on Udemy or LinkedIn. I love connecting, so by all means, reach out. It’s always me responding, even if I sometimes use copy and paste. Ask questions anytime — that’s what I’m here for.

As you build projects with N810, post about them on LinkedIn. Share screenshots, tag me, and I’ll weigh in to help amplify your work. If you see others from the course posting and we’re connected, please like and comment to support the community. That’s part of the program.

And with that, we’ve completed the first day. One day down, fourteen to go. You’re already 7% of the way through this program. Many congratulations. I can’t wait to see you tomorrow, when we’ll go deeper into the theory of N810 and agents. I’ll see you then.

# **G) Day 2 - Understanding Agentic AI: How AI Agents Work with LLMs and Prompts**

Well, this is good news. You’ve decided to come back for more. You weren’t completely put off by what we did yesterday, and now you’re back again. Welcome. Welcome to week one, day two.

I want to be upfront with you about how I like to teach. I’m very much a learning-by-doing type of person. The way I like to teach is the same way I like to learn myself—by jumping into a product and actually using it. Today, however, is a bit of an exception. This session is going to involve more talking than usual. I’ll be giving you some foundational concepts and background that will support everything that’s coming later. We’ll be talking about Agentic AI and some of the constructs around n8n, although there will still be a bit of hands-on work, so don’t worry. Let’s get started.

Let me tell you more about Agentic AI. We’ll begin by diving into the topic of AI agents, before circling back to n8n again. This will be a quick recap of some of the things we already discussed yesterday. There are many different ways to define an AI agent, so many that it’s almost become a joke. One of the earlier definitions, used by companies like OpenAI, described agents as systems that can work independently for you, such as the Operator agent or the ChatGPT agent.

Later, another definition emerged. This one focused on AI systems where a large language model decides what to do next. In this setup, the LLM generates content that effectively describes a plan, such as “do A, then B, then C,” or perhaps “do A, then C, then B.” That generated text becomes the output of the LLM, and we interpret it as instructions for what should happen next. This idea—where an LLM decides the order of activities and orchestrates a workflow—became a common working definition of an AI agent.

More recently, the definition has evolved again. Now, an agent is often described as an LLM that runs tools in a loop in order to achieve a specific goal. That looping behavior is a key part of what we now think of as Agentic AI.

A lot of Agentic AI feels magical, but behind that magic, the reality is actually quite pedestrian. Most of what feels impressive in AI comes down to writing good prompts for large language models. A prompt is simply the input text that we send to an LLM. The LLM itself is a data science artifact—a statistical model—that takes an input and generates an output by predicting what is most likely to come next.

Every time you call an LLM, that call is stateless. It takes the input, generates an output, and then forgets about it. It doesn’t know how it was called before, and it’s being called by countless people around the world all the time. Models like GPT receive an input sequence and produce an output sequence based entirely on that input. What we control are two things: what input we send and how we interpret the output. Most of Agentic AI is about mastering these two activities—getting the input right and making sense of the output.

Typically, the input sent to an LLM is made up of several parts. It often starts with something called the system prompt. This is the text that sets the overall context. It defines the role the LLM is playing, provides background information, and specifies the tone or style of the response. After that usually comes the user prompt, which is the specific message the LLM is meant to respond to.

In many cases, the full conversation history is also included in the input. This might include an initial user message like “Hi there,” the model’s response, the next user message, another response, and finally the current prompt the model is responding to. All of this can be included in a single input sequence. This works well because LLMs were trained on data organized in exactly this way—system prompts followed by user and assistant messages—so they’re very good at generating coherent outputs that follow this structure.

The key point is that we get to decide how this input data is organized. We can apply all sorts of techniques to make it more likely that the LLM’s output aligns with our business objectives. By structuring the input carefully, we can guide the model toward producing the kind of output we want. And once we have that output, we can choose how to interpret it.

For example, the input might ask the model to respond with the order in which certain steps should be executed. The output might then list those steps in sequence, and we can interpret that output as instructions to control a workflow. In this way, the text generated by the LLM is effectively driving a process. This combination of carefully designed input and intentional interpretation of output is the real secret sauce behind Agentic AI.

In the past, this was often referred to as prompt engineering, which has become a bit of an outdated term. Today, people increasingly talk about context engineering instead. This focuses on how to best set up the context sent to the LLM so that it’s positioned to achieve a specific business goal.

Most of what feels like magic in Agentic AI can be explained by five core tricks. These are essentially five conjuring techniques centered around prompting and, to a lesser extent, how LLMs are trained. Alongside these five tricks, there’s also a common pitfall—a trap that people often fall into when building Agentic AI systems.

The five tricks we’ll cover include the illusion of memory, which many of you may already be familiar with; thinking and reasoning, including the idea of reasoning budgets; chaining LLMs together; the use of tools—what that really means and what it doesn’t; and finally, the famous agent loop. In addition to these, we’ll discuss the trap I call the human trap. You might not yet know what that means, but we’ll get to it.

# **H) Day 2 - How LLMs Create Illusion of Memory and Reasoning Capabilities in AI**

Okay, let’s talk about five tricks and one trap. We’ll start with trick number one: the illusion of memory.

This is something you’ve basically already seen before, but it’s always worth taking a minute to explain it clearly. When we make a prompt to a language model—say something like, “My name is Ed”—the model’s job is simply to complete that prompt with the most likely text that comes next. As you know, it’s really generating tokens, which are small fragments of text. In this case, it responds with something like, “Hi, Ed.”

Now suppose we give it a completely separate prompt: “What’s my name?” Every time you call a language model—and I’ve explained this many times—it is stateless. This is GPT the model, the language model itself, not ChatGPT the product. When you make a call to GPT, it takes an input sequence—“What’s my name?”—and predicts the most likely text that should come after it based on its training data. And of course, in that case, it will say something like “I don’t know your name”, or something a bit more charming in typical GPT style.

So the illusion of memory comes from a very simple trick. When you prompt an LLM and say, “My name is Ed”, and it responds, “Hi, Ed”, then later the user asks, “What’s my name?”, you do not send the LLM just the prompt “What’s my name?” Instead—and you probably already know this—you send it the entire conversation so far.

That means what the model actually sees is something like:

“My name is Ed.
Hi, Ed.
What’s my name?”

What the model is doing now is predicting what comes next. In its training data, it has seen countless examples that follow this pattern of message, response, message, response. So it expects something like that. When it generates the next tokens—the next bit of text at the end—it produces something consistent with the full conversation. That’s why it responds, “Your name is Ed.”

This is also why every time you talk to ChatGPT through the UI, it sends the underlying GPT model the entire conversation history each time you press Enter. The model then generates the next tokens based on that full history. This trick of sending the entire conversation creates the illusion that the model remembers what you said 30 seconds ago. It doesn’t. The information is simply included again in the prompt every single time. Many of you already knew this, but it never hurts to go through it again. That’s how trick number one works.

Now let’s move on to trick number two, which is arguably less about agentic AI specifically and more about reasoning and general thinking. This idea started about a year and a half ago when people discovered something that was called chain of thought. The idea was that you could ask an LLM a question and then simply add something like, “Please think step by step,” and you would often get better results just by doing that.

There seemed to be something strange going on. If an LLM generates text that describes what it should do, and then generates text to actually do it, you often get better outcomes. Nothing is actually thinking in a human sense—it’s still just generating likely text—but as a side effect, by generating text that describes a thought process, the model ends up producing better answers.

This led to the idea of thinking models or reasoning models—they mean the same thing. These are models that have been trained so that when they’re given a question, they first generate some text describing how they’ll reason through the problem step by step, and only then generate the final answer. That’s why, with some models, you see what’s called a thinking trace before the final output.

To give you a concrete example—one I use in some of my technical courses—you can prompt a small model that is not in reasoning mode, such as GPT-4.1 nano, and ask something like: “You toss two coins. One is heads. What’s the chance the other one is tails?” Quite often, the model will give the wrong answer, typically something like 50%. That answer is incorrect because this is one of those sneaky questions with a bit of trickery built into it.

If, however, you prompt it differently—“You toss two coins. One is heads. What’s the chance the other one is tails? First describe your thought process, then give the answer.”—or if you use a reasoning model that’s been trained to output its reasoning first, you’ll often see a thought process like: “Okay, this is probably a trick question. They didn’t say the left coin is heads and the right coin is tails. I need to consider all possible outcomes.” It will then reason through the possibilities and arrive at the correct answer, which is two thirds.

You can try these experiments yourself. Depending on the model you choose and how you prompt it, you can reliably reproduce this behavior: a small model without reasoning often gets the wrong answer, while the same model—or a similar one—with reasoning enabled produces the correct result. This reasoning trick is a powerful way to extract more sophisticated intelligence from a model that is, at its core, simply generating the most likely text to follow an input.

One more important detail here is how these models actually generate text. They don’t produce an entire output all at once. Instead, they generate one token at a time. Given an input, the model predicts the most likely next token. That token is then appended to the input, and the entire sequence is fed back into the model to generate the next token. This repeats until the output is complete. This process is called inference.

Because of this, if you ask a model to generate tokens that describe a thought process, it will do exactly that first. Then, when it generates the final answer, that answer will be consistent with the reasoning it already produced. This often leads to better outcomes. It sounds counterintuitive and almost too good to be true, but it works, and it’s been shown empirically to work.

Some models are trained purely for chat. They respond immediately without explicit reasoning. Other variants of the same model family are trained for reasoning, where they generate reasoning text first and then an answer. There are also hybrid models that can do both. People often gravitate toward reasoning models because they perform better on benchmarks and appear more intelligent. However, chat models can be better for certain use cases—especially in agentic AI, where you’re already driving the system step by step.

So don’t assume that a reasoning model is always better. Sometimes a chat model will perform better. The only real way to know is to try both. This entire field is highly experimental. There are no absolute right or wrong answers—only experimentation, measurement, and choosing what works best for your use case.

Finally, some models allow you to control how long they should think. This is sometimes called reasoning effort or a thinking budget. In some models, such as the latest GPT-5.1, you can set this to none (chat mode), minimal, low, medium, or high. You might wonder how this actually works under the hood—how you tell a model that’s just generating tokens to “think more.”

There are several techniques, but the most common one is surprisingly hacky. During inference, as the model generates tokens one at a time, you don’t have to feed back only the tokens it generated. You can insert additional tokens of your own. The model has no idea that it didn’t generate them—it just treats them as part of the input and continues generating text.

Someone realized that if, during the reasoning phase, you wait until the model finishes a sentence—something that looks like a completed thought—and then insert the word “wait”, the model now has to generate text that follows that word. Because the next tokens must be coherent with “wait”, the model often steps back and reconsiders. It might say things like, “Wait, I should double-check my assumptions,” or “Wait, let me review the problem again.”

This causes the model to revisit and challenge its own reasoning. Adding words like “wait”, “on the other hand”, or similar phrases encourages it to explore new reasoning paths. If you’ve ever looked closely at reasoning traces in LLM outputs, you may have seen the word “wait” appear—that’s often this trick at work.

This incredibly simple technique works remarkably well and is one of the core methods used to control a model’s thinking budget and force deeper reasoning before it produces a final answer.

# **I) Day 2 - How Tool Calling Works in Agentic AI Systems and LLM Workflows**

And this may be more theory than you thought you were signing up for, but it’s going to be genuinely useful. This theory will give you valuable intuition when we actually get to building agent systems, rather than just talking about them abstractly.

Now let’s move on to the third trick, which is a simple one: chaining LLMs.

You probably already know this idea. You can write a complicated prompt to a language model, such as: “Come up with a puzzle and then solve it.” Sometimes this is a good thing to do because it gives the LLM a lot of flexibility. We often use the word autonomy here. It gives the model the freedom to go in different directions—to come up with two puzzles and solve them both if it wants to. You’re giving it a broad remit.

However, sometimes you don’t want that flexibility. Sometimes you want exactly one puzzle and exactly one solution. You might want to be very careful to ensure that the puzzle is hard, that it meets certain constraints, and that it’s framed properly. In cases like that, it can be much better to divide the work into two separate LLM calls.

In the first LLM call, you ask it only to come up with a puzzle. You might refine that prompt carefully, adding details and specifics so the model becomes very good at generating the kind of puzzle you want. Then, once it responds with the puzzle, you build a second prompt and make another LLM call. In that second prompt, you say something like: “I’d like you to solve a puzzle. Here is the puzzle.” And you simply include the puzzle generated by the first call.

When we draw this graphically, we often show it as if one LLM is calling another LLM. But of course, that’s not really what’s happening. What’s actually happening is much simpler: you’re making one LLM call, taking the output, and inserting it into the next prompt. Conceptually, though, it helps to think of it as two workflow steps.

This pattern—breaking a task into multiple LLM calls and passing outputs forward—is what we call chaining LLMs. It’s a very obvious idea, but it’s incredibly useful. It gives you more control, and it lets you test each step independently.

Now let’s move on to the fourth trick, everyone’s favorite: tools.

Tools feel incredibly magical, but in reality, they are extremely mundane. And again, this may be something you already know, but it’s worth emphasizing.

Suppose a user asks a question like: “What’s the stock price of Google?” You have some software—maybe custom code, maybe a product like an agent platform—that receives this question. That software makes a call to a large language model that has been “equipped with tools.” In other words, it’s been told that it has the ability to look up stock prices using some external service, such as MarketStack.

The LLM appears to connect to the internet, make an API request, retrieve Google’s stock price, and then respond with something like: “The price of Google stock is X.” This feels magical. But we know that LLMs don’t actually do that. They generate output tokens by pattern matching.

So the question is: at what point did the LLM stop generating tokens and decide to connect to the internet instead? How did it suddenly gain this power?

The answer, of course, is that it didn’t. Nothing like that happened.

What actually happens is far more mundane. When your code builds the prompt that it sends to the LLM, it doesn’t just say “What’s the stock price of Google?” Instead, it says something more like this:

“You can do one of two things.
You can either answer the user directly, or you can respond by saying that I need to run a specific tool on your behalf. If you choose the tool, I will call you again with the results.”

The tool available is something like “Look up stock prices.”
The user’s question is “What’s the stock price of Google?”

The LLM receives this carefully constructed input sequence. The output it generates says something like: “Use tool to look up the stock price of Google.” Your system then interprets that output, actually calls the tool, retrieves the stock price, and sends the LLM a second prompt that includes the tool result. The LLM then responds with the final answer.

So once again, this is just a clever conjuring trick. By shaping the input, interpreting the output, running external code, and calling the LLM again, we create the illusion that the model itself is running tools. But as with so much in LLM systems, it all comes down to clever prompting.

To really drive this home, there’s a simple experiment you can try in ChatGPT. You can give it a prompt like this:

“You are a support agent for an airline. You answer user questions.
Only use a tool to fetch ticket prices.
The tool retrieves ticket prices for London or any city.

Here’s the user’s question:
‘I’d like to go to Paris. How much is a flight?’”

If you do this, ChatGPT will respond with something like: “Use tool to fetch ticket price for Paris.” That’s it. It won’t invent a price. That response alone shows you exactly how tool calling works. It’s just pattern matching based on the prompt. You can try this yourself to see it firsthand.

Now let’s move on to the fifth trick, which is the idea of an agent loop.

An agent loop is the concept of calling an LLM repeatedly, allowing it to use tools over and over again until it has achieved a goal and is finished. This repeated cycle creates the behavior where the system appears to go off and do work autonomously.

Here’s an example.

Suppose you give an LLM the following task: “Your task is to find the current value of my portfolio.” You tell it that it has two tools available. Tool one retrieves the portfolio. Tool two looks up share prices.

Given this prompt, what is the most likely next thing the LLM will output? The obvious answer is: “Use tool to retrieve portfolio.” That makes complete sense.

So you take that output and feed it back into the next prompt. You include the same original instructions, add “Use tool to retrieve portfolio”, and then add the actual result of doing that. Suppose the result is: “Three shares of Google stock.”

Now you send this entire prompt back to the LLM. The most likely completion now is: “Use tool to look up share price of Google.” Again, that makes perfect sense.

You take that output, run the tool, retrieve the share price, and build a new prompt that includes everything so far: retrieving the portfolio, the result of three Google shares, looking up the share price, and the actual price—say, $100 per share.

You send all of that in a single prompt. Remember, every LLM call is stateless, so the entire conversation so far must be included. Now when you call the model again, it responds: “The value of your portfolio is $300.”

It doesn’t call any more tools. It’s done.

And that—calling an LLM in a loop, interpreting its outputs, running tools, and feeding results back in until a goal is achieved—is agentic AI.

# **J) Day 2 - How to Evaluate AI Agents: Stop Anthropomorphizing LLMs in Workflows**

There you have it. Those are the five tricks that allow us to make calls to LLMs with structured inputs and to interpret their outputs in a way that gives the impression that something autonomous is carrying out tasks for us.

Don’t worry if you didn’t fully grasp everything yet. We’re going to revisit these ideas again and again, and over time they will connect naturally. For now, the goal is simply to build some intuition around how these systems work.

So, what about the trap? I mentioned that there were five tricks and one trap, and this trap is something I personally find quite frustrating because it happens so often. I want to warn you about it and get you thinking about it early. I call it the human trap. The more formal term for this is anthropomorphizing.

Anthropomorphizing is the tendency to treat generative AI and LLMs as if they were humans, assigning them roles and responsibilities in the same way we would assign jobs to people. This happens constantly, especially when business leaders want to automate a process, something we’ll be doing a lot throughout this course.

The typical instinct—shared by business users and even experienced engineers—is to create an “agent architecture.” This usually takes the form of a diagram with different agents connected by lines, where each agent is given a role based on how humans would organize the work. You might have agents that represent different jobs, such as researchers, evaluators, or decision-makers.

I’m guilty of this myself. In some of my courses on generative engineering, we build systems like a trading floor with traders and researchers. It’s a very natural way to think, and for toy projects or demos, it’s perfectly fine. It’s fun and visually intuitive. However, it’s not a disciplined or reliable way to design real systems, and it comes with serious problems.

The biggest problem is forgetting what LLMs are actually good at. LLMs are trained to generate realistic and compelling content. That’s their strength. If you tell an LLM, “You are an evaluation agent. Evaluate the previous output and give it a score out of ten, along with a justification,” it will do exactly that.

It will generate a score. It will generate a justification. It will sound confident and reasonable. But that doesn’t mean the evaluation is correct. It doesn’t mean it’s aligned with your true objectives. It simply means the model is following the instructions in the prompt and producing plausible text.

The real danger is that you can fool yourself into thinking you have a sophisticated system: a whole group of agents collaborating, each fulfilling its role and producing polished outputs. In reality, it may all be LLM-generated slop—content that looks meaningful but isn’t actually solving your problem in an accurate or reliable way.

So what’s the right approach? When you divide a problem into multiple agents, you should do it because it genuinely improves performance, not because the roles sound sensible or mirror how humans would work. If you have a complex problem, it may make sense to split it into steps, but you should test that assumption.

You try a simpler approach first. Then you introduce a division of labor. If the results improve, you keep it. If they don’t, you discard it. This is how you should approach agent design—scientifically, through experimentation.

The most important word here is evaluation. You must have a way to measure outcomes. You should reorganize agents or break tasks into smaller steps only when it leads to better evaluations and demonstrably superior performance. That’s the correct justification, not the appearance of having well-defined responsibilities.

Starting with human analogies can be a reasonable first step. After all, human organizations evolved for a reason. But that should only ever be a starting point. Always begin as simply as possible. Start with a single role, then gradually add complexity, experimenting as you go and measuring whether it actually helps.

That’s the right way to work. That’s how you avoid the human trap.

And with that, we wrap up the theory behind Agentic AI. I hope this has given you solid intuition as we move into building agentic workflows, along with some real-world lessons learned from deploying these systems in practice.

Now it’s time to return to N810, starting with navigation and the big-picture building blocks of the platform.

# **K) Day 2 - How to Navigate n8n Cloud: Admin Panel, Instance, and Canvas Tutorial**

As I explained last time, there are two different modes you can use with n8n. The first is n8n Cloud, where the company manages the installation and you connect to it remotely. The second is self-hosted, where you run the code yourself—either on your own computer, a server, or another environment. We’ll get to self-hosting later.

For now, we’re using Cloud, simply because it’s the fastest way to get up and running. I also mentioned a white-label mode earlier, but that’s a bit different and not what we’re focusing on here. So for now, cloud is what we’re using.

There’s one big-picture concept I need to explain next: the basic terminology and structure for navigating n8n. This can be confusing at first, so I want to make it absolutely crystal clear right from the start.

When you first sign in to n8n, you are signing in at the account level. The first screen you see contains cloud-level information about your account. This includes something called an instance.

An instance is essentially an n8n engine. It’s the running software designed to automate business processes. That running installation is what we call an instance, and it has its own set of screens. Within an instance, you can run multiple workflows, where each workflow represents a business process you’re automating.

So there are three levels of granularity to keep in mind:

The cloud / account level

The instance level (the running n8n engine)

The workflow level (individual automations)

When you first sign in, you land on a screen called the Dashboard, also sometimes referred to as the Admin Panel. These two names refer to the same thing. This screen shows cloud-level, account-level details, and the URL reflects that—it’s the app-level URL.

From there, you can open your instance. When you enter the instance, you land on a screen that’s sometimes called Home and sometimes called Overview. “Home” is the most commonly used term. This is the home screen of your instance, and the URL now includes something like /cloud/workflows.

On this screen, you’ll see a list of workflows. Each workflow can be clicked to open its own editor. That editor screen is referred to by several names: sometimes the editor, sometimes the canvas. Technically, the editor is the whole screen and the canvas is the main area inside it, but in practice people often use the terms interchangeably.

The URL for this editor includes your instance URL followed by /workflow and an ID for that workflow. That’s how you know you’re inside a specific workflow.

So again, those are the three levels:

Cloud (account)

Instance

Workflow

Now let’s actually look at n8n and see this in action.

Here I am in my browser. I see the sign-in button, and since I’ve already logged in before, I land directly on this screen. You may have seen this screen already and felt briefly confused, because it looks different from what you see inside a running instance.

This is the Dashboard, also known as the cloud Admin Panel. This is the cloud-level view. You’ll see tabs like Dashboard, Manage, Billing, and Export. These are all related to your account and subscription, not the running software itself.

For example, here I can see my plan, the version I’m running, and how many days I have left in my free trial. All of this lives at the cloud level.

On this same screen, you’ll also see a box representing an instance. This is a running installation of n8n in the cloud. To enter it, I click Open Instance.

Now we’re inside the n8n instance itself. This is the Home or Overview screen of the running engine. Notice that the URL has changed and now includes the instance name followed by /cloud/workflows.

This screen shows an overview of your workflows. This is the actual n8n software, not your cloud account. If you were self-hosting, this screen would look the same.

Right now, everything here is empty because nothing has run yet. You might already have one workflow if you created something earlier. Otherwise, you may see a prompt to create your first workflow.

On the left-hand side, you’ll see the main navigation. You can expand it to show labels. We’re currently on the Overview (or Home) page. You may also see sections like Personal, which represents a project containing a subset of workflows.

You can create projects by clicking the plus button. For now, we’ll stay on the Overview screen.

There’s also a button called Admin Panel inside the instance. Clicking this takes you back to the cloud dashboard. This is how you move between the running instance and the cloud account view.

So to recap:

From the cloud dashboard, you open an instance

From inside the instance, you can return to the cloud dashboard via the Admin Panel

Next, there’s a Templates section. Templates are pre-built workflows you can copy and use as a starting point. We’ll look at templates later, but for learning purposes, we’ll mostly build things from scratch so you understand how everything works.

There are also AI-related features and a Settings screen. The settings control configuration options for the running instance itself.

Now, from the Home screen of the instance, you can click Create Workflow. This takes you into the editor, where you work on a specific workflow that automates a business process.

At the top, you’ll see the workflow name and ID in the URL. The large central area of the screen is the canvas, where you drag and drop nodes to build your automation.

That brings us back to the three levels one last time:

Cloud dashboard (account level)

Instance home (running n8n engine)

Workflow editor (canvas)

To move around:

Use Admin Panel to go back to cloud

Use Overview or Personal to return to the instance home

Click a workflow to enter the editor

To build muscle memory, I recommend going into n8n now and switching between these three levels a few times. Once this navigation becomes second nature, everything else in the course will feel much easier to follow.

# **L) Day 2 - How to Build AI Workflows with n8n: Nodes, Triggers, and Automation**

Okay, now let’s do some quick clicking around within the product itself, and I’ll define things as we go. First, we press Create Workflow to start a new workflow. This brings up the editor, where we can create a workflow that represents automating a business process.

The first thing we do here is add a step. This step is called a node. A node is one of the building blocks of a workflow—a little piece that makes up the entire automation. Nodes come in different flavors. They can be triggers, which start a workflow, or actions, which perform a task. For example, an “on chat message” node is a trigger because it initiates the workflow when a chat message is received.

To add a node, you press the plus button, which opens the node panel. For example, we can go to AI → AI Agent, add that, and then use the connector to link it to the trigger. Nodes are linked by connectors, which simply connect the output of one node to the input of another. You can also delete or move connectors to organize the workflow visually.

Next, we can add additional nodes. For instance, I can add an OpenAI node, which remembers the credentials we set up previously. We’re using GPT-4-mini in this example. Pressing escape closes the node selection, and we can organize the nodes on the canvas. We can also add memory, like Simple Memory, which allows the agent to store context across interactions.

At this point, the workflow is functional. If I type something, like “Hi there,” the AI agent uses its memory and context to respond appropriately. The workflow now demonstrates memory in action. For example, if I say my name is Ed, the AI responds with, “Nice to meet you, Ed. How can I help you today?” This shows that the agent is using conversation history to maintain context.

We can also modify the system prompt, which sets the tone for the AI. For example, I can change it from “helpful assistant” to “snarky, humorous assistant.” After refreshing, the AI responds in the new tone: “Well, well, well, look who decided to show up. What’s on your mind, oh mighty keyboard warrior?” This demonstrates how the system prompt controls the AI’s behavior.

Additionally, we can add tools to the workflow. For example, the MarketStack tool can be integrated to fetch stock prices. Once connected, we can send a query, such as “What’s the stock price of Google?” The AI uses the tool to fetch the most recent trading data and responds accordingly. This shows how nodes, memory, system prompts, and tools can work together to create a fully functional AI workflow.

The editor also allows us to switch between Editor and Executions. Executions show the inputs and outputs of workflow runs. This is useful for debugging and understanding how the workflow behaves over time. We can rename workflows, track executions at both the workflow and overview level, and manage multiple workflows from the home screen.

To recap some terminology:

Node: A single step or building block in a workflow, either a trigger or action.

Connection: A link between two nodes, connecting outputs to inputs.

Workflow: A collection of nodes and connections that automates a business process.

Execution: A run of a workflow, either manually or in production mode.

Active workflow: A workflow running in production mode.

Template: A pre-built workflow used as a starting point.

This concludes week one, day two. We’ve covered theory, terminology, and practical navigation of n8n, setting a solid foundation for building workflows. Tomorrow, we’ll move into our first yellow day, focusing on integrations. You’ll start building multiple workflows, seeing why n8n is such a powerful tool.

Take a moment to celebrate—you’re already 13% of the way through this course. Tomorrow will be an exciting step forward.

# **M) Day 3 - How to Integrate Google Sheets and Google Drive with n8n Workflows**

There are two ways in which Nw10 truly has a real wow factor.

The first is the way AI—and especially genetic AI—is deeply woven throughout the product, making it incredibly easy to create AI agents. AI isn’t something bolted on as an afterthought; it’s embedded at the core, so building intelligent workflows feels natural and intuitive.

The second wow factor is how Nw10 handles integrations.

Historically, integrations have been one of the hardest parts of building systems—connecting different tools, making them talk the same language, and dealing with brittle APIs and edge cases. This is where most of the real pain usually happens.
Except with Nw10, it doesn’t.

Nw10 doesn’t just make integrations simple—it makes them feel almost magical.

Today marks our first day focused entirely on integrations. It’s a yellow day.
Welcome to Week One, Day Three: Integrations Day. Let’s do this.

We’re now right in the middle of week one, and this first yellow day is all about integrations. The day is split into two parts. In the first half, we’ll focus on integrating with documents—things like spreadsheets and files. In the second half, we’ll dive into email and other technical integrations.

For the technical folks watching, you know just how hard this kind of work usually is—and you’re going to be blown away.
For the business folks, you may have always wondered why tech teams complain so much about integrations. After today, you might wonder that even more, because with Nw10, it’s genuinely a cinch.

Before we jump in, let’s do a quick recap. Repetition is always useful.

As a reminder, there are three levels of hierarchy when working with Nw10 in the cloud. At the top level, you have the cloud deployment, where you can log in and view your account-level details. Within that, you have a specific instance that’s running. That instance contains multiple workflows, each representing a business process.

The cloud level is like the admin dashboard where you see everything. The instance has its own overview or home screen. Each individual workflow opens into the editor—primarily a canvas—which is where we’ll spend most of our time today.

Let’s also revisit some terminology.

A node represents a step in a business process. A trigger is a special type of node that kicks off a workflow—this could be scheduled (like running once per day) or event-based (such as receiving a message). An action node represents something happening, like sending an email or updating a document.

A connection links nodes together, passing output from one node into the input of another. A workflow is simply a set of connected nodes that automate a business process. An execution is a single run of that workflow—either triggered manually during testing or automatically when the workflow is active in production.

Finally, there are templates, which are pre-built workflows you can use as inspiration or starting points. We’ll mostly build from scratch, but templates are great for learning patterns and ideas.

Now, one more recap—think of it like those movies where the same scene appears again and again, just slightly changed.

Integrations will play a huge role over the next few weeks, so there are a few important things you need to understand upfront.

First, you are always in control of integrations. You don’t need to do exactly what I do. Many integrations are free, some have free tiers, and others are paid—but you decide what you use. Always check what an integration does and whether you’re comfortable with it.

I’ll demonstrate a specific set of integrations, but Nw10 offers a massive range. You can follow along exactly or take things in a different direction. If I integrate with Slack, you might choose Telegram. If I post messages, you might trigger something else entirely. That flexibility is one of Nw10’s greatest strengths.

Second, integrations can be tiresome and slippery. Things will sometimes go wrong for reasons that seem absurd—like an API key getting messed up because hyphens were converted into long dashes when copied. Suddenly nothing works, and you’re left wondering what went wrong.

This is where patience and thick skin come in. Read the documentation. Try again. Generate a new key. Nine times out of ten, there’s a simple explanation—you just have to keep pushing until you find it. And if you get truly stuck, I’m always available to help. I come with the package.

Third, every integration we do is optional. Nothing is mandatory. If an API is paid, you can skip it. If you don’t like a tool, use another one. This is always in your court.

With that said, the first set of integrations we’ll tackle is Google Drive—working with Google Docs and Google Sheets. If you already have a Google account, you’re in great shape.

If you don’t—yes, I know, you’re part of a very small group on this planet—I’d strongly recommend setting one up. Google Drive, Docs, and Sheets are free, incredibly streamlined, and frictionless to integrate with. We’ll be using them heavily, so this is the one exception where I’d suggest getting set up if you aren’t already.

# **N) Day 3 - How to Build an AI Workflow in n8n with Google Drive Integration**

And here we are at Nanaimo.

I’m going to click on the Sign In button, and up it comes. As you’ll remember, we’re now on the dashboard screen, which is the cloud-level view. From here, I’ll press Open Instance to enter the running instance.

You can see that I’ve got 21 days left on my free trial, and this instance is running version two. I’ll press Open Instance, and that takes us into the instance home page.

Here it is.

If this is your very first time logging in, it may look slightly different, but otherwise it should look just like this. You’ll notice there’s a sample workflow already there—a snarky stock price lookup—but today we’re going to build some new workflows by working with Google Drive.

So let’s get started.

I’m going to click the Create Workflow button at the top. Right away, the editor opens up, with the canvas in the middle. By now, you should be familiar with this view.

To add the first step, I’ll click the plus button. Since this is the beginning of a workflow, we’re choosing a trigger. From the list of trigger nodes, I’ll select the On Chat Message trigger—the one you already know.

Once that’s added, you can either press Escape or click back onto the canvas and then press Escape to return to the main editor view. And here we are.

Now I’ll press the plus button again. This time, I’ll go to AI and select an AI Agent. Press Escape again, and we’re back on the canvas.

We’ll configure this agent with normal memory, just the simple memory setup. Escape again. You can see that simple memory is now enabled.

Next, we’ll choose a chat model. You can use OpenRouter or OpenAI, but I’ll stick with OpenAI’s chat model. It automatically suggests using the OpenAI account, and we’ll keep the default 4.1 mini model. Press Escape, and that’s done.

This is a solid starting point, so let’s quickly test it to make sure everything works.

I’ll type “Hi there,” run the workflow, and off it goes. You can see it thinking, and then responding:
“Hello. How can I assist you today?”

Perfect. Everything is working, and we’re ready to move on.

Before we start building integrations, I want to show you a few simple things about the editor to help you get more comfortable. We’ll add a little more each time.

First, let’s rename the workflow. Right now it’s called something like “My Workflow 6.” We can click on the name and change it to First Integrations, then press Enter. That’s now the new name of the workflow.

You may remember that workflows can also be tagged, similar to how blog posts are tagged. You can use this if you want to organize your workflows, though it’s optional.

Now, while you’re on the canvas, there are several useful navigation shortcuts. You can zoom in and out using the on-screen buttons, but there’s an even faster way.

If you’re on a PC, hold down the Control key. On a Mac, hold down the Command key—the one with the clover symbol. While holding it, click and drag on the canvas. You’ll be able to move around freely.

Try this yourself. Do it a few times.

As you do, notice the small mini-map in the bottom-left corner. It’s like the radar in a video game, showing the entire workspace and where your nodes are located. The white boxes represent the nodes in your workflow. This helps you orient yourself as workflows grow larger.

Next, let’s talk about keyboard shortcuts, because we don’t want to be clicking endlessly—we want to be pro users.

If you press the plus (+) key on your keyboard (no Shift needed), you zoom in. If you press the minus (–) key, you zoom out. If you press zero (0), the zoom resets to the default view.

This works without holding Control or Command, which makes it fast and intuitive. Plus to zoom in, minus to zoom out, zero to reset.

Another useful shortcut is the Tab key. Press Tab, and the node list opens on the right-hand sidebar. Press Escape, and it closes. Press Tab again—it opens. Escape closes it again.

Try pressing Plus, Minus, Tab, and Escape a few times to build some muscle memory for navigating the editor.

Now, let’s move on to Google Drive.

If you’ve never used Google Drive before, don’t worry—it’s extremely easy to pick up. If you don’t yet have a Gmail account, just go to gmail.com, create one for free, and you’ll automatically get access to Google Drive.

From there, you can click the app launcher in the top-right corner—the little grid icon—or just go directly to drive.google.com.

Once you’re in Google Drive, it works like most file-storage systems. You can create folders, upload files, and create new documents. I’ve already created a folder by clicking New → New Folder, and I named it “stuff.”

We’re now looking at the contents of that folder. If this interface is new to you, just click around—you’ll figure it out quickly.

And remember, if you really don’t want to use Google Drive, that’s fine. You can just watch along and focus on understanding how integrations work conceptually.

Now it’s time to create a document.

When I say “Google Doc,” I mean it in the general sense. Sometimes people use “Google Docs” to refer to any file in Drive, and sometimes they mean the word-processor specifically. In this case, we’re going to create a Google Sheet.

I’ll click New → Google Sheets, and here it is. Let’s name it Portfolio.

This is going to be a very simple sheet. We’ll add three columns:

Ticker

Quantity

Price

Under Ticker, we’ll add a few examples. Let’s start with Google—always a favorite. We’ll say we own three shares. Next, Apple—we’ll say two shares. And of course, Tesla, because that’s always the example people use. Let’s say two shares there as well.

So now we have a simple equity portfolio:

A ticker

A quantity

And a price column, which we’ve intentionally left empty

And that’s exactly where automation comes in.

Now that our data is ready, it’s time to build some automations.

# **O) Day 3 - How to Automate Stock Portfolio Tracker with n8n and Google Sheets**

Now I’m going to press the plus button here under Tools, search for Sheets, and open the Google Sheets tool.

Here it is.

This is how we give our LLM the ability to access a Google Sheet. The first thing it asks for is the credential to connect with, and we’re going to choose Create New Credential.

This is the moment where we connect our Nw10 cloud instance to Google Sheets. In many systems, this step can be really difficult and frustrating. For some integrations later on, this will be more challenging. But in the case of Google Sheets, it’s incredibly simple.

There’s a Sign in with Google button right here. Ignore everything else and just press that.

If you’re using the self-hosted version, this step is a bit more involved, but for now, we’ll just click Sign in with Google. A Google authentication screen pops up. I select my Google account, press Continue, and just like that, I see Connection successful.

I close the window, and now there’s a green box that says Account connected.

Honestly, it couldn’t be simpler.

At this point, our cloud instance is connected to our Google account. We haven’t connected it to a specific sheet yet, but the access is there. So I press the X button to return to the tool configuration screen.

Now we configure the tool itself.

The tool description can stay automatic. For the resource, we select Sheet within a document. For the operation, we choose Get rows.

Next, we choose the document from the dropdown. This list shows all the Google Sheets the account has access to. I select Portfolio, which is the sheet we just created. There’s only one sheet inside it, so I choose Sheet1.

There are options to add filters, but we’ll leave everything as-is. This tool is now ready.

So now we return to the main editor screen. We clear the chat, and we’re going to have a quick conversation with our sheet.

I type “Hi there.”
The agent responds as usual: “Hello, how can I assist you today?”

Next, I say:
“Please describe the sheet that you have access to read.”

The agent starts working. It connects to Google Sheets, pulls the data, and responds:

It tells me the sheet contains stock information with the columns Ticker, Quantity, and Price, and it lists the rows for Google, Apple, and Tesla along with their quantities.

That’s it.

We have successfully connected our Nw10 AI agent to a Google Sheet, and it took minutes—maybe even seconds if you were following along quickly.

Now let’s make it do something useful.

You can probably guess what’s coming next.

I move things around slightly on the canvas and add another tool—MarketStack, which we worked with yesterday. At this point, the setup should feel familiar.

We already have the credentials. The tool description stays automatic. The resource is End-of-day data, and the operation is Get many. The ticker is set by the model, and we turn on the Latest filter.

That’s all we need to fetch the latest stock prices.

Next, we add another Google Sheets tool. This time, the resource is still Sheets, but the operation is Update row.

We select the same Portfolio document and Sheet1.

Now comes the important part—telling the tool how to update the sheet.

The tool automatically detects the columns: Ticker, Quantity, and Price. We need to tell it which column to use to decide which row to update. We choose Ticker as the column to match on.

That means our AI can say, “Update the row where ticker equals Google” or “Apple” or “Tesla.”

Next, we define what values the model is allowed to set. We remove Quantity, because we don’t want to change it. We keep Ticker (for matching) and Price (for updating).

We explicitly tell the tool that the model defines the ticker and the model defines the price.

This tool’s job is now very clear:
Match on ticker → update the price.

We tidy up the canvas a bit, and now we have three tools:

Read rows from Google Sheets

Get market data

Update rows in Google Sheets

Now I open my Portfolio Google Sheet in a separate window and place it on the side so we can watch it live.

Back in Nw10, I clear the chat and say “Hi there.”
The agent responds normally.

Then I give the instruction:

“Please update the prices in my equity portfolio sheet to reflect the latest market prices.”

Now keep your eye on the spreadsheet.

The agent starts working. Messages are spinning. Tools are firing.

And then—bam.

Tesla’s price appears.
Apple’s price appears.
Google’s price appears.

All three prices are written directly into the Google Sheet.

The workflow executed successfully.

What we just witnessed was an AI agent reading a spreadsheet, fetching live market data, and writing updated values back into Google Sheets—all automatically.

That’s pretty fabulous.

What I love about this example is how tangible it feels. You can literally watch the numbers change in real time, which makes the power of integrations very real and very exciting.

Now, I want you to dig a bit deeper into the tools themselves.

Double-click on the Update Row tool. You’ll see the inputs and outputs from the last execution. Notice that it was called three times—once for each stock. You can switch between run 1, 2, and 3 and see exactly what data went in and what came out.

On the left, you see the inputs in JSON format. If you’re familiar with JSON, great. If not, don’t worry—you’ll get comfortable with it quickly. You can also switch between JSON, table, and schema views depending on what feels most intuitive.

You’ll see that the tool matched on Apple’s ticker and updated the price accordingly. The same happened for Google and Tesla.

Now do the same with the MarketStack tool. You’ll see it was also called three times—once per ticker—and you can inspect the returned market data for each one.

Finally, look at the Get Rows tool. This one was only called once, and it returned the entire sheet. You can view the result as JSON or as a table. What came back was the original data—tickers, quantities, and empty prices—before the AI updated them.

This gives you a full picture of how the workflow executed.

Now it’s your turn.

Dig into the tools. Explore the inputs and outputs. Switch between JSON and table views. Add another column. Try pulling in highs and lows. Experiment with what else the AI can write into the sheet.

Watch it update live.

It’s a little spooky—and incredibly powerful.

And this is what it feels like to work with an AI agent deeply integrated with Google Sheets.

# **P) Day 3 - How to Build an AI Agent to Automatically Draft Gmail Replies in n8n**

Next, we want to save our current workflow, because we’re going to build a new one. On a PC, you can press the red Save button, or use Control + S (on Mac, it’s Command + S). Once saved, it confirms with a “Saved” message.

Now we can go back to Personal or click Overview to return to the main screen. It’s time to create a new workflow. Press the Create Workflow red button. The first step, as before, will be a Chat Message trigger, though in future workflows we might try different triggers.

Next, we add another AI agent. At this point, you’re becoming a pro—you don’t really need me to guide you. We’ll use Simple Memory, and for the chat model, again, we’ll select OpenAI Chat Model. Let’s test it with a simple “Hi there,” and confirm everything is set up properly.

It’s now time to add a new tool. Press the plus button, type Email, and select the Gmail tool to consume the Gmail API. We then choose Create New Credentials, which brings up the Sign in with Google button. Click it and connect it to your chosen Gmail account—this could be your personal or business email. You can grant it access to everything or limit permissions based on your comfort level. Remember, we’re always in control of what the AI can access.

With the account connected, we return to the tool setup screen. We choose the resource Messages, because we want to read emails, not send them. The operation will be Get Many, but to avoid reading all emails, we apply a filter to only pull emails received after yesterday. Instead of hard-coding the date, we switch to Expression mode, which allows us to use a small JavaScript snippet (via Luxon library) to dynamically get “yesterday.” This keeps it flexible and dynamic, and it’s a good introduction to the low-code aspect of the platform.

At this point, our AI agent has OpenAI chat, Simple Memory, and the new Gmail tool. Before running, it’s useful to know the Tidy Up button, which automatically reorganizes and resizes the canvas—very handy when adding multiple tools.

Now let’s test the workflow. I send myself an email from a different account, saying: “I have exciting news. My agent in Nw10 can read my email. This is huge.” Then in the editor, I instruct the agent: “Please read the most recent email I received and summarize its content.”

The agent connects to Gmail, fetches the latest message, and responds:
“The most recent email you received is from Edward Donner with the subject ‘Important News.’ The content snippet indicates exciting news that the agent can now read emails, which is described as huge with many possibilities.”

We’ve now confirmed that our agent successfully read a real email. It’s amazing how quickly this works with just a few clicks. You can also expand this to add more tools for reading and sending emails later.

Next, we create another Gmail tool, this time to draft emails. I want to be careful here—rather than letting the agent send emails freely, we’ll constrain it. It can create a draft email, choose the subject and message content, but the recipient is locked to my Gmail address. This ensures full control while still allowing automation.

I then instruct the agent: “Please draft an email saying I very much agree with this sentiment. This is huge news.” The workflow runs, and when I check my Gmail, there’s a draft email ready. It reads:

“Hi Edward,
I very much agree with your sentiment. This is huge news. The possibilities ahead are truly exciting.
Best regards, Edward.”

And just like that, the agent has read an email and drafted a response, fully respecting the constraints we set.

Of course, we could extend this further—have it create new emails, respond automatically, or submit them—but we chose to be careful and only give access to tools we trust. This is a good practice until you’re confident in your checks and balances.

So, to recap: today we read emails and drafted a response, showing how agents can interact safely with Gmail. The possibilities are enormous, but we take baby steps to build confidence.

With that, we wrap up Day Three of Week One. These were our first few integrations, and they were exciting ones:

We connected to Google Drive and Sheets and automated stock price updates.

We connected to Gmail, allowing an agent to read and draft emails.

The goal is not just to watch but to experiment yourself—build workflows, inspect nodes, examine tool inputs and outputs, and practice passing data between steps. Tomorrow, we’ll continue with more integrations, explore JSON and table views, and get even more comfortable with connecting AI agents to various data sources.

And just like that, week one, day three is complete. Can you believe it—20% of the course done already? We’ve built what could practically be an automated business process in just a few days. It only gets better from here.

See you tomorrow!

# **Q) Day 4 - Understanding JSON in n8n: Key-Value Pairs, Objects, and Arrays**

We have a big day ahead of us. Welcome to week one, day four. This is an important day. It’s the second yellow day focused on digging into integrations, and in addition to that, we’re also going to talk about data. We’ll also take a look at expressions, at least at a high level. Let’s get into it.

In NHSN, data is largely described using a format called JSON. I imagine that at least half of you are already very familiar with JSON. Some of you may have seen it many times, some of you might come from a JavaScript background and know it inside out, and some of you may be newer to it. You’ve probably encountered it before, even if you don’t fully understand it yet. I’ll cover it briefly, but the main takeaway is this: you will get very familiar with JSON. You’re going to see it a lot.

JSON is a very simple and standard way to describe structured data. It’s designed to be both machine-readable and human-readable, which makes it very convenient. Machines understand it, humans understand it, and that allows us to work with it together effectively.

Let’s cover the four basic ingredients of JSON. If you already know all of this, feel free to mentally fast-forward. If not, this will give you a solid foundation.

First, JSON is fundamentally a way of describing data using key–value pairs. This means that everything has a name (the key) and a value. For example, a person might have a first name with a value, a last name with a value, and an age with a value. These key–value pairs form the core of JSON. The values themselves can be text (called strings), numbers, booleans such as true or false, or a few other types as well.

The second key ingredient in JSON is called an object. An object is simply a collection of key–value pairs grouped together. For example, a person object might include first name, last name, age, and other attributes. Objects are sometimes also called dictionaries, which will sound familiar if you come from a Python background, since they are very similar to Python dictionaries.

The third ingredient is an array. An array is just an ordered list of things. It can contain multiple items, one after another, in a specific order.

The fourth ingredient is nesting. This means that a JSON object doesn’t just contain simple values like strings or numbers. It can also contain other JSON objects and arrays. Those nested objects can themselves contain more objects and lists. In practice, you often see JSON structures that are made up of many layers of objects inside objects, and arrays containing objects, and so on.

Now let’s talk a bit more about each of these in detail.

Starting with key–value pairs: all data in JSON is represented this way. You write a key, followed by a colon, followed by a value. For example, "name": "Alice", "age": 30, or "is_student": false. You could also have something like "middle_name": null.

There are a few important details to notice here. Whenever you use text (strings), you must put double quotes around it. That applies to both the key and the value if the value is text. Numbers do not have quotes around them. Booleans are written as true or false, always in lowercase. This can trip up people coming from Python, where booleans are written differently. The value null is a special value that represents “no value” or “empty,” and it is valid in JSON.

It’s also important to use double quotes, not single quotes. Another common issue is curly quotes—some word processors automatically convert straight quotes into curly ones, and those will break JSON. JSON requires straight double quotes. Additionally, while it is technically allowed to have spaces in keys, it’s best practice not to. Instead of "is student", it’s better to use something like "is_student", as spaces can cause problems in many situations.

Next, let’s look at objects. An object represents a thing with multiple key–value pairs. Objects are written using curly braces. You open a curly brace to start the object, then list the key–value pairs separated by commas, and finally close the curly brace.

For example, an object might contain "name": "Alice", "age": 30, "is_student": false, and "middle_name": null. Each key–value pair is separated by a comma, except for the last one. The final entry must not have a trailing comma. This is different from some programming languages like Python, where trailing commas are allowed.

You’ll often see objects written with indentation and whitespace to make them easier for humans to read. This formatting does not matter to the machine—it’s purely for readability. The same object could be written all on one line and would mean exactly the same thing.

Now let’s talk about arrays. Arrays are written using square brackets. An array might look like ["apples", "bananas", "oranges"]. Each element in the array is separated by a comma. Arrays can contain numbers, strings, booleans, or even a mix of different types if you want.

Arrays themselves are values, which means they can be used as the value in a key–value pair. For example, you might have "favorite_fruits": ["apples", "bananas", "oranges"]. That entire list becomes the value associated with the key.

Finally, let’s look at nesting in more detail. Arrays can contain objects. For example, you could have an array where each element is an object representing a person, such as one object for Alice with her name and age, and another object for Bob with his name and age. Writing it this way makes it clear that the array has multiple elements, and each element is its own object.

Similarly, objects can contain other objects. For example, you might have an address object with keys like street, city, and country. That address object can then be used as the value for an "address" key inside a larger object that also contains a person’s name and age. In this case, the address is not text, a number, or an array—it is itself another object nested inside the main object.

This should give you a good sense of how JSON works and how flexible it is. Objects can contain arrays, arrays can contain objects, and everything can be nested as deeply as needed. If this is all new to you, don’t worry—once you start seeing JSON regularly, it will quickly become second nature.

With that overview in place, next up we’re going to talk about expressions.

# **R) Day 4 - n8n Authentication Methods: API Keys, OAuth2, and Workflow Integration**

This is not a no-code course. It’s a low-code course, which means that occasionally we will take small steps in the direction of coding. One of the main ways we do that is through expressions—and you’re going to love expressions.

Expressions give you much more flexibility in how your workflows run. They give you significantly more power over what happens in your automations. Instead of using fixed values everywhere, expressions allow you to determine values dynamically. You can decide what should be used at runtime rather than hard-coding things upfront.

A good way to think about expressions is like formulas in Excel. Instead of typing a fixed number into a cell, you write a formula that calculates the value. Expressions work the same way. And honestly, they are not complicated. They may look intimidating at first—much like Excel formulas—but once you understand the idea, and once you have a few expressions in your toolkit that you can reuse, they become very easy to work with.

We’re going to look at some expressions now, and then we’ll start using them properly in real integrations. If this part feels a bit theoretical, don’t worry—we’re about to put all of this into practice very soon.

In NHSN, there are many fields where you’ll see a small toggle above the field. This toggle lets you switch between a fixed value and an expression. That’s where we’ll be using expressions today. You can choose to say, “I always want this value to be 3,” or you can provide a small formula that dynamically calculates what the value should be.

Expressions are always surrounded by double curly braces—two opening curly braces and two closing ones. You’ll get very used to seeing these everywhere. When you see double curly braces, that should immediately click in your mind: this is an expression, and there’s logic happening here.

There’s a very important shortcut to know. If, inside an expression, you want to access the incoming data from the previous node, you use a special variable: $json. This means “give me the incoming data in JSON format.”

A lot of working with expressions comes down to navigating through JSON objects. Remember that JSON is made up of key–value pairs, and the value itself can also be another object with its own key–value pairs. Very often, you’ll be doing things like taking a person object and pulling out the first name, or taking an address object and extracting the country from it.

To do this, you use a dot (.). For example, if you have a JSON object representing a person with a name, age, and address—and the address itself has a street, city, and country—then writing $json.name will extract the person’s name. Writing $json.address will extract the entire address object. Writing $json.address.city will extract just the city value.

If you look at the examples carefully, you’ll see how those dots allow you to drill deeper into the JSON structure and pluck out exactly the piece of data you need. That’s really all you need to understand about JSON data structures and how expressions navigate through them.

At various points, we’ll also use expressions that are slightly more advanced than simple $json lookups. You don’t need to understand these right now—I just want you to recognize what they look like. We’ll work with them properly later, and they’ll make much more sense when you see them in action.

For example, $json gives you access to the incoming data for the current node. But what if you want to access data from an earlier node in your workflow? In that case, instead of $json, you use $node. You write $node["Node Name"] and then continue drilling into its data structure. This allows you to grab data from any previous node in the workflow.

This is powerful because it means you can write expressions that combine information from multiple points across your entire workflow, not just the immediately previous step.

Another useful thing to know is how to convert JSON data into a string. Sometimes you want to pass data as plain text, for example when sending it to a language model. In that case, you can use JSON.stringify(). You write JSON.stringify() and pass in whatever data you want inside the brackets—such as $json. This converts the JSON object into plain text.

If you’re new to all of this and it doesn’t fully click yet, that’s completely fine. Just hold onto the general idea. The understanding really comes when you start building things and experimenting. If you already know JSON, then these are simply a couple of extra expressions worth adding to your mental toolkit.

As a general principle, you never need to memorize expressions. Don’t try to memorize syntax. As long as you understand what expressions do and how they work conceptually, you can always look up the exact syntax when you need it. The documentation in n8n is excellent, and tools like ChatGPT or Claude can tell you exactly which expression to use. What matters most is recognizing what the correct solution looks like when you see it.

That’s enough on JSON and expressions for now—just enough to allow us to build a lot more integrations. Before we start doing that, though, I want to introduce the topic of the integrations we’ll be working on today, because they’re going to be a lot of fun.

Often, the trickiest part of building an integration with a third-party service is authentication. Authentication is about proving to the third party that you are who you say you are. There are many different ways to do this, and today we’ll be using several of them.

Here are the authentication approaches we’ll be using, listed in order of increasing complexity.

The simplest method is using an API key. In this approach, you generate an API key with a third-party service and then provide that key to n8n. Once the key is validated and turns green in n8n, you’re connected. That’s what we did with OpenRouter and OpenAI, and we’ll do the same with many other services.

An API key is essentially a password or secret. Once it’s set up, everything just works. If only all integrations were that simple.

Things get more complicated when you don’t just want a single generic connection to a service, but instead want to log in as a specific user. For example, you don’t want to connect to Gmail in general—you want to connect specifically to your Gmail account. That requires authentication at the user level, and that’s where OAuth 2 comes in.

OAuth 2 is the standard method used for secure, user-specific authentication with third-party platforms. The next two authentication types we’ll use are both different flavors of OAuth 2.

The simpler OAuth 2 pattern is what we used with Google Sheets and Gmail. In this case, n8n Cloud already has a pre-configured OAuth setup. You click a button, a Google login screen opens, you sign in, and you’re done. You are authenticated as a specific user.

This only works in n8n Cloud, and only for services where n8n Cloud has already built the necessary infrastructure. Google Sheets and Gmail are good examples. The bridge between n8n and Google is already in place, so the process is quick and painless.

The hardest case is full OAuth 2, where that shortcut doesn’t exist. This applies when you’re running self-hosted n8n, or when you’re using n8n Cloud with a service that doesn’t have a pre-built OAuth connection. Slack is a good example of this.

In these cases, you have to build the OAuth scaffolding yourself. You need to configure callback URLs, permissions, and multiple steps in the authentication process. Proper OAuth 2 is quite fiddly. It often requires patience and a thick skin. You may need to try two or three times before everything finally works.

The good news is that once the authentication is done, it’s done. After you’ve successfully authenticated with a service like Slack, you can reuse that authentication across your workflows simply by selecting it from a dropdown. It’s a one-time setup per service.

Now that you know this, we’re ready to build some integrations. Enough talking—let’s get practical.

We’re going to build three integrations today, all focused on communication and connectivity.

The first one is push notifications to your phone. This is one of my old favorites because it’s simple, fun, and very satisfying to see something pop up on your phone as a result of a workflow. This will be a gentle, API-based integration.

The second integration will be Telegram. You may or may not use Telegram, and that’s okay. Even if you don’t plan to use it, it’s still a great example of how integrations work in n8n. The same principles apply to many other services, so it’s worth seeing how it’s done.

The third integration is Slack—and Slack is going to be painful. Prepare yourself. This will involve the full OAuth 2 flow, and it can be frustrating. If you don’t feel like doing it yourself, you can simply watch and learn. It’s still valuable to see how the process works.

As a quick survival guide for integrations: if something goes wrong, try again. Many integrations fail the first time and succeed on the second or third attempt. Pay close attention to error messages and clues about what might be wrong.

Post questions in Udemy if you get stuck. I may or may not be able to reproduce your exact issue, but someone almost certainly can help. Share screenshots, explain what you’re seeing, and we’ll try to figure it out together.

Also, read the documentation. The n8n docs are very clear and extremely helpful. Use them alongside experimentation.

And if all else fails, just pick a different integration. One of the amazing things about n8n is how many integrations it offers. If Telegram isn’t working for you, try Slack. If Slack is too painful, try push notifications. There are plenty of options, and at least one of them will work.

With that, let’s go back to n8n. We’re going to build three communication-focused integrations. Have a thick skin—but it’s going to be fun. Let’s do it.

# **S) Day 4 - How to Integrate Pushover Notifications with n8n Workflows**

Okay, first up, we’re going to build an integration that sends push notifications directly to our phones. For this, I’m going to use a tool called Pushover. It’s a really neat little service that I love and use in all my courses. It’s extremely straightforward to set up, and it’s free for the first month at least. After that, it costs a very small amount, but you only really need it for the duration of this course—unless, of course, you end up loving it and want to keep using it.

The first thing you should do is go to pushover.net. This is what the site looks like. Once you’re there, click on Sign Up and create a new account.

After you create your account and log in, you’ll immediately see an API key on the page. This is called your User Token or User API Key, and it starts with the letter U. You’ll find it at the top right of the screen. This is the first of two tokens that you need for this integration.

When you log in, you’ll see a screen like the one I’m showing here. My own user token is visible on my screen, but I can’t show it to you because it would give full access to my account. Just know that this user token is printed right on the main page after login, and it always starts with the letter U.

The second token you need comes from creating an application. To do this, click on Create an Application / API Token. Inside that page, give your application any name you like—it doesn’t matter what you call it. Once you create the application, you’ll be given a second token. This one is called the Application Token, and it always starts with the letter A.

At this point, you should have two tokens:

A User Token starting with U

An Application Token starting with A

The final setup step is to install the Pushover app on your phone, whether you’re using an iPhone or an Android device. Once you install it and log in, your phone will appear in the list of devices on the Pushover website. At that point, everything is connected, and you’re ready to start sending push notifications from n8n. You can also use the Pushover website to test that notifications are arriving correctly.

Now, let’s go back to n8n.

I’ve already logged in and selected my instance. Since you’re pros at this point, I’m going to create a new blank workflow by clicking the Create Workflow button.

Next, I’ll add the first step by clicking Add First Step. I’m going to choose Chat Message, and then I’ll press Escape to come back to the main workflow view.

After that, I’ll click the plus button again and add an AI Agent, then press Escape once more. I’ll assign a chat model to it—OpenAI in this case—but you can use any model you like. Now the chat model is set up.

I’ll also add Simple Memory, which is exactly what we’ve done before and should feel very familiar by now.

Now it’s time to add a tool. Before doing that, just a reminder that you can click the Tidy Up button to make everything look neat and organized.

I’ll press the plus button under tools and search for Pushover. Sure enough, there is a Pushover tool available. Once I add it, the first thing we need to do is authentication.

To authenticate, we select Create New Credential. This is the simplest type of authentication—just an API key. The important thing to remember here is that the API key you paste into this field must be the Application Token, the one that starts with the letter A. If it doesn’t start with A, it’s the wrong token.

I paste in the application token, click Save, and n8n thinks for a moment. If everything is correct, the credential turns green. That’s exactly what you want to see. If it doesn’t turn green, something is wrong—go back and double-check that you used the token starting with A.

Once that’s done, we move on to the next field: User Key. As you might guess, this is where the User Token goes—the one that starts with the letter U and appears on the Pushover dashboard when you log in. I’m not pasting it on screen because it shows in full, but that’s what goes here.

There’s also a Message field. Whatever you put into this field will be sent as a push notification. If you typed the word “bananas” here, every time this tool runs, it would send “bananas” to your phone. That’s not very useful.

Instead, we want the language model to decide what message to send. To do that, we click the button that says Let the model define this parameter. This allows the AI to dynamically generate the message content, which is exactly what we want.

After pasting in the user key, we save again. It’s worth making sure the workflow is actually saved. You can press Ctrl + S on Windows, Command + S on Mac, or click the Save button. Once it says “Saved” and the red highlight disappears, you’re good.

Now let’s tidy things up again to keep the workflow looking clean.

Next, I’m going to add one more tool. I’ll click the plus button and search for Date. I’ll add a Date & Time tool and choose the operation Get Current Date. There are several options here, but we’ll just leave it at that. This tool allows the language model to access the current date. It’s simple, but it might come in handy.

Now let’s test everything.

I’ll type “Hi there” as a starting point. The assistant responds with something like, “Hello, how can I assist you today?”—which we’ve all seen many times by now.

Next, I’ll say: “Please send me a push notification with today’s date.” Keep an eye on the right-hand side as it runs.

And there it is. Did you hear the notification sound? I got a push notification on my phone with today’s date. It worked.

If you didn’t hear a sound, that’s okay. Pushover has notification settings that control whether a sound is played. I personally set the priority to high so it makes a satisfying noise, but that’s entirely optional.

The important thing is that the notification arrived. If it didn’t work for you, don’t panic. Go back, double-check the keys, and try again. This is the simplest type of integration using API keys, and once it works, it works reliably.

And that’s it. You’ve now built a working integration using API-key-based authentication. You can send push notifications to your phone from any workflow you build in n8n.

That’s a powerful capability—and it’s just the beginning.

# **T) Day 4 - Create Telegram Bot Using n8n: Complete AI Chatbot Integration Tutorial**

First, I’m going to save this workflow as the Push Workflow. Once that’s saved, I’ll go back to the Personal workspace. At this point, the push workflow is ready.

Next, we create a new workflow specifically to integrate with Telegram. This workflow is only meant for Telegram users. If you’re not a Telegram user, you can skip this part. Telegram is very similar to WhatsApp and is especially good for this kind of automation. I’ll walk you through the entire process.

Telegram integration is API key–based, which makes it very simple. If you’re using Telegram on your phone or desktop app, the first thing you need to do is search for a specific user called BotFather. You’ll find it by typing botfather into the search bar. Even though the name sounds odd, BotFather is what Telegram uses to create and manage bots.

Once you open a chat with BotFather, you’ll see a screen similar to mine, showing that you’re chatting with BotFather, which has millions of monthly users. The first command you type is /start, which displays a list of available commands.

From there, you type /newbot. This tells Telegram that you want to create a new bot. Telegram will then ask you to provide a name for your bot. In my case, I named it something like TG_convo, but you can choose any name you like.

After naming the bot, Telegram will ask you to choose a username for it. This username must end with “bot”, for example tetris_bot or tg_convo_bot. I named mine tg_convo_bot, but again, any name is fine as long as it ends with “bot”.

Once that’s done, Telegram responds with a congratulatory message confirming that your bot has been created. It also provides a link to your bot and mentions that you can add a description, profile picture, and other settings later. At this stage, the bot is operational.

Immediately after this, Telegram provides you with an API key. This key is extremely important. You should carefully copy it, making sure there are no extra spaces at the beginning or end. This API key is what we’ll use to connect Telegram to N8N.

There is also another way you might encounter BotFather, where it opens in a separate interface with menu options like “Create New Bot”. Either way, the end result is the same: you create a bot and receive an API key. Once you have that key, you’re ready to integrate Telegram with N8N.

Now we move to N8N. Inside N8N, click Create Workflow. Add a first step and search for Telegram. When you select Telegram, you’ll see a list of triggers—these are events that can start a workflow.

Choose the trigger called On Message. This means whenever a message is received in Telegram, the workflow will start. Once selected, the Telegram node opens and asks for credentials.

Click the credentials dropdown and choose Create New Credential. It asks for an access token, and this is where you paste the Telegram API key you copied earlier. After pasting it, click Save. If everything is correct, the connection turns green and shows “Connection tested successfully”.

With that done, your trigger is ready. Now we add the second step. Click the plus button, go to AI, select AI Agent, and choose OpenAI as the chat model. Disable memory (use simple memory or none) and don’t add any tools initially.

Optionally, you can add a tool to provide the current date and time to the AI agent. Once that’s configured, move on to the next step.

Click the plus button again. This step defines what happens with the output of the AI agent. Search for Telegram again, but this time select Actions, not triggers. Choose Send a Text Message.

For credentials, it automatically uses the same Telegram credentials you already set up. You don’t need to create them again. The text we want to send back should come from the output of the previous AI agent node.

There’s an extra complication here. Telegram conversations are tied to a chat ID, and when sending a message back, we must make sure it goes to the same chat that triggered the workflow. This means we need to correctly pass the chat ID along with the message.

To understand how this works, the easiest approach is to first test the workflow without fully configuring everything and see what breaks.

Before testing, deactivate the Send Text Message node by clicking the power button so it’s turned off. Also deactivate the memory node, as it can interfere during testing. Then click Execute Workflow.

At this point, N8N starts listening for Telegram messages. Now go to Telegram, open the chat with your bot, and press Start. As soon as you send a message, the workflow will likely fail.

When it fails, you’ll see an error saying “No prompt specified”. Double-click on the AI Agent node to inspect what’s happening.

On the left side, you’ll see the incoming data from Telegram in JSON format. This includes several nested objects, such as message, which itself contains fields like text.

The problem is that the AI agent is expecting a chatInput field, which does not exist in Telegram’s JSON payload. Telegram is not a chat trigger—it’s an API trigger—so we need to explicitly tell the AI agent where to get the prompt text from.

The correct input is the message text sent by the user in Telegram. You can clearly see it inside the JSON under message → text.

To fix this, go to Source for Prompt in the AI agent node. Instead of selecting a chat trigger, choose Define below and use an expression.

Now click and drag the text field from the JSON panel into the prompt input. N8N automatically generates an expression like {{$json.message.text}}. This tells the AI agent to use the Telegram message text as the prompt.

You could also manually type this expression, and you’ll see the resolved value (for example, /start or hi there) displayed below it.

Now the AI agent is correctly receiving input from Telegram.

Click Execute Workflow again. Go back to Telegram and type a message like “hi there”. The workflow runs successfully.

If you open the AI agent node, you’ll see that the input text (“hi there”) was correctly extracted and sent to the language model. The model responds with something like “Hello! How can I assist you today?”

This confirms that the integration is working. We successfully extracted Telegram input using expressions, passed it to the AI agent, and received a valid response.

This is a slightly more advanced integration compared to basic workflows, but as you can see, it works end to end once everything is wired correctly.

# **U) Day 4 - How to Integrate Telegram Bot with n8n AI Agent Workflow Automation**

Now it becomes a bigger deal, because we’re going to hook up the output of the AI agent so that it sends messages back to Telegram. This is the final and most important part of completing the integration.

To begin, we turn the power back on for the Send Text Message node. Double-click the node so we can see its configuration again. Remember, to send a message back to Telegram correctly, we need to provide two things: the chat ID and the text. The chat ID ensures that the reply goes back to the same person who sent the original message.

Let’s start with the Text field. On the right side, you’ll see a toggle between Fixed and Expression. This toggle is critical. Fixed means the bot would always reply with the same static text, like “bananas,” no matter what the user says. That’s not what we want.

We want the text sent to Telegram to be dynamic, based on the output of the AI agent. So we switch the toggle to Expression.

Now, what exactly are we pulling in? If you look at the data coming from the AI agent, it’s JSON. Specifically, it’s a single object containing one key-value pair. The key is called output, and the value is something like:
“Hello! How can I assist you today?”

That output field is exactly what we want to send back to Telegram.

To do this, we enter an expression using curly braces. When you type two opening curly braces, the closing ones appear automatically. Inside the expression, we start with $json, which refers to the incoming JSON object. Since the object has a field called output, we simply append .output.

So the full expression becomes {{$json.output}}. As soon as you enter this, you’ll see a preview showing the actual text value, such as “Hello! How can I assist you today?” That confirms it’s working correctly.

At this point, the Send Text Message node is now correctly wired so that whatever the AI agent produces will be sent back to Telegram.

There is an easier way to do this, of course. Instead of typing the expression manually, you can simply drag and drop the output field from the AI agent’s JSON into the Text field, and N8N will generate the expression automatically. However, walking through it manually helps you understand exactly what’s happening under the hood.

Next comes the Chat ID, which is slightly trickier.

Once again, we don’t want a fixed value here. Each Telegram conversation has a unique chat ID, and we must respond using the same one that triggered the workflow. So we switch Chat ID to Expression mode.

Now the question is: where does the chat ID come from?

It comes from the Telegram trigger, which is the very first node in the workflow. In the incoming data panel, there’s a dropdown that lets you inspect data from earlier nodes. Select the Telegram trigger node from that dropdown.

When you look at its JSON, you’ll see fields like message ID, chat, and inside chat, a chat ID. That’s exactly what we need.

Wouldn’t it be nice if you could just drag that chat ID directly into the Chat ID field?
Good news—you can.

Click and drag the chat ID value into the Chat ID field and release it. N8N automatically creates the correct expression for you. Even if the expression looks complex, you don’t need to worry about writing it yourself. Drag-and-drop handles everything.

You’ll also see a preview value, which confirms that the chat ID is being resolved correctly.

Now everything is wired up. The AI agent produces an output, that output becomes the Telegram message text, and the chat ID ensures the reply goes back to the correct conversation.

Let’s test it.

Click Execute Workflow so the workflow starts listening for Telegram messages. Open Telegram, bring the chat window into view, and send a message such as:
“Hi there, what’s two plus two?”

The workflow runs, and a moment later, Telegram replies:
“Two plus two equals four. How can I assist you further?”

It worked.

At this point, we’ve successfully created a full conversational loop. A message comes in from Telegram, the workflow triggers, the AI agent processes the input, and the response is sent back to the same Telegram chat using the correct chat ID.

Now there’s one more important detail: memory.

The Simple Memory node needs a session key so it knows how to associate messages with a specific conversation. By default, it assumes it’s connected to a chat trigger, which isn’t true in this case. That default setup won’t work.

We need to define our own session key.

Open the Simple Memory node and change the session key source to Define Below, then switch it to Expression mode. What should we use as the session key? The chat ID makes perfect sense.

So we drag the same chat ID expression into the session key field. This ties the memory directly to that Telegram conversation.

Turn the Simple Memory node back on—it was disabled earlier—and press Execute Workflow again.

Now let’s test memory.

In Telegram, send:
“Hi there, my name is Ed.”

The bot responds appropriately.
Then send another message:
“What’s my name?”

The response comes back:
“Your name is Ed.”

That confirms the memory is working.

You can experiment further here. Instead of using the chat ID as the memory key, you could use the Telegram username. That way, the memory would persist even across different chat sessions with the same user. Depending on your use case, that might be an even better approach.

At this point, we’ve built a two-way Telegram integration. Telegram messages trigger the workflow, the AI agent processes them, responses are sent back to Telegram, and memory is maintained per conversation.

There’s just one last wrinkle.

Up to now, we’ve had to keep pressing Execute Workflow every time we wanted to test a message. That’s fine for testing, but it’s not how a real automation should work.

Everything so far has been running in test mode.

To make this workflow truly live, we need to publish it.

First, save the workflow. Then press the Publish button. You’ll be asked to provide a version name and optionally a description. Enter whatever you like and confirm.

Once published, N8N tells you that the workflow is now live and listening for events from Telegram. At this point, you won’t see nodes lighting up anymore—because the workflow is running in production.

Now try messaging your Telegram bot again.

Send:
“Hi there.”

The bot responds immediately.

Ask something like:
“What’s the current date?”

If everything is set up correctly, the AI agent will call the date tool and respond with today’s date. The workflow is now fully deployed, running live, and handling messages automatically.

This confirms that the Telegram integration works end to end, including tool usage and memory, and is now running in production.

Once you’re done experimenting, you can stop it by clicking the three dots at the top and selecting Unpublish. You’ll see a confirmation that the workflow has been unpublished.

You can rename it something like Telegram Workflow, save it, and head back.

That completes the Telegram integration. It was a deep one—covering triggers, expressions, memory, responses, and publishing to production.

Now, if you thought this one was a bit intense…
the next integration is even more grueling.

It’s time for Slack.

# **V) Day 4 - How to Build a Slack Bot with n8n OAuth Integration and Automation**

Okay, welcome to Slack time. This is our third and most hardcore integration, and it’s with the business messaging platform Slack.

To do this integration properly, you need access to a Slack workspace where you have admin permissions, because we’ll be creating a Slack app and bot. If you don’t have admin access, that’s totally fine—just follow along and observe. This is still very valuable because it exposes you to OAuth-based integrations, which are common and important in real-world systems.

In my case, I’m in my own Slack workspace where I do have admin permissions. You can tell because I see the admin controls. The first thing I do is go down to Apps and workflows and select it. This takes us into the app-management side of Slack, where we’ll set up a Slack app and Slack bot.

When I click this, Slack opens a web browser and lands me on the Installed Apps page. You’ll see a list of apps already installed in the workspace. In my case, there are a couple of previous attempts—I’ve actually done this twice already, successfully, but with quite a bit of frustration along the way. Still, it works, and so can yours.

One important thing that often takes a while to remember is where to start. Look at the top-right navigation. There’s a menu, and from there, you need to click Build. This takes you to the app-building interface.

On the Build page, you’ll see a button that says Create New App. Click that, and then choose From scratch. Slack now asks you to give your app a name. I’m calling mine N8N3, because this is my third attempt, but you can name yours anything you want.

Next, you select the workspace where this app will live. Once that’s done, click Create App. Slack creates the app and drops you into a configuration screen. There’s a lot here—many menus, many settings—and we’ll be touching several of them. Many of the IDs and tokens found here will later be required inside N8N.

The first—and arguably most important—section we need to visit is OAuth & Permissions. This is where Slack defines what your app is allowed to do.

OAuth uses the concept of scopes, which specify permissions. Scopes define exactly what actions your app and bot are authorized to perform. We need to add several scopes for our bot to function correctly.

Click Add OAuth Scopes. A long list appears. I’ll walk you through the exact ones we need. I discovered these mostly by trial and error—adding too few, hitting errors, and then coming back to add more.

The first scope to add is app_mentions:read. As you type, Slack filters the list, making it easy to find. This allows the bot to read messages where it is explicitly mentioned.

Next, click Add again and select channels:history. This allows the bot to read messages from channels it’s part of.

Add another scope: channels:read. This lets the app read channel information.

Next, add chat:write. This is critical—it allows the bot to send messages into Slack channels.

Then add im:history, which allows the bot to view direct messages in channels where it has been added.

Finally, add users:read, which allows the app to read information about users in the workspace.

Once all scopes are added, click Install to Workspace. Slack will ask you to confirm that the N8N3 app is allowed to access the workspace with these permissions. Click Allow.

At this point, Slack generates a Bot User OAuth Token. This token is extremely important—we’ll need it shortly inside N8N.

Next, we move to Event Subscriptions, which is the section right below OAuth & Permissions.

Event subscriptions define when Slack should call out to an external system—in our case, N8N—to notify it that something has happened.

Turn Enable Events on. Slack will ask for a Request URL, which is the URL Slack will call when an event occurs. We’ll come back to this later, once N8N gives us that URL.

Before that, we need to specify which events we care about. Under Subscribe to Bot Events, click Add Bot User Event and select app_mention. This means Slack will trigger events only when someone mentions the bot in a message.

At this point, we’ve enabled event subscriptions and defined the type of events we want Slack to send. The plumbing on the Slack side is partially in place.

Now it’s time to move over to N8N.

In N8N, click Create Workflow and add a first step. Search for Slack. We’re adding a trigger, and the trigger we want is Bot App Mention.

Select that trigger. Now we need credentials. Click Create New Credential. The first required field is the Access Token.

To get that token, go back to Slack. Navigate to your app, open OAuth & Permissions, and copy the Bot User OAuth Token.

Return to N8N, paste the token into the credential field, and click Save. If everything is correct, you’ll see Connection tested successfully, which is a very important confirmation.

There’s an optional field here for a Signing Secret, which Slack recommends for added security. You can find it under Basic Information in the Slack app settings. However, in my experience, using it caused technical issues, so I’m skipping it for now. You’re free to try it if you want, but be prepared to debug.

Now let’s finish configuring the Slack trigger.

The next step is selecting which channel the trigger should listen to. For that, we need to go back to Slack one more time.

In Slack, I create a new channel. I go to Channels, click Create Channel, and name it convo3 (since I’ve done this a few times already—you can call yours whatever you want). I leave it as a public channel and press Create.

Once the channel exists, I invite the bot into it by typing:
/invite @N8N3

Now the bot is a member of the channel.

Next, I need the channel ID, not the channel name. To find it, I click the three dots (More actions), choose Edit settings, scroll down to the About section, and there I see the Channel ID. I copy it to the clipboard. This step is easy to miss and took me a while to find initially.

At this point, the channel exists, the bot is in it, and we have the channel ID copied.

Now we go back to N8N again.

In the Slack trigger configuration, for Channel to Watch, select By ID, and paste the channel ID you just copied. This tells N8N exactly which Slack channel to listen to for bot mentions.

With that, the Slack trigger is now correctly configured.

There is still one more major step left, and it’s an important one—but that’s coming up next.

This part alone already shows why Slack is the most complex integration so far: OAuth scopes, tokens, event subscriptions, channel IDs, bot invitations, and back-and-forth setup between Slack and N8N.

And we’re not done yet.

# **W) Day 4 - Connect Slack to n8n Using OAuth2 and Webhook Triggers Step-by-Step**

At this point, you’re probably feeling like Slack is trying to personally hurt you. And honestly, that reaction is completely fair. We’re only halfway through, and this is already the hardest integration so far. But here’s the good news: this is about as hard as it gets. Once you’ve done a few integrations like this, you start seeing the same patterns again and again. You build a mental toolkit—OAuth, webhooks, scopes, triggers—and each new integration becomes easier.

Now we’re back in N8N, looking at the Slack trigger node, exactly where we left off. We’ve already configured the channel ID and most of the settings. But there’s one critical thing we intentionally left unfinished earlier.

Back in Slack, when we were setting up Event Subscriptions, we left the Request URL blank. This is the URL Slack needs in order to call N8N when a message arrives. To solve that, we need to use webhooks from N8N.

In the Slack trigger node, there’s a collapsed section called Webhook URLs and webhooks. These are the endpoints external systems can call to trigger this workflow. N8N conveniently gives us two URLs: one for testing and one for production.

We’ll start with the test webhook. Toggle to the test URL, click Copy, and it’s now in your clipboard. Before going back to Slack, there’s one very important step: press Execute Step on the Slack trigger node. This puts the workflow into listening mode, waiting for Slack to call that webhook.

Now we go back to Slack.

We open our app again and return to Event Subscriptions. At this point, I realize something important—I forgot to press Save Changes earlier. This is very easy to do in Slack, and it will absolutely break things if you miss it. So we carefully redo this section.

We turn Enable Events on. Under Subscribe to Bot Events, we add app_mention again. Then, in the Request URL field, we paste the test webhook URL we copied from N8N.

Immediately, Slack shows Verified. That only happens because N8N is actively listening. If you don’t see “Verified,” Slack will show “Not verified,” and you’ll have to retry—sometimes multiple times. Once it’s verified, we press Save Changes.

At this point, the Slack trigger is officially wired up. Slack can now send events into N8N.

Back in N8N, we continue building the workflow. As usual, we add an AI Agent node. By now, this should feel familiar. We select an OpenAI chat model. For now, we’re not adding memory or tools—we just want to complete the round trip.

Next, we add a Slack node on the output side. This time, it’s an action, not a trigger. We configure it to send a message. The resource is Message, the operation is Send, and we choose Send to Channel.

For the channel, we again select By ID and paste in the same channel ID we copied earlier from Slack. This ensures messages are sent back to the same channel.

For now, we keep the message text very simple. We leave it as a fixed string and just type “bananas”. This helps us verify the plumbing before adding expressions.

Now we test.

We press Execute Workflow so N8N starts listening. We go back to Slack, open the channel, and send a message mentioning the bot—for example, “Hi there.” We press Enter and wait.

The workflow runs. The Slack trigger fires. That’s a huge win already. If it didn’t fire for you, it means something earlier was misconfigured and needs careful checking.

However, the workflow errors with “No prompt specified”. This should look familiar. We saw this exact issue during the Telegram integration.

When we open the AI Agent node, we see what’s happening. The agent thinks it’s connected to a chat trigger and is looking for chatInput, but Slack doesn’t send data in that format. Instead, Slack sends a JSON payload.

So we fix it the same way as before. We change the prompt source to Define below, giving us full control over how we pass data to the model.

Instead of plucking out just one field like the message text, we take a different approach here. We decide to pass the entire Slack JSON payload to the AI and let the model interpret it.

We type a prompt like:

“Please respond to this message from Slack.”

Then we add a couple of blank lines. After that, we insert the full JSON payload.

If we simply insert $json, it doesn’t work properly—it shows up as a JavaScript object reference. To convert JSON into readable text, we wrap it using JSON.stringify($json). This converts the entire payload into a text string the model can read.

Now the AI receives the instruction plus the full Slack message context.

We test again.

We press Execute Workflow, go back to Slack, and send the same message again. This time, the workflow completes and sends a response—but it sends “bananas.”

That’s expected. The workflow works end-to-end, but the final message is still hardcoded.

Now we fix the last piece.

We open the Send Slack Message node. We switch the message text from Fixed to Expression. Instead of typing the expression manually, we simply drag the output field from the AI Agent node and drop it into the message field.

This automatically creates the expression {{$json.output}}, which pulls the AI’s response dynamically.

We test one more time.

Execute workflow. Go back to Slack. Send “Hi there.”
This time, Slack replies:

“Hi there. How can I assist you today?”

That’s it. We just completed a full OAuth-based Slack integration with a webhook trigger, AI processing, and message response.

Now there’s one final step: production deployment.

Up until now, everything has been running in test mode using the test webhook URL. To go live, we need to switch to the production webhook.

We open the Slack trigger node again and toggle from Test URL to Production URL. We copy the production webhook URL.

However, when we try to paste it into Slack immediately, Slack doesn’t verify it. That’s because the workflow hasn’t been published yet.

Before publishing, we optionally add one more node—like a Date & Time tool—to demonstrate tool usage. Then we press Publish. We give the version a name and confirm.

Now the workflow is running live.

We return to Slack, go back to Event Subscriptions, replace the old test URL with the production webhook URL, and Slack verifies it successfully. We press Save Changes (very important).

Finally, we test in production.

We send “Hi there” in Slack. No boxes light up in N8N anymore—because this is live. And the response comes back immediately.

We try another message: “What’s the current date?”
Slack replies with the correct date, proving the tool is being used correctly in production.

At this point, everything is working.

We’ve completed three integrations:

A simple one-way push with Pushover

A two-way integration with Telegram

A full OAuth-based, webhook-driven, production-grade Slack integration

Along the way, you learned OAuth scopes, webhooks, triggers, expressions, JSON handling, JSON.stringify, and environment separation between test and production.

If this felt intense, that’s normal. What matters is that you now recognize the patterns. From here on, integrations get easier—and much more fun.

Tomorrow, the real payoff begins: building an actual business project using these integrations.

You’re officially 27% of the way through building serious workflow expertise.
Take a breath. You earned it.

# **X) Day 5 - n8n JSON Workflow Tutorial: Webhooks, Authentication & Integration**

Look, I hear you.

Yesterday was grueling. We got through a lot. The integrations were perhaps slightly more than you bargained for, but we got it done. We’re through it now.

And today is redemption day.

Today is about putting everything into action. We’re going to build our first proper business project in n8n — the kind of thing you could realistically build for your own clients, or as part of creating an AI agency. This is where things really start to happen. This is where the commercial value lives.

So let’s get into it.

First, a quick recap.

One of the first things we covered yesterday was JSON. Most of you are probably already familiar with it, but if not, hopefully you developed some intuition. You’re going to be seeing a lot of JSON going forward, and it shouldn’t worry you in the least.

At its core, JSON is just a collection of key–value pairs. Something colon something, where those values can be strings, numbers, booleans, or — as you later discovered — lists, objects, or even null values.

JSON contains objects, which are also known as dictionaries, and these are represented with curly braces. Inside them, you have key–value pairs separated by commas. JSON can also contain arrays, which are represented with square brackets, and those arrays can contain objects. Objects can contain other objects, and you can compose larger structures from smaller ones made up of strings, numbers, lists, and more.

Before long, you get very used to seeing JSON everywhere — the curly braces, the structure, and the way it represents complex information as a hierarchy of smaller pieces.

That was JSON in four steps.

Next, we looked at expressions in n8n. Often when you’re filling in a value in a node, you can choose between a fixed value — a piece of static text — or an expression. An expression is dynamic: it gets evaluated at runtime and replaced with an actual value.

You can toggle between fixed values and expressions, and expressions are always surrounded by double curly braces. A single curly brace is just part of JSON syntax, but double curly braces indicate an expression.

We also introduced the $json shortcut, which means “grab the JSON output from the previous node.” We used this several times, and hopefully it’s starting to click. You can also use dot notation to drill into specific keys.

For example, if you have a JSON object representing a person named Alice, who is 30 years old and has an address object inside it, you could access the name with $json.name, or drill further into the address with $json.address.city to retrieve the city value.

We also used another important trick: if you don’t want the data from the immediately previous node, but instead want to reference data from an earlier node in the workflow, you can do that using $node. You specify the node name in square brackets, followed by .json, to access its output.

You don’t need to memorize this syntax. The important thing is to recognize it and understand what it’s doing. You can always look it up when you need it.

Another thing we covered was how to turn a blob of JSON into plain text so it can be passed into an LLM. For that, we used JSON.stringify. Again, no need to memorize it — just know that this option exists and that it’s useful when you want to feed structured data into a language model.

Then we moved on to authentication — how n8n connects to third-party systems and identifies itself.

There are a few different approaches here.

The simplest is using an API key. This is the ideal scenario. You just connect to a service like OpenAI, paste in your API key, get the green confirmation box, and you’re done. There’s no user-specific authentication involved.

The next level is OAuth 2 with preconfigured integrations. This is what we used with Google Sheets and Gmail. It’s still OAuth under the hood, but n8n has already done most of the heavy lifting. You click connect, a popup appears, you authenticate, approve access, and you’re done.

The hardest approach is full OAuth 2, which is what you need when building proper integrations with third-party platforms like Slack. In this case, you authenticate as a specific user or bot, configure scopes, manage permissions, and work with OAuth tokens.

With Slack, you also had to deal with webhooks. This isn’t strictly part of OAuth itself, but it’s another important integration concept, and we introduced it yesterday.

Webhooks are something many of you may have seen before, but for some this may be new.

Here’s the basic idea.

When we connect an action node in n8n to a third-party service like Slack, we typically call an API endpoint — a web address — using an HTTP request. This is a request–response interaction: n8n makes a request, Slack responds, and an action is performed, such as posting a message.

But there’s another type of interaction that isn’t request–response based.

Sometimes, we want Slack to notify n8n when something happens — for example, when a user sends a message. This is where webhooks come in.

In this case, n8n exposes a URL and tells Slack, “When this event happens, call this URL.” Slack then makes an HTTP request to that endpoint whenever the event occurs. That request triggers a workflow in n8n.

This exposed endpoint is called a webhook — essentially a hook that external systems can call to notify you that something has happened.

In our setup, we told Slack, “Here is the URL to call when a message comes in.” In n8n, we configured a trigger node that listens on that webhook URL. We then pasted that URL into Slack’s event subscription settings, telling Slack exactly when and how to notify us.

Slack doesn’t really know anything about n8n. All it knows is that when a particular event happens — like a bot mention — it should make a request to the URL we provided.

That’s why webhook nodes in n8n are trigger nodes. They exist to react to events that originate outside the system.

So, webhooks are simply endpoints that external systems call to notify you that something has happened. They make a web request to your endpoint, and that request triggers your workflow.

If any of this still feels a bit fuzzy, that’s completely fine. The important thing is to get the gist. We’ll be doing this over and over again, and each time it will become clearer. By the end of the three weeks, making HTTP requests and handling webhooks will feel completely normal.

These patterns are extremely common, and they’ll solidify with repetition.

Now, with that recap out of the way, there’s just a little bit of new material left — and then we finally get into the project.

# **Y) Day 5 - n8n Node Types Explained: Core Nodes, Subnodes, and Cluster Nodes**

I know how much you love terminology, so I’ve got a little more terminology for you. This is just to make sure you’re comfortable with the way n8n talks about nodes, and that the language you see in the docs feels familiar.

Nodes are the steps in a workflow. They’re the things we’ve been dropping onto the canvas. Nodes come in a few different flavors.

First, there are core nodes. These are the main building blocks you’re already familiar with — the things that appear directly on the canvas. Each core node carries out a node operation, which is one of two things: either a trigger that starts a workflow, or an action that represents a specific task.

When you select a new node, you press Tab to bring up the node picker. You choose a node type, like Slack, and then you choose an operation — either one of the available triggers or one of the actions. What appears on the canvas is the core node configured with that specific operation.

So that’s the distinction: a node is the block on the canvas, and a node operation is what that node actually does when it runs.

In addition to core nodes, there are sub-nodes. A sub-node is a smaller constituent part of a core node. It’s a step within a larger step.

A good example is a tool. You can’t drop a tool onto the canvas by itself. A tool must be associated with an AI agent, which means it only exists as part of something bigger. That’s the difference between a core node and a sub-node.

There’s one more term to introduce: cluster nodes.

A cluster node is a group of nodes that together make up a single logical step. It has a root node and one or more sub-nodes attached to it.

You’ve already seen an example of this. When you drop an AI agent onto the canvas, what appears isn’t just a single box. There’s the main agent node, plus required components like the model, and optional components like memory and tools. All of these hang off the main node.

That whole structure — the AI agent, its model, memory, and tools — is a cluster node.

It’s not absolutely critical to memorize these terms, but they do appear in the documentation, and it’s useful to know what people mean when they use them. We’ll refer back to this terminology when looking at diagrams later.

Now, there’s one important technicality I mentioned earlier that we need to cover.

When data flows through your workflow, nodes don’t work on a single object. They work on something called items.

You may remember that when we looked at the JSON coming into a node, it wasn’t just a JSON object. It had square brackets around it. It was an array — an array containing a single item.

That might have seemed odd. Why is it an array at all?

The reason is that what gets passed between nodes in n8n is always a list — an array of items. Each node is written as if it processes one item, but in reality it processes all of the items in the list.

For example, if you use an expression like $json.fruit.toUpperCase(), you write it as if you’re working with one item. But under the hood, that expression is applied to every item in the input array.

The output is another array, containing the transformed version of each input item. The top box would represent the input array, and the bottom box would represent the output array.

This is a bit of a technical detail, and it’s not super critical right now. The key idea is that in n8n, you’re often working with multiple items at once, even if it looks like you’re only handling one.

So far, we’ve only been dealing with workflows that pass a single item around, which is why this distinction hasn’t mattered much yet. But later, when we work with many items, this will become important.

Because of this, $json is actually a shortcut. The full version would be $input.item.json, meaning: take one item from the input, and access its JSON. The shortcut exists because this pattern is so common.

If you’re familiar with programming concepts, you can think of this as an implicit loop. n8n is effectively iterating over each item and applying your expression to all of them.

Again, this is just a technical clarification. You don’t need to master it right now — just be aware that this is how things work under the hood.

And with that, it’s time to move on to our commercial project.

We’re going to build an equity portfolio rebalancer.

This will be an AI agent that has access to a stock portfolio and can carry out instructions to rebalance it. It will be able to read a portfolio from a Google Sheet, look up equity prices, make rebalancing decisions, and then email the trades that need to be executed and send a push notification.

This project checks a lot of boxes. It uses multiple integrations, it has real-world relevance, and it’s something you could realistically build for a client. The goal is to introduce a few new ideas while reusing many of the integrations we’ve already set up.

You might want to try building this yourself first and see how far you get. If not, we’ll go through it together step by step. At the end, I’ll also leave you with some extra challenges to take it further.

All right — without further ado, let’s get into it.

Here we are in n8n. I’ve opened my instance and brought up the workflows screen. We’re going to press Create Workflow to start building our business workflow.

This is exactly what you would do if you were running an automation agency and building something for a client.

The first thing we’ll do is give it a name, because it’s always good to keep things organized. We’ll call it Equity Portfolio Rebalancer.

This is now our business workflow.

Next, we add a step. The first nodes we see are trigger nodes — nodes whose operation is to start a workflow. There are several options here.

We could run this on a schedule. We could trigger it from a chat message. We could trigger it with a webhook call, meaning any external system hitting a URL would start the workflow.

Instead, we’re going to use On Form Submission. This lets us create a simple web form that users can fill out. When the form is submitted, the workflow runs.

This is our first web form in the build.

Setting up the form is similar to tools like Google Forms or Typeform. You’re simply constructing a questionnaire for the user.

We’ll give the form a title: Portfolio Rebalancer. You can add a description if you want, but we’ll skip that.

Now we add an element to the form — the actual question. We could add many questions, but we’ll just add one.

The question is:
“How would you like to rebalance your portfolio?”

It’s a text field, and we’ll give it a default value:
“Ensure the portfolio is 60% equity and 40% fixed income.”

If you’re familiar with investing, this will make immediate sense. If not, don’t worry — this is the kind of instruction a real client might give, and our system needs to be able to handle it.

With that done, we press Escape and return to the canvas.

Now we have the start of our workflow, with a trigger node in place. The lightning bolt icon next to the node indicates that it’s a trigger operation.

And with that, we’re officially off to the races.

Next up, we’re going to look at the Google Sheet that will define our portfolio.

# **Z) Day 5 - Build an AI-Powered Portfolio Rebalancer Using N8N and Google Sheets**

Continuing with our business project.

So this is a look at a Google Sheet I've set up to represent my stock portfolio. The idea is that I will of course share this sheet. You can recreate it if you wish, or you can just use one that's set up. It's going to be set up with a bunch of stock tickers. Here they are. These are in fact in my portfolio. They are tickers that represent what's called ETFs, or they are sort of investments in a group of different shares.

Each one of them, I've got some quantity of them and they represent some of them are 100% equities—they are all stocks. Some of them are all bonds, all fixed income products, and some of them are a blend between the two. Columns C and D represent how much is stocks, how much is equity, and how much is fixed income. And that is the breakdown.

It’s common, particularly with retirement accounts, but also with longer-term savings and with many portfolios, that you have this breakdown between equity and fixed income. And it's common that you might want to rebalance it—and in fact, pay people to do this for you to take some cut of your money in return for rebalancing your portfolio. But there's no need, because an agent can do it for you, and it can do it for free.

That's what we are building today: our own financial planner that is able to rebalance the portfolio by making trades to set your balance of equity and fixed income to what you want, or really to do any kind of rebalancing that you might wish.

In this spreadsheet, there are a few sets of things that we're going to want our agents to fill in. One of them is this column here, “Price.” Price represents the current value of this equity in the market. We've already seen an agent fill in this column, and we'll be able to do that again. But this one is a new column. This is for the agent to fill in to say how many shares should we have—how much quantity of these different tickers after we've made some trades to rebalance the portfolio. It will fill in this column as it makes its decisions.

When it does that, there’s going to be a new total value coming up, and we will find out what kind of proportion between equity and fixed income it gets at the end of it. Of course, you can add in other kinds of metrics, like whether it's global or national investments, should you wish to rebalance in different ways.

That's the spreadsheet setup. Obviously, the price is all blank here. But if I put in a price like 100, then it fills in the value. And if I put in a new quantity after rebalancing of three, then the total value goes in there and the other things fill in. But we're going to leave it blank for now. This is a spreadsheet that we are going to be equipping our agent with, and it will operate on this.

Let's go and do that.

We go back to Nw10. Here it is. We are going to add in an agent and let's take a look at it. There it is. We've got lots to do with this AI agent. We're going to give it a chat model. I'm going to use OpenAI—you can use whatever chat model you wish. We are going to choose GPT 4.1.

The reason for that is that I want it to be a really smart model that does it well. You can choose what you want to use. It costs a few cents a pop with GPT 4.1, and of course, it's a fraction of that with GPT 4.1 mini. You should go with whatever you're comfortable with. I do find that the latest models like GPT 5.2 for me right now actually don't perform as well because they do too much reasoning and take too long. The outcomes for me are better just using GPT 4.1, which for this kind of thing is one of my favorites.

So I'm selecting GPT 4.1, but of course you can do anything over OpenRouter. You could be using Gemini—use whatever you want. Back we go. I'll press the tidy up button to make it a bit cleaner. And we will then keep going with some tools.

We're skipping memory—we don't need memory for this because it's just on a form submission. There's no chat history to have to remember. We go straight to tools, and we're going to go to Sheets—of course, Google Sheets tool. It's already set up with the right credentials. We know we can connect. Fine.

We're doing a sheet with a document. We want to get rows. This is going to be the way that we fetch the contents of our portfolio. The documents we're going to get is going to be from a list. We're going to choose the portfolio, and the sheet is also going to be from a list. We're going to choose sheet number one.

So I chose portfolio from the list here. And I'm now choosing sheet one right here. I'm not going to set any filters—I want it to be able to retrieve the whole thing. Okay. That has set up our Google Sheets access. Let's press the clean button.

Now we will give this a try and see what happens. Hopefully you're expecting something to go wrong. Let's see. Okay, I'm now going to press Execute Workflow and bam—up pops this window. It's a form. It says "Portfolio Rebalancer." It's the very form that we just described. It has the one question that is the field we put in there: How would you like to rebalance your portfolio? The default answer is 60/40. I'm going to press Submit.

Pressing Submit is basically sending an HTTP request which Nw10 has configured as a webhook. It is expecting this to come through, and bam—it has come through. If I click back here, you'll see that the workflow has run and it says there's a problem, just as we were expecting. Or are you expecting? I hope you're expecting.

Now let's go in and fix that problem. You'll see the error message tells us "No prompt specified." You'll recognize that because we've had it before. If you open up AI agent, we'll see more details that it was trying to bring in JSON chat input. It thinks it's connected to a chat trigger node, and it's not—it's connected to something very different.

The first thing we need to do is to change this from saying the source for the prompt is a chat trigger node to "Define." Below, we want to tell it how to look in the inputs and choose what should be the prompt to the LM. This is the input. This is the webhook that got called. As a side note, notice that it is of course an array of items. As I explained, what really gets moved between these nodes are these arrays of items—but there's only one. It's the submission of our web form, and it only has this one question: How would you like to rebalance your portfolio? This is what we want to be sent to the LM.

So let's drag and drop that in there. Let go. You'll see that it's put in there the right JSON to use. It then selects How would you like to rebalance your portfolio. That's working nicely. It's going to take this content and make it the prompt to the LM.

Going back to the canvas, I'm pressing Execute Workflow. Up comes this pop-up, and I'm going to submit it. This time it will hopefully be sending this exact instruction to the agent when this goes through.

What I think we'll find is something a bit curious happens, because we've not done a very good job of telling the agent what we want it to do. We've just said ensure the portfolio is 60% equity and 40% fixed income, but we haven't told it that the portfolio is in this Google Sheet. We haven't really given it good directions.

This used to be called prompt engineering. We've not done a good job of prompt engineering. The new expression is context engineering, which is the way to best position your whole agentic workflow to equip your agent with the best possible information. We've not done a good job of any of that.

Let's see what it says back now that it's just successfully run. You can see that in the output it said to ensure a portfolio is allocated 60% equity and 40% and follow these instructions. It has given us some sort of bland, generic content about what to do. It never used the tool because it didn't know that the tool was going to tell it our portfolio. It just gave back some random instructions.

We need to improve the prompt so that it knows what it has to do, and we also need to give it a few more tools so it can do it. And that's what we will come to next.

# **AA) Day 5 - Agentic AI Workflow Automation: Balance Autonomy and Instructions**

So this whole week is about automation—automation of business processes. A big part of automation when you're using AI agents is positioning an AI agent so it can carry out your business process in the best possible way. A lot of that comes down to what they once called prompt engineering, and is now, of course, context engineering. For the time being, we're just going to do pretty simple prompting stuff.

I'm going to double-click on the AI agent and up it comes. This, you'll remember, is the user message. We're taking the stuff that's coming in here and we're just shoving it into the prompt. The thing about these expressions is that you can mix and match. You can put normal English above this. When we've got this expression right here, you can see what the result will be.

Well, look at what I can do. I can put a couple of lines up here and I can say “bananas,” and then this is what the result will be—bananas. That wouldn't be very helpful for our agent, but maybe this would be more helpful. I just pasted in something I wrote earlier: You have access to the user's equity portfolio, which is in a Google Sheet. You also have access to market data. The user has asked you to rebalance their portfolio with this instruction. And then I've got the expression.

When we look at the results in the field immediately below, look what's happened. We've got the text I just typed, and then we've got the actual contents of this instruction. It's kind of obvious when you see it. So that's a really nice way to make a prompt and add in the user's input as well.

But wait, there's more. Right underneath this prompt, I'm going to put some more stuff as well. You could put “bananas” here too at the end, but that’s not very helpful. Instead, I’m going to paste something more wordy. Let me read this to you. I'm saying, in order to achieve this:

Read the portfolio from the sheet.

Fetch the latest prices of the positions.

Update the table with the prices.

Make decisions on portfolio rebalancing.

Update the table with the portfolio rebalancing decisions.

Read the table again to check that your rebalancing met the objectives. Make any further changes.

Iterate if necessary.

Finally, send an email with the trading decisions and a concise push notification.

You can see the tools I’m about to add.

So, what do you make of this? Have a think about what I've done here and what might be your counterpoint. What might feel a bit funny about what I've done just here? Well, here's the thing. If you were the challenging sort, which I know you're not, you might say to me: “Look, you've kind of written out a workflow there. You've got steps, even numbered steps. Why would we need an AI for this? If we've got steps to do, we could just lay out a workflow. We could use workflow tools, or there are a lot of other simplistic non-AI workflow products where you can just do one step, then another step, and another step. Surely the whole point of agentic AI is that you're just giving it a goal and letting it figure it out.”

To which I would say: yes, you're right, very good challenge. But these are quite loosey-goosey steps. These are not steps that would be easy to turn into code. Look at this step: “Read the table again to check you achieved the objectives and make any further changes to improve the rebalancing.” This is kind of hand-wavy, rough human-like instructions—like you might tell a person. You’re not just giving one very high-level goal, rebalance the portfolio. You're giving it some guidance so it knows how to do it, but only in a high-level, flexible way.

That gives it some autonomy, as I like to say, to decide exactly how to achieve this, how many iterations to do, how to read the spreadsheet. We're leaving a lot up to the agent. This is the kind of balancing act that is all about getting the most ability, the highest performance out of agentic AI. It's about figuring out how to keep the instructions high-level, so that it has the flexibility to be intelligent and nuanced about your instructions, while also giving it enough rails so that it sticks to what you want it to do.

There's something of an art to this, and the best way to get it right is by experimenting. There's no right answer—the best way is to try. Originally, when I first built this, I did just have a very simple objective: rebalance the portfolio. I found that it wasn't reliable. I got to this point with some iteration, and that's what you have to do too. I certainly encourage you to experiment with different levels of detail and find out what balance works best. Each model will have a slightly different balancing point, and you may find that if you're using Gemini, it’s able to work with higher-level instructions and be very reliable. So best thing to do is experiment. I wanted to give you that understanding of why one does it this way and how I came up with these steps.

Okay, onwards. To achieve all of this, we now need to add in some tools. Have you been paying attention to what we need to do? Let's go back. Maybe you could have a shot at this before I do it with you. Let’s move, give ourselves some space, and add in the first tool.

What should we do? Well, let's go for MarketStack. Add in MarketStack. Here it is. We're going to get end-of-day data. We want to get many, we let the model choose which ticker. And we just say here we have to add a filter and say “latest.” That is all set. That is MarketStack configured.

I should confess that you get 100 free lookups, and I exceeded my 100 free lookups, so I upgraded to the paid plan. You should be able to stay well within the 100 lookups for this project, but if you experiment like I did, you might go above that number. You can choose what to do, but I decided it was worth it. That is the next tool: MarketStack, one back to an old friend added to our agent.

Two more tools. Now they are both about updating the sheet, updating the Google Sheet. There are two ways in which the sheets might need to get updated, so let's do them right now. This is going to be using Google Sheets. This time we want it to be an “update row.” We're going to update that same spreadsheet so it can update the prices. This tool will allow it to update the prices.

I've selected my Google Doc “Portfolio” from the documents, for the sheet I select Sheet 1, which I think is the only sheet in that document. The document looks like this: it has a Price column and a New Quantity After Rebalancing column. What I'm doing now is setting it so that it can filter on the ticker and set the price.

That is one of our tools added. Now we're going to do exactly the same thing. Another tool, also Google Sheets, this time allowing it to change the portfolio—the rebalancing decisions will be added. Update row again. I’ll choose the document and set it up.

I’ve chosen “Portfolio,” Sheet 1. This time we’ll filter on the ticker, and we’ll change the column “New Quantity After Rebalancing.” Still fetching the columns. Let it finish. On week three, we’ll be self-hosting, so we’ll be able to run at our computer’s speed.

Finally, it says “select column to match on.” We say the ticker, using that to match, and set the New Quantity After Rebalancing. That is our other tool.

Let’s take stock. We have tools for our agents: a tool to get the contents of the sheet, look up market data, update rows in the sheet for pricing, and update rows in the sheet for rebalancing (the New Quantity After Rebalancing column). This is all pretty exciting. One more tool—do we have any more tools? Let’s go and do those final tools.

# **AB) Day 5 - How to Integrate Gmail and Pushover Notifications with n8n AI Agent**

So, yes, as well, we have the tools left to put on our communications tools—to send a push notification and an email. You now have a whole bunch of these tools in your toolbox, and you should feel free to use any of them. If you would rather have it send decisions via Slack or Telegram, then go for it. But we're going to stick with the ones I picked, which is my favorite, Pushover.

Let's add in Pushover. The Pushover tool will send a push notification. We’ll let the model decide what message to send us; that becomes part of this tool. We have to put in our user key here—the one that begins with the letter “U.” Obviously, rather than reveal it publicly, I just copy and paste it in. I also changed the priority to high because I needed it to make a noise on my phone.

Now the email tool. I press Tools and go to Gmail, consuming the Gmail API. This time, I’ll have it actually send an email for real. Before, I was a bit hesitant because I didn’t want to give it too much power, but this time we’re going to fix that. The email will be sent to me, using my Gmail address, with a fixed subject: Equity Rebalancer Trading Decisions. The message itself will be defined by the model, and it will be sent as an HTML email. That tool is now set.

Next, I press the clean-up button to make everything nice and tidy. I also zoom out a bit to get a better view. Now we’ve got everything laid out and are almost ready to go. There’s one more thing we can tweak to make it better. Over in the portfolio sheet, I added another row showing the proportion of fixed income. Now we can see both equity and fixed income breakdowns. For example, if I put in 100 as the price for everything, the portfolio shows a 60/40 split. After rebalancing, the portfolio makeup changes based on the new quantities. This improves the clarity of the spreadsheet.

Along with that, there’s a change we can make to the workflow. In the tool that gets rows from the Google Sheet, there’s a field called Tool Description that we’ve left at its default setting. By default, it just says “Get rows in sheet in Google Sheets,” which is what the LM sees when deciding whether to use this tool. We can do better. I pasted in a description I wrote earlier: This tool provides details of the user's portfolio in a Google Sheet. It includes the positions and the equity/fixed income breakdown. Use this tool to retrieve the Google Sheet before updating prices and making rebalancing decisions. You must also use this tool after rebalancing to check if the breakdown achieves your goal. If not, keep iterating. This makes the instructions very clear for the LM and is part of good context engineering.

I also updated the AI agent’s user message prompt, adding a note: Important: you must confirm you’ve achieved your goal. Only respond “OK” when your mission is complete. This helps ensure that the agent only signals completion once everything is done.

One more adjustment is the maximum iterations. This controls how many times the agent can use tools before stopping. The default is 10, which may not be enough, so I bumped it up to 30. That gives the agent plenty of iterations to think through what it needs to do.

Without further ado, let’s give this a try. We check that our portfolio is in good shape and press Execute Workflow. This is a classic example of automation. We’re taking something manually intensive and running an agent workflow to handle it, automating a process that currently requires a lot of manual work.

The agent is now running. You can see it’s thinking about what to do. On the executions tab, we can track a detailed trace of what’s happening. It’s already set all the prices and is now thinking about the rebalancing. The workflow updates the rows and adjusts quantities to achieve the target 60/40 split. Meanwhile, I received the push notification on my phone: Portfolio rebalanced to 60/40. Everything ran smoothly and faster than the traces could show.

Looking at the executions page, we see that all tasks were completed successfully. The OpenAI chat model was called ten times. The sheet was accessed four times, MarketStack was called four times for share prices, and the two update-row tools were used multiple times—once to set prices, then again to rebalance, and finally to adjust quantities to achieve the exact target. The push notification and email were sent, and everything was executed in a single workflow.

The email arrived with the rebalancing instructions, confirming that the workflow worked as expected. This shows how sophisticated this agentic workflow can be. We achieved a 60/40 split, updated quantities accurately, and kept the total portfolio value consistent.

Finally, a little more about the executions page. You can see a visual trace of everything running with numbers. Clicking Logs shows all the calls and actions, and switching to the Details tab shows exactly what happened at each point—inputs, outputs, and prompts. For example, it fetched a row, made adjustments to achieve the 60/40 split, and even made a final adjustment to the BND ticker. This is a very powerful debugging tool. If the workflow doesn’t behave as expected, you can review the logs, tweak prompts, and iteratively improve performance until it reliably achieves the desired outcomes.

# **AC) Day 5 - n8n Workflow Automation: Using If Nodes for Conditional Logic**

Okay, so I want to add one more element to our workflow—some traditional workflow logic. One of the great things about Nan is that it combines the best of agentic AI with traditional workflow tools, similar to Zapier, allowing API integration with visual workflow capabilities. That’s what we’re going to do here.

The AI agent’s output is supposed to be the word “okay”. This is something we can test for. I want to set it up so that if things go wrong and it doesn’t achieve the expected outcome, it will alert me with a push notification marked as a warning. If it goes right, I also want a push notification—but just a normal, non-emergency one. Essentially, we want two different notifications: one for problems, and one for “all is well,” chosen depending on whether the AI agent’s output is “okay” or not.

This sounds like it might need code—an if statement—but there’s also a simpler, low-code way using an if node. The if node allows us to visually lay out simple logic: if the agent did what it was told and answered “okay”, we’ll send a happy push notification. Otherwise, we’ll send an upset push notification.

To do this, I pressed the plus button and typed if to add an if node. The node routes items to different branches based on a condition. For value one, I used an expression: $JSON.output, which is the output field from the LM. If this is equal to the fixed text “okay”, the node considers it true. This gives us two branches: true and false.

On the true branch, we add a Pushover node to push a message. I pasted in my user key and set the priority to low, with the message “Rebalancing successful”. On the false branch, we also use Pushover, with the user key, but this time set it to emergency priority, with the message “Rebalancer failed”. After setting the user tokens, the workflow had no errors, and everything was ready.

Here’s an important distinction: the Pushover node used here is a core node in the workflow, not a tool called by the LM. As a core node, it’s always executed when the workflow reaches it, independent of the LM. When a node is used as a tool, it’s called at the LM’s discretion, and the LM can decide whether or not to call it and can even define some inputs dynamically. That’s the key difference between a tool node and a core action node.

With that done, I cleared the portfolio sheet and executed the workflow again. The workflow triggered successfully. The AI agent ran, rebalanced the portfolio, and I received two push notifications: one saying “Portfolio is rebalanced”, and another saying “Rebalancing successful”. The if node worked perfectly, routing to the correct branch and sending the appropriate message. This shows how you can add logic to your workflow to enforce stricter business processes.

The final step is deploying to production. I copied the production URL from the form submission, ensured everything was saved, and pressed the publish button. This made the workflow live. With an empty portfolio, I submitted the form, triggering the workflow. The agent immediately ran, set prices, and started rebalancing. I received push notifications as expected, and the email with portfolio rebalancing instructions arrived successfully.

The executions and logs confirmed that the workflow ran correctly. You can even see how many tokens were used by the AI agent, which is helpful for context engineering. One limitation to note: with larger portfolios, the agent can start losing coherence. Solutions include providing more information in the sheet, equipping the agent with a calculator for computations, or improving the tools to guide rebalancing decisions. The mission is to make this workflow more reliable, industrial-strength, and autonomous while still keeping it simple enough for the AI agent to make quick, accurate decisions and notify you.

This completes our deployed workflow. It’s a real-world example of automating a business process. You could even extend this trigger from a web form to a scheduled task, letting it run automatically once a day and notifying you of portfolio rebalancing decisions. That is a fitting conclusion to our first week of the program: automating workflows with AI agents.

Congratulations—you’re a third of the way through the program! Next week, we’ll explore voice agents and 11 Labs, which is going to be a lot of fun. But for now, take a moment to celebrate the progress you’ve made.

# **II) Section 2: Week 2: Accelerate With Voice Agents And RAG**

# **A) Day 1 - How to Build AI Voice Agents with ElevenLabs Agent Platform**

Well, I’m really happy you’re back for more. Last week was quite intense—we covered a lot of material and ended up with a pretty substantial project by the end. There was a lot going on, and I hope you enjoyed working through it. Now, though, we have a couple of really fun days ahead.

This is Week Two, Day One of the Agentic AI Builder program, focused on AI agents and voice agents in n8n. For today, however, we’re not going to touch n8n at all. I’m very excited to welcome you to ElevenLabs Day. Today is a purple day, which means it’s all about core skills—specifically, building new expertise in the area of voice agents.

Last week was all about automation. This week is called Accelerate, because the focus is on using the latest frontier technologies to do more—and do it faster. ElevenLabs is a perfect example of a product that enables acceleration. It’s somewhat magical, honestly, and I hope you feel the same way once you see it in action.

ElevenLabs is both a company and a product, as is often the case in tech. It was founded very recently, in 2022, and it’s already a unicorn company. It’s widely considered the leading company in audio generation, especially when it comes to quality. While it is more expensive than many alternatives—including free and open-source options—you truly get what you pay for. In terms of audio realism and quality, ElevenLabs is considered top shelf.

The company was founded by a former Googler along with someone from Palantir, who came together to build the business. On the website, ElevenLabs positions itself around two main sibling products. The first is the Creative Platform, which focuses on generating audio and building audio avatars. The second is the Agents Platform—and you can probably guess which one we’re going to use. Most of our time will be spent in the Agents Platform.

In many ways, the Agents Platform feels a bit like n8n itself. You’ll notice some familiar canvas-style layouts and workflows, but it’s highly specialized for voice agents. As you’ve already seen in the curriculum, we’ll be building workflows that connect ElevenLabs and n8n, and we’ll make decisions about which responsibilities belong to ElevenLabs and which belong to n8n.

Regarding pricing, as I mentioned earlier, ElevenLabs is on the pricier side—but it does offer a free tier. You can follow along with this entire course without paying anything. I’ll admit that I personally signed up for the Starter plan at $5 per month, because I’ve been using it quite a bit and really enjoying it. That said, it’s absolutely not required—you can stick with the free tier, which is quite generous.

Let’s take a look at the product itself. If you go to elevenlabs.io (one word), you’ll see the tagline: “The world’s most realistic voice AI platform.” Right at the top, you’ll notice two navigation options: Creative Platform and Agents Platform—the two platforms I mentioned earlier. Each has its own set of use cases and documentation.

There’s also a dedicated Developers section that includes documentation and API references for both platforms, side by side. If you click on Pricing, you’ll see the free tier along with paid plans like Starter, Creator, and Pro. Pricing may vary by region and can change over time, so what you see might not exactly match my screen, but hopefully the free tier is still available—and maybe even more generous.

On the homepage, there’s an instant demo where you can press play and hear a sample voice narration. The quality is genuinely impressive. You might think it’s cherry-picked because it’s on the landing page, but ElevenLabs really is that good—you’ll see it for yourself once you start using it.

The Creative Platform offers things like text-to-speech, audiobook creation, and even music generation. The Agents Platform, on the other hand, is where you build voice agents, which is exactly what we’ll be focusing on. You can also explore voice replication examples—like Michael Caine or Matthew McConaughey—though using those yourself typically requires a paid plan.

Overall, ElevenLabs is an industry leader in audio quality. It’s not the cheapest option, but the quality more than justifies the price, and the free tier is sufficient for learning and experimentation.

To get started, click Sign Up. During the signup flow, you’ll be asked which platform you want to use—choose the Agents Platform. You can switch platforms later, so you’re not locked in, but this is the right place to start. You’ll likely be prompted to create an agent immediately—just choose the Start from Scratch option. The process is straightforward.

Once logged in, you’ll see a dashboard. On the left side, there’s a toggle that lets you switch between the Creative Platform and the Agents Platform. While the Creative Platform includes features like speech, audiobooks, video, and music generation, we’ll be staying firmly within the Agents Platform for now.

Inside the Agents Platform, the main navigation is divided into Configure, Monitor, and Deploy. We’ll start by clicking Configure Agents. You may see an existing test agent if you’ve already experimented. Other sections, like Tools, will become clearer as we progress.

For now, we’ll begin our first experiment by clicking the New Agent button.

# **B) Day 1 - Build Your First AI Voice Agent with ElevenLabs Conversational AI**

Okay, here we go.

To begin, I click New Agent. The interface asks, “What type of agent would you like to create?” I select Blank Agent. Next, it asks for a name that reflects the agent’s purpose. I’ll call it First Agent. I keep Chat Only turned off and then press Create Agent.

The agent is created, and the configuration screen appears. The first thing we see is the system prompt, which defaults to “You are a helpful assistant.” You already know what that means—it sets the overall behavior and personality of the agent.

Below that is the first message, which starts with: “Hello. How can I help you today?” You’ll notice a note explaining that you can add variables using double curly braces, which should feel very familiar if you’ve used expressions in n8n.

The default voice selected is Eric, described as smooth and trustworthy. We’ll be the judge of that. The language is set to English, and we can also choose which LLM model to use. By default, it’s Gemini 2.5 Flash.

If we open the model selector, we can see additional options, including experimental GPT-4o-style models. There’s also Gemini 3 Pro, but notably not Gemini 3 Flash, which makes sense because that model was only released very recently. For now, we’ll stick with the default setup.

At this point, everything is configured, so it’s time to try it out.

I click Preview to run the agent. An avatar appears along with a “cool AI agent” button. Let’s give it a shot.

The agent greets me: “Hello. How can I help you today?”
I respond, “Well, hello there.”
The agent replies smoothly and naturally, asking how it can help.

I ask for a fun fact, and it tells me that a group of flamingos is called a flamboyance—a delightful and fitting name. I then ask for a joke about flamingos, and it delivers one with good timing and humor. After thanking it, the agent politely asks if there’s anything else it can help with.

That interaction is genuinely impressive. Of course, credit goes to Gemini 2.5 Flash for the intelligence behind the responses, but ElevenLabs deserves real credit for how fast, seamless, and natural the voice interaction feels. It genuinely feels like speaking to a real voice agent, and the frictionless experience is fantastic.

Next, let’s take it a bit further and try a business-style conversation.

I update the system prompt so that the agent is now a helpful assistant for an airline. The agent assists customers with their travel needs. I also provide some reference information: a return ticket from New York to London costs $599.

Now that we’ve added context, let’s change the voice. We browse the available options:

Miss Walker – warm, reassuring, and round

Jason – calm and soothing

Stokes – relaxing, casual, and warm

We decide to go with Miss Walker.

With that done, I press Preview again to test the agent.

The agent greets me, and I say I’d like to go on a trip. It asks for my destination and travel dates. I tell it I want to travel from New York to London next week. The agent immediately references the price we included earlier—$599—and asks if I’d like it to check availability.

This confirms that the agent is successfully using the context and information provided in the system prompt. It’s relatively simple functionality, but what stands out is how quickly and easily it’s put into action.

Now, let’s explore some of the more advanced features of ElevenLabs by navigating through the tabs.

We start with Workflow, which is marked as new. This should look familiar—it closely resembles the n8n canvas. You can add nodes, connect agents, and even use tools to route conversations between different agents.

That said, we’re not going to spend much time here. We’ll be using n8n for workflows instead, since we already know it well and want to invest our effort there. This workflow tool in ElevenLabs is still somewhat raw, as indicated by its “new” label, but you’re welcome to experiment with it.

If we click Templates, we can choose something like Qualification Flow. This template shows an agent whose job is to decide whether a user has a technical issue or a billing issue. Based on that decision, the LLM routes the conversation to a different agent using a tool.

Each agent has its own prompt and can respond differently depending on the issue type. This allows you to set up multiple voice agents and route conversations dynamically based on conditions. It’s fairly self-explanatory, so we’ll move on for now.

Next is Branches, where you can create and test different versions of your agent.

Then we come to Knowledge Base, which is where things start to get really interesting. This is where you provide your agent with background knowledge or subject matter expertise.

Let’s try it out. I click Add Document and choose Create Text. I name the document Apple Product Details. I then paste in a chunk of text that ChatGPT generated for me—summarized information about Apple products based on their website.

Once I click Create Text, the document is added to the agent’s knowledge base.

Now we go back to the agent and update the system prompt to say: “You are able to answer questions on Apple products.”

Let’s test this. We’ll switch voices again and choose Jason, described as calm, meditative, and soothing.

I press Preview and start the conversation. I ask about buying an Apple Watch, specifically asking for the latest version and its price. The agent responds accurately, referencing the Apple Watch Series 11, its approximate price, and also mentioning other models like the SE and Ultra versions.

This confirms that the agent is successfully consulting its knowledge base to answer questions.

This is our first real glimpse of RAG (Retrieval-Augmented Generation). Later this week, we’ll build this in a much more robust, industrial-strength way using n8n, and we’ll connect that system to ElevenLabs. But even here, you can see how easy it is to give an agent domain expertise just by uploading documents through the UI.

And with that, we’ll leave it here.

You should absolutely give this a try yourself.

# **C) Day 1 - ElevenLabs Voice Agent Tools: Deploy Conversational AI with Widgets**

Okay, moving on through the remaining features of ElevenLabs using the main navigation.

The Analysis section is where you can review previous conversations that have taken place with your agent. You can see the history of interactions and confirm that they’ve all run successfully. This is useful for validating behavior and checking how your agent has responded over time.

Next is Tools, which is exactly what it sounds like. This is where you equip your agent with tools, very much like you would in n8n. This section is particularly important because it’s one of the two ways we can connect an ElevenLabs agent to n8n. In practice, one of these tools can be configured to call out to an n8n workflow. We’ll be doing this tomorrow when we add our first custom tool.

There are also system tools—built-in tools that you can enable with a simple toggle. These allow the agent to do things like end a conversation gracefully instead of continuing indefinitely. There’s also a custom button option that allows you to surface additional information to the user, as well as a detect language tool, which lets the agent automatically identify the language being spoken and continue the conversation accordingly.

One particularly important tool is Transfer to Agent. This is how you can build workflows across multiple agents. You define a description that tells the model when it should transfer control, specify the conditions under which the transfer should happen, and define the transfer message. Essentially, one LLM decides that it’s time to hand off control to another agent with a different prompt or responsibility. This is the core mechanism for chaining agents together using tools.

You can achieve similar results using the newer Workflow feature or by selecting a template, which is essentially a shortcut for configuring all these tools automatically.

That covers Tools.

Next is Tests, which is where you can define different test scenarios and review past runs. This helps you validate agent behavior under various conditions.

The Widget section is a very important one. This is where you configure how your agent actually appears to users. You may remember the nice blue avatar you saw earlier—this is called the orb. This is where you customize it.

That said, you’re not limited to the orb. You can upload an image instead—perhaps a photo of yourself or a brand mascot. You can also provide a URL pointing to an image if you already have one hosted elsewhere.

Crucially, this section also gives you an embed code. You can paste this directly into a webpage to surface your agent there. For example, if you’re using WordPress, you can embed this as a script tag, and your agent will appear directly on your site. The same applies to any platform where you can deploy HTML. If you’re working with a technical team, you can simply hand them this snippet, and they can add it to your website. Once embedded, the blue orb will appear and connect directly to this agent. It really is that simple.

If you scroll further down, you’ll find extensive configuration options for styling and coordinating the avatar, as well as adding more details to the widget itself. There’s a lot here that can be customized without needing to be particularly technical.

You can then share the widget wherever you like. There’s also a newer Widgets V2 option, which I haven’t personally used yet. This version is more sophisticated and supports headers, lists, code blocks, and additional layout controls, giving you even more flexibility over how the widget is embedded and displayed.

Overall, this is a major step forward—it allows you to embed a live, interactive voice agent directly into your website with minimal effort.

Next is Security, which includes various configuration options. You can require authentication so that users must log in before interacting with your agent. You can also control what users are allowed to do—for example, whether they can switch to text-only mode instead of voice. By default, this option allows users to choose whether they want to type instead of speaking, but you can restrict or expand this flexibility as needed.

There’s also support for webhooks, which ElevenLabs can call at the beginning and end of a conversation. By now, you know exactly what webhooks are and how they work. This allows ElevenLabs to call out to another system via a URL you provide. You can also set usage limits, such as the maximum number of calls per day for a particular agent, to ensure things don’t go off the rails.

Finally, there’s the Advanced tab, which contains additional settings. You can configure the agent to be text-only if needed. There’s also a newer feature called Scribe for automatic speech recognition.

You can define keywords that are likely to come up in conversations—such as product names like Apple Watch. This is especially useful for unusual or branded terms, as it increases the likelihood that the speech recognition system correctly identifies them.

There are also parameters that control the personality and behavior of the agent—how quickly it jumps into the conversation, how long it waits before asking something like “Are you still there?”, and how it handles silence. I believe I adjusted these from the default at one point because the agent was responding too quickly. You can experiment with these settings to find what feels most natural.

You can also configure the agent to automatically end the conversation after a period of silence, along with a few other behavioral tweaks.

At this point, I strongly encourage you to explore these settings yourself. Play around with the fields, press the Preview button frequently, and get a feel for how everything works. Definitely experiment with the Knowledge Base as well.

ElevenLabs makes building voice agents incredibly simple—and this hands-on experimentation is the best way to really understand it.

# **D) Day 1 - Building Multi-Agent Workflows with ElevenLabs Voice AI Agents**

Okay, we’ll bring this home by building a quick agent workflow to solve a small business-style problem. Nothing too serious—just a toy example to show what’s possible.

We’ll start by going to Knowledge Bases and adding another document. This time, we’ll create a new text document and call it Stock. This document represents what the store currently has available. For our example, the stock will include a few Apple products: an Apple Watch Series 11, an iPhone 13 Pro Max, and a MacBook Pro M4. Of course, in a real scenario this could be a much longer and more detailed document. Once that’s done, we create it as another knowledge base.

Now we’re going to set up a small agent workflow using the Workflow feature. To get started quickly, we’ll pick a template—specifically, the Qualification Flow template that we looked at earlier. This gives us a solid starting point for our experiment.

Let’s begin editing the workflow. The first agent is called the Qualification Agent. We’ll repurpose this to act as the main agent, the one that kicks off the conversation. We’ll keep things simple. The conversation goal (the prompt) will be: “You’re a customer support representative at a store that sells Apple products. You direct the user’s question as appropriate.”

We turn on Override Prompt, which means this prompt completely replaces anything defined in the agent tab. For the voice, we’ll choose Miss Walker to answer the call. We also make sure that this agent does not inherit any knowledge base, meaning she comes into the conversation without product or stock knowledge. That’s intentional—her job is just to route the conversation.

That completes the setup for the first agent.

Next, we move on to what was originally called Technical Support. We’ll rename this agent to Product Support, which is more accurate. The prompt here will be: “You’re able to answer general questions about Apple products.” We again enable Override Prompt.

For the voice, we’ll choose someone new—Russell, who’s described as outgoing and excited. This agent will need access to product knowledge, so we disable inheritance and explicitly attach the Apple Product Details document to this agent’s knowledge base. Now the Product Support agent has both the right prompt and the right information.

With that done, we move to the final agent, which was originally Billing Support. We’ll rename this one to Stock Specialist. This agent’s job is to answer questions about what the store currently has in stock. The prompt reflects that purpose, and we again override the previous prompt.

For the voice, we’ll choose Stokes. This agent also needs a knowledge base, so we disable inheritance and attach the Stock document we created earlier. At this point, the Stock Specialist is fully configured.

The last thing we need to do is define the edges—these are the conditions that determine how the main agent routes the conversation.

We select the first edge, which is an LLM condition. It’s currently labeled “Technical Issue,” so we change that to Product Question. The condition becomes: “The user has a general question about Apple products.”

Next, we select the second edge, also an LLM condition. We rename it from “Billing Issue” to Stock Question. The condition is: “The user has a question about what Apple products the store has in stock.” These conditions are clear and unambiguous, which is exactly what we want.

With everything set up, we’re ready to test the workflow.

We press Preview and start the AI agent. The main agent answers the call and greets the user. We ask about the latest model of the Apple Watch. The system correctly routes the conversation to the Product Support agent, who answers using the Apple Product Details knowledge base. The response is accurate and natural, and once the interaction is complete, the conversation terminates cleanly. We rate the conversation as very good.

Let’s try a second test, this time triggering a different route.

We start again and ask what models of the MacBook Pro are in stock. This time, the conversation is correctly routed to the Stock Specialist, who consults the stock knowledge base and confirms that the MacBook Pro M4 is available. Another successful test run.

What you’ve just seen is a simple but powerful example of intent-based routing. The user asks a question once, and that question is routed seamlessly to the correct specialist—there’s no need for repetition. In a more advanced setup, you could add additional edges to allow conversations to bounce back and forth between agents as needed.

We’re not going to dive deeper into this tool, because our real focus will be on connecting voice agents to n8n. However, it’s important to know that if you have simpler use cases—such as a customer support agent with a knowledge base and basic routing—ElevenLabs can handle everything on its own.

It’s easy to use, high quality, and surprisingly powerful. So feel free to experiment with it and explore what it can do.

That wraps up our first day experimenting with ElevenLabs and building voice agents. I hope you were genuinely impressed by how easy it was to create multiple voice agents, add knowledge bases, and introduce basic workflows.

We won’t go much further inside ElevenLabs alone, because the real magic happens when we combine it with n8n. That integration—the marriage of ElevenLabs and n8n—is incredibly powerful, and that’s what we’ll start building tomorrow.

Before we move on, though, take a moment to appreciate the progress: we’re already 40% of the way through. I hope you’re starting to feel more like a pro—because once we hook these two platforms together, you definitely will.

# **E) Day 2 - ElevenLabs n8n Integration: Building Conversational AI Voice Agents**

Look at this journey.

Some days are more challenging days, and some days are more satisfying days.
Today is one of those days where it’s both.

Today is challenging.
Today is satisfying.
Today is a bit of both.

We’ve got something challenging in front of us, and we’ve got something satisfying too. Today, we’re integrating 11 Labs with n8n. We’re putting them together, and honestly, it’s going to be fun.

So obviously, we’re going to be using n8n and 11 Labs together in order to build voice agents that have workflows inside n8n.

Now, there are actually two completely different ways to do this—two very different integration patterns. And we’re going to do both. One of them is definitely better, and I’ll explain which one and why, but we’re still going to build both approaches. That’s because it’s good practice, and it will really help you understand what’s going on under the hood.

First Integration Pattern: n8n Is the Boss

The first approach is where n8n is running the show. It orchestrates everything. The entire end-to-end workflow is described inside n8n, with all the different steps laid out clearly.

One of those steps is calling the 11 Labs API to convert text into speech—this is when we want the system to talk back to us. We’ll use 11 Labs specifically for generating the audio.

Another step, usually earlier in the workflow, is speech-to-text. This is where we take incoming audio and convert it into text, which then goes into an LLM.

So in this pattern, these speech-to-text and text-to-speech actions are simply steps inside an n8n workflow, where n8n calls out to 11 Labs as needed.

Hopefully, that makes complete sense—this is how everything can come together when n8n is in control.

Second Integration Pattern: 11 Labs Is the Boss

So what’s the other way?

As you probably guessed, the other approach is where 11 Labs is running the show. In this pattern, 11 Labs owns the orchestration and drives the process from start to finish.

Here, you build a voice agent inside 11 Labs, much like we’ve already done before. You then give that voice agent a tool. We didn’t actually use tools previously, but I showed you how they work.

That tool is used to make a call into n8n, where n8n runs some sub-workflow. In this setup, your business logic and workflows live inside n8n, but they’re triggered by 11 Labs.

So in this model, the two systems collaborate. However, 11 Labs owns the overall end-to-end process and is effectively in charge.

Why Choose One Pattern Over the Other?

So why would you do it one way versus the other?

The big benefit of n8n-led orchestration is simplicity. Everything is in one place. You get a single canvas where you can see the entire workflow from start to finish, which is great.

Another advantage is flexibility. You get a more switchable API setup. You’re calling out to 11 Labs, but you could easily replace that with any other text-to-speech or speech-to-text provider. You just swap out those API calls, and you’re using a different vendor. That’s super convenient.

Those are the main benefits of n8n-driven orchestration.

So Why Ever Choose 11 Labs-Led Orchestration?

At this point, you might be wondering why you’d ever choose the 11 Labs-led approach. It sounds more complex, right?

Well, here’s the key reason:

Latency.

Latency is everything when it comes to voice agents.

If you use n8n to orchestrate everything, you first have to collect the audio, then make a call to convert that audio into text. It’s a linear process.

11 Labs, on the other hand, is a voice-first platform. It’s designed for real-time interaction from the ground up. As you’re speaking, it’s already converting your speech into text. It’s not a stop-start process—it’s continuous and optimized for conversation.

Because of that, it’s far better at handling real-time voice interactions. And when you combine it with n8n, you get the best of both worlds: powerful voice capabilities from 11 Labs and flexible orchestration from n8n.

Yes, it’s more complex—but the payoff is huge.

Advanced Capabilities of 11 Labs

There’s another important benefit to using 11 Labs as the primary orchestrator.

11 Labs has advanced features—like the ability to connect to telephone systems, allowing your voice agent to actually talk to people over the phone.

If you rely solely on n8n to collect audio, you miss out on these advanced, production-grade voice features. By letting 11 Labs lead, you unlock everything it has to offer.

So there are some very strong advantages on the right-hand side as well.

Which Approach Do I Recommend?

So which one do you think I’m going to recommend?

Definitely the right-hand side—the 11 Labs-led orchestration.

I usually say that simpler is better. That’s one of my mantras. I always try to push for simplicity over unnecessary complexity.

But this is one of those rare cases where the added complexity is absolutely worth it.

Latency matters. Waiting while talking to a voice agent is agonizing. Real-time responsiveness is non-negotiable. For that reason alone, you want 11 Labs as your voice agent platform, and you want to use the right-hand pattern.

That said, we’re going to build both approaches. Many people naturally start with the left-hand pattern because it’s more obvious and easier to understand. But very quickly, you’ll feel the difference—and you’ll understand why the right-hand approach is so much more compelling.

Quick Refresher Before We Build

Now, don’t hate me—but before we start building (and yes, there’s some really good building coming up), we need a quick refresher on a few API concepts. This is good stuff, and it’ll help everything make more sense.

First, let’s quickly revisit webhooks.

A webhook is when you expose an endpoint—a URL—that other systems can call to notify you that something has happened. Instead of you asking for information, someone else calls you.

For example, Slack might call your webhook to tell you that a message arrived. An email system might call your webhook to say an email was received. When an external system calls your endpoint like this, that endpoint is called a webhook.

APIs, Endpoints, and HTTP

You’ve heard me say “calling an API” a lot.

When we say that, we usually mean making a web request using HTTP. HTTP is the standard protocol the web uses to exchange information—just like loading a web page.

The web address you call is known as an endpoint.

So the proper language would be:
“I’m calling an API by making an HTTP request to an endpoint.”

On the flip side, if someone is calling your endpoint to notify you of an event, that endpoint is called a webhook.

HTTP Methods: GET and POST

There are different types of HTTP requests, known as methods (or sometimes called verbs).

The two most common ones are:

HTTP GET – used to retrieve information

HTTP POST – used to send information

When you make a GET request, you’re asking for data, and the response usually comes back as JSON.

When you make a POST request, you’re sending data, and that data is usually sent as JSON.

You’ll see us selecting GET and POST in several places, so this is just to make that terminology familiar.

# **F) Day 2 - Automate Text-to-Speech Using ElevenLabs API in n8n Workflows**

Oh, that was just a cheap move to get you to come to the next video.

We’re not actually going to do any building yet.

We’ve got one more slide to go.

I just want to say one more thing. If you look back at the diagram that shows the idea of making an endpoint request–response, where you make an HTTP call to an endpoint, and then the idea of receiving an HTTP request to a webhook to notify us that something has happened, there are nodes in n8n that apply directly to each of those concepts.

We’ve already discovered them.

For example, with Slack, we had a trigger node that received a webhook, and we also had a node for posting a message, which is a typical request–response API interaction.

In many cases in n8n, you’ll find this kind of pair of nodes: one node that receives incoming events via a webhook, and another node that is responsible for making API calls out to the external service.

There are also generic nodes used to talk to platforms that haven’t been explicitly built into n8n. In those cases, there isn’t a dedicated integration component for that third party, but the platform still exposes endpoints and expects webhooks.

Because of that, you can still integrate with it generically, based purely on those URLs.

There is a generic version of a request–response node called the HTTP Request node. This node simply calls a URL endpoint. You give it a URL, and the output of the node is whatever comes back from that call.

On the other side, there is also a generic version of a webhook. This is called the Webhook node.

The Webhook node has both a test URL and a production URL, just like the Slack trigger did. It provides a URL that can be called as a webhook while you’re in test mode, or once you’ve deployed and activated the workflow.

The workflow is triggered when someone else hits that URL. You can configure it to either respond immediately to the third party, or to run an entire workflow and then respond with whatever a later step returns.

Those are configuration details we’ll look at later.

But the main point is this: with some external systems like Slack, you get out-of-the-box nodes—usually two of them: one for triggers and one for making API calls.

If you don’t have an out-of-the-box node like that, you can effectively build your own integration using the HTTP Request node and the Webhook node.

Hopefully that makes sense.

This is something that will come up again and again, so you’ll get used to it. That’s how you build your own integration with a platform that doesn’t already come bundled with n8n.

Okay.

Now it’s time to actually build stuff.

I’ve teased you enough.

Now we’re really going to do it, and we’re going to start in 11 Labs.

You remember looking at 11 Labs. I’ve just signed in, and you should as well.

At the top, you’ll see a dropdown with options like Agents Platform and Creative Platform.

Remember, we’re going to begin with Technique One, where n8n is the boss.

So we’re not going to be setting up agents inside 11 Labs. Instead, we’re going to use 11 Labs purely as a backend service that can convert speech to text and text back into speech.

And the way we do that is using the 11 Labs API.

To do that, we need an API key.

Here’s how you get it.

You go to Developers, which is down here, and you click on it. That takes you to the developer section.

There’s a tab called API Keys. When you go there, you’ll see existing API keys—like the one I already have.

You’re going to press Create Key.

I know you already know how to create API keys, but I’m still going to show you.

You give the key a name—anything you like. For example, I can call it “key two”.

It’s a good idea to keep Restrict Key turned on.

The first restriction you’ll see is Monthly. This looks like a monthly limit, which is a good thing to have.

You might think, like I originally did, that typing 10 here would mean $10 per month—but it doesn’t.

This is talking about the number of credits. Most API calls take around 30 to 60 credits.

You should have plenty of free credits included with your plan. If you’re on a paid plan like me, you’ll have tens of thousands of credits available.

You can put whatever monthly number you want here, but I recommend something like 500 credits as a good starting point. Keep an eye on it, and if you use up your monthly credit limit, you can always come back and increase it.

Next, you select access permissions.

You don’t need write access. Read access is enough for everything we’re doing here. Even so, you can give access broadly if you want.

Once that’s done, you press the Create Key button.

A key pops up. You copy it into your clipboard and click OK.

Keep that key safe, because we’re going to paste it somewhere else in a moment. You might also want to store it securely.

If you ever have a problem with your API key, you know what to do. You come back here, create another one, disable old keys you’re no longer using, or edit permissions as needed.

Always be careful with API keys. Copy them carefully. Don’t paste them into a word processor. You know the deal.

And remember—you can always create a second one.

Okay.

Now we’re back in n8n.

I’m signing into our cloud account and into our instance.

Here we are.

I’m going to press the Create Workflow button to start a new workflow.

We’ll begin this time with a Chat Message node, just like usual.

Click it, then press Escape to come back.

If you see logs up here, remember you can just click them to dismiss them.

Now click the plus button or press Tab, and go to the AI Agent selection.

Here it is.

Press Escape again to return.

Now we choose the chat model.

As always, you could use OpenRouter or OpenAI. This time, just to keep things different for this week, I’m going to use Google Gemini.

There are two Google options:

Google Gemini, which refers to Google AI Studio

Vertex AI, which is more enterprise-level

If you don’t already know Vertex AI, stick with Google Gemini.

For credentials, I click Create New Credential.

I’ve already set up an API key in AI Studio (you can just search for “Gemini AI Studio”).

I paste the key in carefully, press Save, and see that the connection was tested successfully.

And that’ll happen for you too, because you’re a pro with API keys and they never cause any trouble.

So now we’re going to be experimenting with Gemini.

We’re not going to give it any tools or memory for this one.

Let’s check the available models.

I see Gemini 2.5 Flash.

Let’s see if we can use Gemini 3.

Yes—there it is: Gemini 3 Flash Preview.

By the time you’re watching this, it’ll probably just be called Gemini 3 Flash. Pick whichever one you want.

That’s what I’ll pick.

Now we open the chat and say:

“Hi there.”

Off it goes.

It responds:
“Hello! How can I help you today?”

Great.

Gemini is working.

There’s not much voice-related happening just yet.

The next thing we’re going to do is add a step that makes an API call to 11 Labs to turn this text into audio.

I press the plus button and search for 11 Labs.

Up it comes as a possible next node.

When you click on it, you might see that it’s not installed yet. There’ll be a button to install it.

If you’re using n8n Cloud, you just press the button and it installs automatically.

If you’re using the self-hosted version, there are a couple more steps, but they’re very simple. It’s just a community node, and you can Google the instructions.

Once installed, it appears like this.

Now we scroll down to the node actions.

We choose Convert Text to Speech.

That’s what we want.

The node appears, ready to connect to 11 Labs.

The first thing we need to do is select a credential and create a new one.

This requires the 11 Labs API key we created earlier.

I paste in the key from my clipboard.

If you accidentally paste in your Gemini key, it won’t work—it must be the 11 Labs key.

I save it.

And because I’m a pro, it connects and tests successfully on the first try.

You got that too, because you’re a pro as well.

Congratulations.

Now we choose:

Operation: Text to Speech

Voice: Ms. Walker

We choose Ms. Walker because she’s warm and reassuring, and that’s what we like.

Now we need to choose the text that will be spoken.

This is the input that will be sent to 11 Labs to convert into audio.

If we typed something fixed—like the word “bananas”—it would always generate audio for “bananas”, and that’s not what we want.

We want the text to change every time.

So we need to use an expression.

You might be wondering why we can’t say “let the model decide”.

Remember, you can only do that with tools.

This is not a tool. This is just the next step in the workflow.

We always want audio to be generated, so there’s no LLM decision here.

It’s a fixed workflow step.

So how do we tell it to use whatever text came from the LLM?

We use an expression.

You already know this.

It starts with double curly braces.

Inside, we reference the incoming JSON using $json.

The field we want is called output.

So the expression becomes:

$json.output

And when we do that, we see the result:
“Hello! How can I help you today?”

Perfect.

You don’t even need to type it.

You can just drag and drop the output field into the text box.

If it doesn’t drop properly, just try again.

There we go.

Same result.

So now this node is configured so that whatever text comes into it, it looks at the output field, takes that text, and sends it to 11 Labs for text-to-speech using Ms. Walker and our 11 Labs API key.

And that’s it.

We’re fully wired up.

# **G) Day 2 - How to Build a Voice AI Agent with n8n Webhook and ElevenLabs API**

Okay.

Now we go back to our workflow and give this another try. We refresh it, say “hi there,” and see what happens.

Off it goes.

It’s gone to Gemini. It’s converted to speech. The whole workflow ran successfully.

But it didn’t say anything to me.

Maybe you were expecting that, but it couldn’t actually do that. All we did was call an API that generated a file, and that file is an audio file.

Here it is.

We can now download it and listen to it.

I’m going to press the download button, and it’s just downloaded something. Then we’ll go and listen.

So I’ve launched that thing, and I’ve got this little player here. Let’s see what we got.

“Hello. How can I help you today?”

Wow, okay. She’s very chirpy today.

“Hello. How can I help you today?”

That, I guess, is Miss Walker speaking with much enthusiasm to us. And that is how we got here. We got through this API.

Let’s just try changing this to another person.

Let’s try Jason. I think we need someone to counteract that energy level—someone calm, meditative, and soothing.

Again, this is going to be making an API request to 11 Labs.

Back we go.

We reset the chat session and say “hi there.”

Okay, here we go.

It goes across, it goes to the agent, and it’s converted. That was all super quick. We got back a bunch of stuff.

Now we come back into this node. Here it is.

We download this file too, and then I’m going to try right-clicking on this. I’m going to show it in Finder.

Then I’ll bring up the player as before, and we’ll listen to this.

Okay, here we go.

Let’s listen.

“Hello. How can I help you today?”

Definitely a more soothing Jason. Thank you.

And that gives us a good sense of how it works to generate audio. It doesn’t feel very satisfying yet, though—we’ve got a little bit more to go.

Okay, so next up, we’re going to go back to our workflow and do some new things.

First of all, we’re going to get rid of this guy. No more coming in that way.

Instead, we’re going to come in a completely different way.

We are going to add a webhook.

So I’m going to click plus, and I’m going to say “On webhook call.” This is saying that I want to be triggered by a webhook.

Now, we’ve already experienced this a few times because we’ve used things like Slack and Telegram as well. Both of them allowed us to set up a webhook using n8n nodes specifically designed for Slack and for Telegram.

In this case, we’re using the generic one—the one I mentioned that’s generic.

This is the generic “On webhook call” node that allows us to configure any webhook. Anyone that hits this endpoint for test or this endpoint for production will trigger this particular webhook.

So it’s going to sit there listening, and anyone that hits it will trigger it once we press the “listen for event” button.

And that’s how it works. It’s that simple.

There’s no more configuration than that, which I just love. It’s so easy.

So when something comes in, it’s going to hit this webhook.

All right, so what are we going to do with this?

If I go back to the canvas again, let’s see where it put that webhook. I don’t know why it’s over there, but we’re going to move it right here.

So in comes the webhook.

And you can see, by the way, that it has GET selected. You can just look at it again—there’s this HTTP method that can be GET, or there are other things as well.

But I mentioned that GET and POST are the most common.

I’m going to change it from GET to POST because we want this to be pushing information to us.

Okay, so that is set up.

Now, whatever gets posted in, we’re going to set it up so that we can post some audio.

I’m going to want to send that to 11 Labs again.

So here we add in 11 Labs. This time, it’s something that’s going to take audio and convert audio into text—it’s going to transcribe.

We come up here and say we want text. We want speech-to-text, taking audio and turning it into text.

That’s what we’re going to do.

This is going to connect with our 11 Labs account. It defaults to that, which is ideal.

Now I’ll go back to the canvas, and we have this.

Once this has gone to text, I’m simply going to plug that into the agent.

Wow. Could it be as simple as that?

So basically, we’re going to have a webhook URL that we can post audio to. When we do, it’s going to call 11 Labs to turn that audio into text, and it’s going to send the text to the agent.

Now, you’re probably spotting that I’m doing something bad here. There’s something I’m missing.

We’re going to try this out, see it fail, and then we’ll fix it so that we get it just right.

The main thing we’ve done wrong—and you probably guessed this—is that we haven’t configured these different steps to correctly look at their inputs and interpret them properly to do the job they’re supposed to do.

In this case, with this node, we haven’t told it how to read the input data and pluck out what it needs.

And similarly, for the AI agent, remember that this thing is expecting to be connected to a chat trigger. It’s not, so it’s going to get confused. It’s not going to find chat input.

The easiest way to fix this is to try it, see the data that comes in, and then correct it. And that’s what we’re going to do.

But first, there’s one more step we need to add to this flow.

The last step is all about what happens when this webhook is called—what gets returned to the web page that called it, or to the caller.

Whoever the caller is, it’s going to be a web page, but it doesn’t have to be.

Right now, the way we’ve set this up is to respond immediately. That means when you call this webhook, it responds with something like “okay,” while the flow continues running in the background.

That’s kind of useless, because what we want to return to the web page is the audio. We want it to say something back.

So we don’t want to respond immediately.

There are a few options here, but we’re going to use “Respond using Respond to Webhook” node.

That means we need to add another node at the end.

Let me hide all of this for a second and give ourselves some more room.

We add a node right here, and it’s of type “Respond to Webhook.”

Here it is.

This node can respond with whatever came back.

We can respond with the first incoming item, but what we actually want is to respond with the field data.

The way this comes in, we want to respond with a binary file, which is the audio data that comes back here.

That’s what we want it to do.

Potentially, we might need to add some more configuration here. We’ll see if this hangs together, and we’ll fix it once the whole flow is in place.

So to recap, let’s hit the tidy button and clean this all up.

Very nice.

Here’s what’s happening.

We’ve set up a webhook—that little lightning bolt represents a trigger. When someone hits the URL, it collects audio, sends it to 11 Labs to convert audio into text, passes that text into our agent, converts the agent’s output back into speech, and produces an audio file.

That audio file is then returned as the response to the webhook—to the URL that someone else hit.

Got that?

Click around and make sure you understand what each of these nodes does.

Then we’re going to test this thing out.

On my desktop, I’ve got a file called voice HTML.

I’ll put this in the course resources so you can get it too.

It’s a very simple web page.

You type in a URL, press “Start recording” to record directly in the web page—in this case, in a Chrome browser—then press stop.

When you press “Send to n8n,” it posts that audio to the URL. Whatever comes back can then be played.

It’s just simple HTML—a raw HTML file.

I could tell you that I painstakingly wrote this HTML file, but that would be a complete lie.

I simply asked—normally I use Claude Code, but this time I asked ChatGPT—“Can you make this HTML page?” And it just did.

It even added helpful things like a tip saying to use the test URL while building and switch to the production URL later. I didn’t ask it to do that, but it was absolutely right.

So good for it.

You could also write this from scratch if you wanted, or just use mine. I’ll make it available.

Okay.

With that, we’re now going to find this URL, put it in here, and give it a whirl.

And it’s not going to work at first, because we still need to fix up the workflow.

But let’s at least try it.

# **H) Day 2 - Build an AI Voice Agent with n8n, ElevenLabs, and Gemini Workflow**

Okay.

So I’m going to go over to my workflow here. I’m going to go to the webhook, and I’m going to take the test URL. I copy this URL.

Now I go over to the web page, and I paste that webhook URL into the input field.

Next, I’m going to start recording.

“Well hi there.”

Oh, I realize I have to say it loud while I’m using this site. Hang on.

“Well hi there. What is two plus two?”

Let’s try that again.

“Well hi there. What is two plus two?”

That seems to record quite nicely.

Now I’m going to say Send to n8n.

But first, of course, I go back to n8n, go back to the beginning, and press Execute Workflow so it starts listening and waiting for the test call.

Then I go back to the web page, press Send to n8n, and come back here.

And we’ve got a problem right away.

This is a problem we were expecting.

It says: Cannot read properties of undefined.

Let’s double-click and see what’s happening.

What came in is something called audio, but the file it was looking for was called data. That’s not right.

So we have to change this to say audio, so now it knows it’s looking for a file called audio.

Okay, let’s save it.

Back we go.

Oh, we also have to press Execute Workflow again so it starts listening.

Now we can press Send to n8n again.

Back here we go.

And now we’ve got a problem in the AI Agent node.

It’s going to be the same type of issue.

Alright, so I double-click here and take a look.

You can see under Source it thinks it’s connected to a chat trigger node. That’s no good.

We need to click here and say Define Below, because just like before, we’re going to give it details of where to look for its input.

We’re not going to give it something fixed. We want it to evaluate an expression.

And we’re going to get that expression from over here.

What we want to take is the transcribed text:

“Well hi there. What is two plus two?”

That’s what we want, right?

Isn’t it cool? That is the transcribed text that came from 11 Labs.

So it works.

We’re going to take that and drop it into the input field.

Or, if you were thinking ahead, you might have guessed what the expression would be.

And if you were right, it’s simply JSON.text as the expression.

You can see the result, and now it’s going to work.

Okay, we escape back.

We go back to the canvas, and now we’re going to try it one more time.

So we press Execute Workflow.

We go back to the web page.

We press Send to n8n.

Back over here it goes.

It goes off, it goes off.

“Two plus two is four.”

Ha! There we go. Worked first time.

Because that happened so fast, I’m going to do it one more time. I wasn’t expecting it to be that quick.

Let’s try it again.

Press Execute Workflow.

Go back here.

Press Send to n8n.

“Hello. Two plus two is four. How can I help you today?”

Fabulous.

So there you have it.

We now have a working audio app.

It was a little bit of work, but it was great for you to see this, just as I promised.

You can see that n8n is in control. You’ve got the full workflow:

A webhook where the audio is posted in from our web page

An API call to 11 Labs that transcribes the audio to text

That text goes through our AI agent (this time using Gemini)

Gemini produces a response

That response is converted back into speech

The final node ensures the webhook response is an audio file

That’s why the web page receives audio back as the response.

Congratulations 🎉

You’ve just made your first audio voice agent.

I call it an audio voice agent. There are other kinds of voice agents, but this is a voice agent.

Now your mission is to experiment with this.

Obviously, I didn’t add memory here, because we don’t yet have something like a chat history ID. You’d have to work around that a bit.

But do it. Give it memory. Maybe even give it a tool or two. You’re already a pro with tools.

Then have a real conversation through this web page.

The idea here is that this simple web page we set up—this kind of “fake” web page—could be anything.

You could embed this in a WordPress site. You could make it fancier. ChatGPT would have an even easier time making it look prettier.

And you could turn it into something that lets users speak directly to your end-to-end workflow.

This is great experience.

Of course, it’s not the way I ultimately recommend doing it, which is what we’re going to look at next.

But it is a very good way to understand APIs in action.

And if you had a bigger workflow with lots of other steps, and at some point you wanted to add an audio interaction, this is exactly how you would do it.

Okay, so that wraps up technique approach number one.

Approach number two, as I said, is more complicated in some ways—but also simpler and quicker for us now that we’ve built this.

So let’s put this one to bed and dive into approach number two, where 11 Labs calls the shots.

# **I) Day 2 - How to Connect ElevenLabs AI Voice Agent to n8n Workflow with Webhooks**

Okay, so the first thing we do is click to rename the existing workflow. We give the old one a clear name, something like “N8N Orchestrates”, save it, and then go back again to the workflows list. After that, we create a brand-new workflow. This new workflow is going to be the one we actually use going forward.

Once the new workflow is created, we start thinking about how it should begin. The first question is always: what’s the trigger? In this case, the trigger is very clear. We want this workflow to be invoked externally by ElevenLabs, so the trigger must be a Webhook. Since ElevenLabs will be pushing data into N8N, the webhook method should be POST. This webhook becomes the starting point of the workflow.

After setting up the webhook trigger, we decide what should happen next. The webhook input should go into an AI Agent node. We add an AI Agent and acknowledge upfront that it won’t fully work yet — that’s fine, we’ll configure it properly in a moment. For the chat model, we choose Gemini, and specifically we want to use the latest version available. In this case, that’s Gemini 3 Flash Preview, which we manually type in as models/gemini-3-flash-preview since it may not appear in the dropdown.

We don’t add memory to this agent, because this is not meant to be a conversational chat assistant. It’s an action-oriented agent that receives a question, performs a task, and returns an answer. The next decision is what action we want the agent to perform. To keep things simple and avoid introducing new integrations, we decide to use a familiar tool: Google Sheets.

We configure the Google Sheets tool to work with our existing stock portfolio sheet. This workflow will allow the agent to answer questions about how much of a particular stock the user owns. To make this clear to the AI, we manually set the tool description and write something like: “Use this tool to retrieve the equity portfolio of the user to answer questions about their holdings.” This helps the LLM understand exactly when and why it should use this tool.

Before wiring this up to ElevenLabs, we want to test the workflow locally. To do that, we temporarily add another trigger: When Chat Message Is Received. N8N allows multiple triggers, so this is fine. We connect this chat trigger to the same AI Agent node and temporarily add Simple Memory, just to make testing smoother.

We then open the chat and send a test message like, “Hi there.” The agent responds correctly. Next, we ask a real test question such as, “How many SPY do I hold?” The workflow runs, the Google Sheets tool is used, and the agent responds with something like, “I currently hold two shares of SPY.” This confirms that everything is working exactly as expected.

Once we’ve validated that the logic is correct, we clean things up. We delete the temporary chat trigger and remove the memory from the agent, since neither is needed in production. Then we re-enable the webhook trigger by turning it back on. At this point, the workflow is almost ready.

There’s one final piece we need to add on the N8N side. Because this workflow is triggered by a webhook, it must explicitly send a response back. To do that, we add a Respond to Webhook node. We configure it to respond with the first incoming item, which will contain the AI agent’s output. This ensures that ElevenLabs receives a proper JSON response like “You currently hold two shares of SPY.”

Now that the N8N workflow is complete, we move over to ElevenLabs. We go to the Developers section and then into the Agents area. Here, we create a new agent using the Blank Agent option and give it a name, such as N8N Agent. For the system prompt, we keep it simple with something like, “You are a helpful assistant.” We then choose a voice — this time, instead of using Jason or Eric, we select Russell.

Next comes the most important part on the ElevenLabs side: tools. We go to the Tools section and click Add Tool, choosing Webhook as the tool type. This tool will allow the ElevenLabs agent to call our N8N workflow.

We give the tool a clear function-style name with no spaces, such as equity_portfolio_question_tool. In the tool description, we explain to the language model when it should use this tool. For example: “Use this tool to ask any question about the user’s equity portfolio to a specialist assistant who can answer.”

We then configure the webhook details. The HTTP method is POST, and for the URL, we go back to N8N, copy the webhook test URL from the webhook trigger node, and paste it here. We increase the timeout to 60 seconds, since this is an AI-driven request and could take a bit longer. We also enable pre-tool speech so the agent says something while the tool is running, avoiding awkward silence.

Because this is a POST request, we define the request body schema. We describe what information is being sent to N8N. We explain that the body contains the question to ask the equity portfolio specialist. We define a single field named question, set its type to string, and describe it as: “The specific question about the user’s equity portfolio for the specialist to answer.” This allows the LLM to correctly populate the request body when it decides to call the tool.

With that, we add the tool. Now everything is connected. The ElevenLabs agent understands when to use the webhook tool, knows how to structure the request, and knows where to send it. On the other side, N8N receives the request, processes it through the AI agent and Google Sheets, and responds with a structured answer.

At this point, the entire system is ready. ElevenLabs handles the voice interaction, N8N orchestrates the logic and data access, and the webhook connects the two seamlessly. We’re fully set up and ready to roll

# **J) Day 2 - How to Connect ElevenLabs AI Voice Agent to n8n Using Webhooks**

At this point, we already expect that the workflow will initially fail. That’s completely fine and actually part of the plan. The reason we expect an error is because we haven’t yet configured the chat input mapping correctly inside N8N. Even so, we should be able to confirm that the webhook is being called and see exactly where things break.

We go back into N8N and make sure the workflow is active. We click Execute Workflow, which puts the webhook into a listening state. This is important because N8N only receives test webhook calls while it is actively running. With the workflow waiting, we move back over to ElevenLabs.

In ElevenLabs, we return to our agent — the one we just configured with the webhook tool. We click Preview to start interacting with it. The agent comes online and greets us. We then ask a real question, something like:
“How many shares of SPY do I have in my portfolio?”

The agent attempts to answer but fails gracefully. It responds with something like:
“I’m sorry, I was unable to retrieve the number of SPY shares you hold.”
This confirms exactly what we expected: the tool was invoked, but something went wrong downstream.

Now we switch back to N8N to inspect what actually happened. Right away, we see that the webhook was successfully called. Even if we weren’t watching in real time, the execution history clearly shows an incoming request. That’s great news — it means ElevenLabs is correctly calling N8N.

The problem becomes obvious once we inspect the webhook input. The workflow was still configured as if the input were coming from a chat trigger, but it isn’t. Instead, this input is coming from a POST webhook request. Inside that request body, we can see a field called question, and it contains exactly what we asked in ElevenLabs:
“How many shares of SPY do I have in my portfolio?”

This is important because it proves that the schema we defined in ElevenLabs worked exactly as intended. We told the LLM to populate a question field, and that’s precisely what it did.

To fix the issue, we open the AI Agent node in N8N. Where it previously assumed a chat-based input, we now change it to Define Below. This allows us to explicitly tell the agent where to find its input text. We drag the value from the webhook payload and map it properly.

The expression we use is simple and predictable:
{{$json.body.question}}

This tells the agent to extract the question from the body of the incoming webhook request. Once this is set, the agent now knows exactly what text it should process.

We save the changes, go back to the workflow canvas, and once again click Execute Workflow so N8N is listening. Then we return to ElevenLabs and press Preview again.

This time, when we ask:
“How many shares of SPY do I have in my portfolio?”
the agent responds smoothly. It may say something like:
“Let me check your portfolio for you.”
A moment later, it replies:
“You have two shares of SPY in your portfolio.”

Success. The full round trip works end to end.

At this point, everything is functioning correctly, even though there is still some latency. Compared to the first approach, however, this setup is noticeably faster and more responsive. Despite initially calling this the “more complicated” approach, it actually turned out to be cleaner and simpler once everything was wired together.

If we look at the N8N execution history, we can see all the green checkmarks. The webhook was triggered by ElevenLabs, the AI agent ran successfully, the Google Sheets tool was used, and the Respond to Webhook node sent the output back correctly.

Now it’s worth revisiting what actually happened behind the scenes, because while the build was simple, there are a lot of moving parts conceptually.

The journey starts in ElevenLabs, where we created a voice agent (Russell) and gave it a webhook tool. This tool allows the agent to call out to an external system — in this case, N8N. We configured the tool to use a POST request, which allows us to send structured JSON data.

We explicitly described to the LLM that it must include a question field in the request body. When the user spoke, the agent correctly filled in that field with the user’s question and sent it to N8N.

N8N received the webhook call at the exact URL we configured. Instead of expecting chat input, it parsed raw JSON. The extracted question was passed to Gemini, which then used a Google Sheets tool to look up the portfolio data. The result was then passed to a Respond to Webhook node, which sent the answer back to ElevenLabs.

Back in ElevenLabs, Russell received the response from the tool and spoke the answer to the user. That completes the full loop.

One important limitation becomes obvious at this stage. When using the test webhook, N8N can only process one execution at a time. That means follow-up questions often fail. The solution is to move to production mode.

To do that, we go back to N8N and publish the workflow. This generates a production webhook URL. We copy that URL, return to ElevenLabs, open the webhook tool configuration, replace the test URL with the production URL, and press Save (this step is very easy to forget).

Next, we publish the ElevenLabs agent as well and generate a shareable public link. Now both systems are live in production.

We open the shareable link in a new browser tab and begin testing again. The agent answers our first question correctly. Then we ask a follow-up question like:
“What other stocks do I have besides SPY?”

This time, it works perfectly. The agent queries N8N again, retrieves additional portfolio information, and responds with a list of other holdings. Multiple turns now work smoothly, end to end, in production.

At this point, everything is running exactly as intended. We have a fully deployed, interactive voice agent with relatively low latency, combining the strengths of ElevenLabs for voice and N8N for orchestration and business logic.

The key takeaway is this:
The best way to integrate ElevenLabs with N8N is to let ElevenLabs be in charge.
ElevenLabs handles voice, conversation flow, and tool selection. N8N acts as the backend brain — executing workflows, calling APIs, querying data, and returning structured results.

Yes, the setup is a bit fiddly at times, especially around webhook schemas and request bodies. But this is boilerplate work, and tools like ChatGPT are excellent at helping generate and debug these configurations.

What matters most is understanding the architecture and building blocks. Once you grasp those, everything else becomes much easier.

And with that, this completes Week 2, Day 2: Integrating ElevenLabs with N8N.
You’re now past the halfway mark — about 47% complete — and ready to move on to an entirely new topic next.

# **K) Day 3 - What is RAG in AI: Retrieval-Augmented Generation Explained**

Okay everyone, today is a purple day, which means it’s a core learning day. That also means there’s a lot of foundational material to get through. Some of you may have noticed that yesterday was actually a yellow day, but the slides accidentally had a purple strip on the front and back instead of yellow. I realized this only at the end, and for a moment I considered rerecording the whole thing. But then I figured I’m probably the only person who notices or cares about the color of that strip. Still, if you did notice and it bothered you, my apologies. I won’t make that mistake again—and today really is a purple day.

Since it’s a core day, we’re diving into something important. Today I’m going to demystify and fully explain one of the hottest topics in generative AI: RAG — Retrieval Augmented Generation, along with the newer, juicier twist on it. If you already know RAG well, feel free to put this on 2× speed and breeze through it. But for many of you, this will be genuinely interesting.

This is a technical topic, but I’m deliberately not going deep into the technical weeds. Instead, I’ll give you enough intuition so that you understand how and why it works, and—most importantly—how to apply it to get better commercial outcomes. Before we get there, though, we need a quick recap of something fundamental: APIs.

Last time, we spent a lot of time clarifying API terminology, and I warned you that I’d probably revisit it a few times. This is one of those times. When people say they’re “calling an API,” what they usually mean—especially in the context of web APIs—is that they’re making an HTTP request. The URL they send that request to is called an endpoint. So when someone says, “I’m making an API call,” they’re really saying, “I’m making an HTTP request to an endpoint at a specific URL.”

Then there’s the concept of a webhook, which is essentially the opposite direction. If you’re on the receiving end and you expose a URL that others can call to notify you that something has happened, that URL is a webhook. Some people even call webhooks “reverse APIs” because instead of you calling out, someone else calls into you. You might say, “Here’s my webhook—call this URL when this event occurs.” That’s what a webhook is.

We also talked about HTTP methods. The most common ones are GET and POST, although others exist. Typically, GET is used when you’re fetching information and expecting JSON back, while POST is used when you’re sending information. One important detail we discovered yesterday is that POST requests can include a body, often containing JSON.

This becomes especially relevant when large language models are involved. Yesterday, we gave an LLM in ElevenLabs a tool that allowed it to make an HTTP POST request to a webhook. That POST request included a JSON body containing a question. N8N received the webhook, extracted the question, passed it to another LLM, optionally used a tool, and then sent the answer back. That entire flow is a real, practical example of APIs and webhooks working together.

Hopefully, that recap gives you a solid mental model of endpoints, webhooks, methods, and how they all fit together. With that groundwork laid, we can now completely change topics and start fresh.

New blank sheet of paper.

Let’s talk about RAG.

RAG became massively popular a couple of years ago, and the core idea behind it is unbelievably simple—so simple that you’ve probably thought of it yourself if you didn’t already know the term. The motivation was this: people wanted LLMs to have deep expertise in specific commercial or domain-specific areas. Traditionally, the way to do that was to train or fine-tune models with domain-specific data, which is expensive, time-consuming, and data-hungry.

Then a complementary idea emerged. Instead of retraining the model, why not just give it the extra knowledge at runtime? This approach is much cheaper, much faster, and easier to implement. It may be a little less powerful than full training, but it’s often “good enough”—and sometimes excellent. This idea became known as Retrieval Augmented Generation.

At its core, RAG is simply about making LLMs appear more knowledgeable by giving them more context. In the most blunt terms possible, RAG is about stuffing extra information into the prompt. When you phrase it like that, it almost sounds trivial. But there’s a collection of techniques and best practices around doing this well, which is why it became such a big deal.

RAG stands for “retrieval augmented generation,” which is a lot of words for something fairly straightforward. It’s a clever hack that happens to work extremely well. I often describe RAG as a “big old hack”—not because it’s bad, but because it’s pragmatic. There’s an entire cottage industry of techniques built around it, many with long, impressive names. But underneath, it’s trial-and-error engineering that delivers strong results.

I like to explain RAG using two ideas: a small idea and a big idea.

The small idea is simply this: can we make an LLM more knowledgeable just by adding relevant information to the prompt? Imagine you’re a travel airline answering customer questions. If someone asks, “How much does it cost to travel to Paris?”, you could include the question in the prompt along with background information like ticket prices, travel rules, or seasonal discounts. LLMs are very good at using this additional context when predicting the next tokens, so the answer will naturally align with the information you provided.

You can try this yourself in ChatGPT. Ask a question, then add extra context like pricing details or policies, and you’ll see how much better the answer becomes. This is not surprising—if the information is in the prompt, the model will use it.

The obvious problem with this approach is scale. If you’re a travel company with ticket prices for every city in the world, you can’t shove all that data into the prompt. Even if you technically could, it would be full of irrelevant information, making the model’s job harder, not easier. You’d be overwhelming the LLM with a massive essay when all you really need is one small, relevant snippet.

That’s where the big idea of RAG comes in. Instead of sending all your data, can you select a relevant subset of information that’s most likely to help answer the question? It doesn’t have to be perfect. Maybe only a third of the information you send is actually useful—but that’s still far better than sending nothing or sending everything.

This selective retrieval is the heart of RAG. It’s a balancing act: too little information and the model lacks context; too much and you overwhelm it. Tuning this balance—how much data to retrieve, how relevant it is, and how it’s structured—is what separates basic RAG from excellent RAG.

The final and most powerful idea behind RAG is this: you can actually use an LLM to help decide what information is relevant. You don’t necessarily need to use the same LLM that answers the final question. You can use another model upstream to figure out which chunks of data are most likely to help answer a question like “How much does a ticket to London cost?” and retrieve just those pieces.

That’s the core intuition behind RAG. A simple idea, scaled with smart retrieval, that turns general-purpose LLMs into domain experts—without retraining them.

And that’s why RAG became such a big deal.

# **L) Day 3 - How Embedding Models Enable Semantic Search in RAG Systems**

To make the idea behind RAG (Retrieval-Augmented Generation) more concrete, let’s walk through a simple diagram in words. The core idea is about giving an LLM extra knowledge by injecting relevant information directly into the prompt before it answers a question.

Imagine we are in a chat application. A user asks a question—say, “What is the ticket price to London?” This question comes into some system, maybe built using a low-code platform like N8N or a custom code setup, whose job is to figure out how to answer the question using an LLM.

Before sending the question to the LLM, the system first checks whether it already has useful information stored somewhere. This storage is usually called a knowledge base—essentially a database filled with relevant documents, facts, or reference material. You might remember this term from earlier examples, such as when working with tools like ElevenLabs.

A very simple approach would be to scan the knowledge base for the word “London.” If the system finds documents mentioning London, it collects all of that information and adds it to the prompt. The final prompt sent to the LLM might look like this: “The user is asking about ticket prices to London. Here is some additional information that may help.” Then all the retrieved London-related content is appended.

The LLM then does what it always does: it predicts the next tokens based on the provided context. Because the prompt now contains both the question and relevant background information, the model is more likely to produce a correct and helpful answer about ticket prices to London.

However, this approach has a serious weakness. Suppose the user doesn’t mention London directly. Instead, they ask something like “How much does it cost to go to Heathrow Airport?” Heathrow is one of the major airports in London, but if the word “Heathrow” doesn’t appear in the knowledge base—or if the system is only searching for the word “London”—then nothing relevant is retrieved. As a result, the LLM has no helpful context and may fail to answer properly.

This makes the approach very brittle. It depends heavily on exact word matches and on whatever retrieval logic the developer has written. While the idea of adding knowledge to the prompt is good, the execution becomes clunky and unreliable when based purely on keyword matching.

The real breakthrough comes from figuring out how to do a fuzzy lookup in the knowledge base—one that doesn’t rely on exact words but instead looks for similar meaning. This is often called semantic search. The goal is to retrieve information that is conceptually related to the user’s question, even if the wording is different.

To understand how this works, we need to take a short detour and talk about embedding models.

An embedding model is a special type of language model. If this starts to sound overly technical, it’s worth emphasizing that this is extremely important in real-world, commercial AI systems. Almost any serious agent or RAG system uses embeddings because they allow you to move faster and leverage stored expertise effectively.

Unlike typical LLMs—which take text as input and predict what text comes next—an embedding model has a different job. It takes text as input and outputs a list of numbers. These numbers represent the meaning of the text rather than the next words to generate.

Embedding models are also known by other names: encoders, embedding models, vector embedding models, or embedders. All of these terms refer to the same concept. While text is still tokenized internally like any other model, the output is not tokens—it’s a numerical representation of meaning.

This list of numbers is called a vector. To make this easier to visualize, imagine the model outputs just three numbers. Those three numbers could be treated like X, Y, and Z coordinates, representing a point in 3-dimensional space. In reality, embedding models usually output hundreds or thousands of numbers—often around a thousand—so you can think of each embedding as a point in a thousand-dimensional space.

While that sounds very sci-fi, the idea itself is simple: it’s just a long list of numbers that captures the meaning of the text. If there were only three numbers, it would be easy to draw. With a thousand numbers, it’s harder to visualize, but mathematically it works the same way.

Now here’s why this matters. If you pass two pieces of text with similar meanings into an embedding model, the resulting vectors will be close to each other in this space. In other words, text that means similar things ends up represented by similar numbers.

For simplicity, imagine everything lives in 3-dimensional space. Each paragraph of text becomes a point. The closer two points are, the more similar their meanings are. Technically, this similarity is calculated using something called cosine similarity, but you don’t need to worry about that detail right now. The key idea is that closeness equals similarity.

This same principle applies in high-dimensional space as well. Whether it’s 3 dimensions or 1,000 dimensions, we can compute how similar two vectors are and therefore how similar the meanings of their original texts are.

Crucially, this similarity is not about shared words. For example, “How much does it cost to go to London?” and “What is the ticket price to Heathrow?” use very different wording, but they express essentially the same idea. A well-trained embedding model recognizes this and places their vectors close together in space.

Embedding models are good at this because they are trained on massive amounts of data using sophisticated techniques that teach them which pieces of text are related in meaning. Over time, they learn how to map similar concepts to nearby points in vector space.

At this stage, we can simply accept this as a given: embedding models exist, they turn text into vectors, and the distance between vectors tells us how similar the meanings are.

So what does this allow us to do?

It enables semantic search—a fuzzy, meaning-based search across large datasets. Instead of scanning for exact words, we embed both the user’s question and all the documents in our knowledge base. Then we look for the documents whose vectors are closest to the question’s vector.

This is incredibly powerful. When someone asks “How much is the ticket price to London?”, the system can retrieve information about travel costs, Heathrow flights, or related pricing data—even if none of those documents use the exact same words.

That retrieved information is then added to the prompt, and the LLM uses it to generate a much more accurate and grounded answer. This is the big idea behind RAG: combining semantic search using embeddings with generative language models to produce better, more reliable responses.

# **M) Day 3 - How RAG Works: Vector Databases and Semantic Search Explained**

This is the moment where everything comes together. At this point, a new mental diagram should form, and hopefully the entire idea behind RAG finally clicks into place.

We start again with a user chatting with our system. The user asks a question—something like, “How much does it cost to travel to Heathrow?” This question comes into our application, whether it’s custom software or a low-code platform like N8N. On its own, this question is not going to directly match anything in our database using simple keyword search.

So what do we do next?

The very first step is to take the user’s question and send it to an embedding model. We ask the embedding model to turn this question into a set of numbers—a vector. You’ll often hear this described as vectorizing the question, which again sounds very sci-fi, but really just means converting text into a numerical representation of its meaning.

Once we have a vector for the question, we then look into our database to find relevant information. But here’s the key difference from before: everything in our database has already been turned into vectors as well. Every document, paragraph, or chunk of knowledge has its own vector representation.

Now we can ask a much smarter question: Which vectors in our database are closest to the vector for “How much does it cost to go to Heathrow?” Closeness here means similarity in meaning. The information we retrieve might not be perfect—it could include things related to Heathrow that aren’t about ticket prices—but overall, it’s very likely to surface relevant data.

Typically, we take the top few closest results—maybe the ten nearest vectors or everything within a certain similarity threshold. Once we’ve identified those vectors, we do something important: we ignore the vectors themselves and go back to the original text that produced them.

We collect that original text—the actual human-readable content—and combine it together. Then we construct a prompt that says something like: “The user is asking how much it costs to go to Heathrow. Here is some content that may be relevant.” After that, we paste in all the retrieved text.

This prompt is then sent to the LLM. The LLM generates an answer, and because it now has high-quality, relevant context, the answer is likely to be correct. At that point, you’re happy—and that is the core idea behind RAG.

Hopefully, by now, RAG feels much less mysterious. You don’t need to know every low-level detail, but having a solid mental model of how the pieces connect is extremely valuable.

One additional clarification is worth making. The database at the bottom of this diagram—if it supports storing vectors and efficiently querying for similar vectors—is often called a vector database. In the past, only a few specialized databases were built for this purpose. Today, vector search has become so popular that most mainstream databases support it.

Databases like PostgreSQL and MongoDB now allow you to store vectors and perform vector similarity queries. So you don’t necessarily need a niche or exotic system anymore—many existing databases can do this out of the box.

Another common source of confusion is the role of vectors in the final prompt. Some people think that the prompt sent to the LLM contains vectors—like numbers representing ticket prices, plus numbers representing similar data. That is not the case.

The LLM on the right side of the diagram knows nothing about vectors. It does not understand embeddings or cosine similarity. All it wants is text so it can predict the next tokens. The embedding model and the vector search logic exist entirely outside of the LLM’s awareness.

The embedding model’s only job is to help select relevant context through fuzzy, semantic search. Once that selection is done, vectors are completely forgotten. What gets sent to the LLM is plain text—actual content that helps answer the question.

The embedding model and the LLM do not even need to come from the same provider. They could both be from Google, or they could be from completely different companies. They serve different purposes and don’t interact directly with each other.

So that’s the complete picture of RAG.

What makes this so powerful is that it creates the illusion of a model that knows an enormous amount of information—potentially everything about your company. In reality, the model is simply very good at retrieving the right information at the right time.

This is why RAG is such a strong business solution. Any time you want an expert knowledge worker—someone who knows all your products, all your internal documentation, all your policies, or all your employee information—RAG is the go-to technique.

For example, HR systems that need to understand company policies and employee data often use RAG. Internal support bots, documentation assistants, and enterprise knowledge workers are all classic RAG use cases. Anywhere you need fast, contextual expertise, RAG fits naturally.

There are also many extensions and variations of RAG. You might hear about graph RAG, hierarchical RAG, reranking, and many other techniques. The ecosystem around RAG is large and growing quickly.

One particularly important point is that RAG is very experimental and hacky by nature. The single most important thing when working with RAG is measurement—measuring retrieval quality, measuring response quality, and evaluating whether changes actually improve results. Without measurement, it’s impossible to know whether one approach is better than another.

If this topic really grabs you, entire courses and weeks of study are dedicated to RAG, including evaluation and optimization. But for now, it’s time to move forward.

Next, the focus shifts to agentic RAG, a newer and very hot topic. To understand it properly, the next step is to compare traditional RAG—the original approach—with agentic RAG and see what makes it different.

# **N) Day 3 - Agentic RAG vs Traditional RAG: Building Smarter AI Retrieval Systems**

It probably won’t come as any surprise to you that the next step after traditional RAG feels very obvious. In traditional RAG, we understand the process clearly. A user chats with us, their message goes through vector-based retrieval, and our code performs a semantic search using vector similarity. That retrieved context is then sent to an LLM, which uses the additional context to generate a response. That entire flow—user message, retrieval, response—is what we call traditional RAG.

However, when you think about introducing agents into this world, the linear nature of traditional RAG starts to feel a bit limiting. It feels like a fixed pipeline: you write the code, it always performs vector retrieval in the same way, and then it sends the result to the LLM. But RAG, by its nature, often works better when it is more iterative, more interactive, and more adaptive.

This is where the agentic twist comes in.

With agentic RAG, the user still starts by sending a message, just as before. But instead of that message going directly into a vector-based retrieval step, it first goes to an agent. When we say “agent,” what we really mean is an LLM that is responsible not just for answering the question, but also for deciding how to answer it. The LLM now controls the workflow: figuring out what to do next, which tools to use, and how best to find the information needed to respond.

One of the tools we give this agent is access to vector-based retrieval. The agent can query a vector database whenever it wants. It might try one retrieval approach, then try another variation, experimenting with different RAG techniques to find better or more relevant context. We can also give it access to other tools.

For example, we might give the agent direct SQL access. If the agent realizes that there’s a structured table—say, a ticket prices table with columns for city or destination—it may decide that vector search is unnecessary. Instead, it could write a SQL query like “select ticket prices where city equals London,” execute that query, and retrieve the data in a more traditional way.

The key idea is that the LLM is now in control. It decides whether to use vector search, SQL, or some other retrieval strategy. It can combine results, iterate, and refine its approach. Once it has gathered the best possible context, that same LLM can then generate the final answer to the user.

There are, of course, other architectural options. You could have one agent whose sole job is retrieval and another LLM that only focuses on answering the question. That separation of responsibilities is perfectly valid. But in its simplest form, agentic RAG is just a contrast between two approaches: traditional RAG as a linear workflow, and agentic RAG as a flexible, iterative process driven by an LLM.

And there’s no prize for guessing which one we’re going to build. Yes—we’re going to build agentic RAG.

Today was very much a core expertise day. It may have felt theory-heavy, but the goal was to make it applied, practical theory—ideas you can actually use in real systems. To make things more concrete, and not just about slides, it’s time to talk about Supabase.

N8N already comes with built-in database capabilities and some support for vector databases. However, for this project and this week’s work with data, databases, and vector storage, the decision is to use a third-party database that’s extremely popular among agencies, startups, and mid-sized companies. That database is Supabase.

Supabase is widely used and very well regarded, which makes it a great tool to learn. Unsurprisingly, N8N has a Supabase node—it really had to, given how popular Supabase is.

So what exactly is Supabase? At its core, Supabase is a managed, cloud-hosted version of PostgreSQL. PostgreSQL is one of the most popular relational databases in the world, and Supabase makes it incredibly easy to set up and use. It’s very user-friendly, very startup-friendly, and ideal for quickly building real products.

The recommendation is for you to visit the Supabase website and spend some time exploring it. It offers a generous free tier, which is essential for this course. You’ll see that Supabase provides several services, but the PostgreSQL database is the main one we care about.

You’ll also notice that Supabase supports storing vector embeddings—that is, vectors generated by embedding models. This is exactly what we need for building an agentic RAG system. It’s the right kind of database for this project.

Your homework is to explore Supabase, set up an account, read through some of the documentation, and get comfortable with the interface. Check out the pricing page so you understand what the free plan includes versus the paid plans. The free tier includes things like unlimited API requests and up to 50,000 monthly active users—which is far more than we’ll need for this week’s project.

There’s no need to upgrade to a paid plan for this course. The free tier is more than sufficient.

When you set up your Supabase account, you’ll be asked to name your organization. A good approach is to name it something personal, like your name followed by “research,” “education,” or something similar. This reinforces the idea that this is your personal sandbox for learning and experimentation, not a corporate environment.

Depending on your region, the signup flow may look slightly different, but overall it’s very straightforward. If you run into any issues, you can message me or check the course resources in case there are additional notes there.

Looking ahead, the next two days are where things really come together. We’ll be building a RAG pipeline with a voice agent. The goal is to create an expert voice agent that can accelerate a business by handling large volumes of questions about a company, scaling knowledge delivery, and streamlining operations.

This kind of agent acts like a knowledgeable expert with full access to everything about your company. We’ll build this step by step, but in a way that leaves you with a foundation you can expand—either for your own company or as a commercial product you offer to others.

Tomorrow, we’ll start by integrating Supabase for the first time. We’ll take data, turn it into vectors, and store it in Supabase. This will focus on data ingestion and building automated pipelines.

Finally, we’ll wrap up the week by building the expert voice agent itself. Just like what we did previously, the voice agent will call a webhook running in N8N, which will execute a workflow to answer expert-level questions using RAG.

That’s what’s ahead. Hopefully you’re excited—I certainly am. And with that, you’re officially over halfway through the program: 53% complete. You’re well on your way to becoming an N8N Pro.

# **O) Day 4 - Building RAG Pipelines with Supabase Vector Database and Embeddings**

It is week two, day four. This marks the start of a two-day series in which we wrap up week two, and together these two days will have that familiar combination of being both challenging and satisfying.

As you will see, it is, of course, a yellow day. It’s a yellow day because we’re doing integrations—specifically Supabase—which you already had your first look at. Hopefully, you logged in, clicked around, set up your organization, completed some of the initial setup, and got a general sense of what we’re talking about.

But first, as usual, we have some recapping to do. I want to start with a quick refresher on RAG, because the next couple of days are all about building RAG ourselves—for real.

At its core, RAG is about making your model appear to be more knowledgeable. RAG stands for Retrieval-Augmented Generation, and it’s a technique that works extremely well. The simplest version of RAG is just taking some information and shoving it into the prompt. A user asks a question, and you provide the model with some information that might pertain to that question.

The clever part of RAG is how we figure out which information to include in the prompt. This is where vector embeddings come in. The idea is that there are certain types of LLMs that are very good at taking text and producing a set of numbers that represent the meaning of that text. These numbers can then be used to find similar information.

This gives us a way to search through a knowledge base for content that is likely to be useful in answering a particular question. At the heart of this idea is a type of LLM you may not even have known about: the embedding model, also known as an encoder or embedding LLM.

An embedding model takes text as input and outputs a list of numbers, called vectors or vector embeddings. These numbers represent the meaning of the input text. You can think of this as a point in space. If it were three numbers, it would be a point in three-dimensional space. If it’s a thousand numbers, it’s a point in a thousand-dimensional space.

The key idea is that text with similar meanings ends up close together in this space. That allows us to search for relevant contextual information in our knowledge base—information that has a similar meaning or pertains to the same topic as the user’s question.

You’ve seen this diagram before, and if you’ve taken my other courses, you’ve probably seen it many times. I know some of you are thinking, “Oh no, not this diagram again.” I’m sorry—I like this diagram.

The flow works like this: the user asks a question, and that question comes into our code or workflow. We then generate the vector embedding for the question. For example, if the question is “How much does it cost to fly to Heathrow?”, we convert that question into a vector.

Next, we look at our database, which already contains vectors for all of our stored information. We retrieve the pieces of information whose vectors are closest to the vector for the question. If the question is about flying to Heathrow, then content about ticket prices to London should ideally be retrieved.

Importantly, we don’t retrieve the vectors themselves—we retrieve the original text associated with them. That text is what we place into the prompt. We say to the LLM: “The user asked this question. Here is some relevant context.” That context might include ticket prices to London.

The LLM is then able to connect the dots. Given the question and the relevant context, it predicts the most likely next tokens, which form the answer. That is RAG.

When I do these recaps, I like to add a little extra detail to spice things up. One challenge that often comes up when building vector databases is deciding how to represent documents. Should you create one vector for an entire document? Or should you break it up?

If a document contains multiple topics—say, ticket prices to different destinations—it often makes sense to break it into smaller pieces. Each paragraph or section can become a mini-document with its own vector. That way, only the most relevant chunks get retrieved.

This process is called chunking. You’re turning a document into chunks. There’s an entire cottage industry around chunking strategies, and people get very passionate about what works best. You’ll hear lots of rules about how you should or shouldn’t chunk.

At the end of the day, there’s only one true principle: you have to test it. You need to try different strategies and see what works best for your data and your questions. In my AI engineering courses, we go deep into this—building testing methodologies and using metrics to evaluate chunking strategies—but that’s more advanced material.

For what we’re doing here, we’ll use default chunking approaches, which often work very well.

Another thing we covered yesterday was RAG versus agentic RAG. Traditional RAG works like this: the user asks a question, we perform vector-based retrieval, we fetch relevant content, and we pass that content to the LLM to generate an answer.

Agentic RAG takes this a step further. The user asks a question to an agent, and that agent is powered by an LLM that manages the entire workflow. The LLM doesn’t just answer the question—it decides how to answer it.

The agent has tools, including vector-based retrieval tools, and it can choose when and how to use them. It may adjust parameters, try multiple retrieval strategies, or even re-run retrieval if the initial context isn’t good enough. This iterative, decision-making process is what makes it agentic.

You may hear people say that RAG is dead. Usually, they say this for one of two reasons. The first is that context windows are getting larger and larger, allowing you to pass huge amounts of information into a model. Some people argue that you could just put your entire knowledge base into the context window and skip RAG altogether.

I think that’s a red herring. In real-world systems, companies often have gigabytes of documents—far more than any context window can handle. Even if you could fit it all, it would be wasteful to include irrelevant information. If the question is about ticket prices to London, there’s no benefit in also including ticket prices for every other city in the world.

The second reason people say RAG is dead is because of agentic RAG. Some argue that traditional RAG will be replaced entirely by agentic RAG. I think that’s probably true—but it’s really just a terminology issue. Agentic RAG is the natural evolution of RAG, not its replacement.

So I say: long live agentic RAG.

There are two distinct phases to building a RAG system. The first is data ingestion. This involves pulling information from source systems—text files, spreadsheets, PDFs—and loading it into a knowledge base or vector store.

You typically start by extracting the data from its source. Then you transform it, often cleaning it up or adding metadata that can be used later for filtering or organization. After that, you perform RAG-specific steps: chunking the data and vectorizing it.

Vectorizing—also called embedding or encoding—means passing the chunks through an embedding model to generate vectors. These vectors, along with the original text and metadata, are then loaded into the vector store.

This process resembles traditional ETL (Extract, Transform, Load) pipelines, with the addition of chunking and vectorization.

The second phase is the RAG pipeline itself—the question-answering flow. A user asks a question, which goes to the agent. The agent vectorizes the question, searches the vector store for similar chunks, retrieves the relevant text, and then passes that context to the LLM.

The LLM uses that context to generate the final answer. That completes the loop, and together, these two phases form a complete RAG system.

# **P) Day 4 - How to Use n8n and Supabase Integration for AI-Powered RAG System**

And with that, it gives me enormous pleasure to introduce you to this week’s business challenge.

This week is all about accelerating your business, or your client’s business. This is such a classic use case—one of those things you can immediately go out and implement for clients in the real world.

The setup is that we need to build an expert that is more than just a chatbot. This has to be something capable of answering detailed, in-depth questions about a client’s business. In this scenario, the client has a large amount of information about a lot of products stored in a big Google Sheet—a huge document—and they want something that can quickly answer questions related to any of that information.

We’ll be working with a fairly big dataset—not massive, but large enough to matter. The idea here is scalability. This solution should work whether you’re dealing with enterprise-level data or a startup’s internal data. It needs to be able to handle huge quantities of information. This is a classic RAG business setup.

For our solution, as I mentioned yesterday, we’re going to be using a managed database provider called Supabase. There are easier ways we could do this. There are built-in databases and nodes that make life much easier, but I’ve intentionally made things a little harder by choosing Supabase.

I did that for a reason. Supabase is very popular, it’s a great third-party database, and it’s something many people use in production. By giving you the skills to integrate with Supabase, it’s a big tick in your integrations list. You’ll have worked with Supabase, set up credentials, and gained experience you can reuse and extend for many other purposes.

Because of that, I felt it was an important infrastructure step. That’s why I picked a slightly harder challenge and decided we would integrate with a Postgres database on Supabase.

There is one downside. At one point, we’ll need to run some code to set up the database. I’ll give you that code, and you’ll execute it in Supabase. You would be completely justified in thinking, “Wait, this is supposed to be a low-code / no-code course. How would I do this myself?”

I’ll talk you through that when we get there. I’m not going to walk through the code line by line, but I will explain how you could generate this yourself, how easy it is, and how you’d handle issues if something goes wrong. That’s just one small obstacle we need to get through.

Apart from that, Supabase is going to be fantastic. It’s very scalable and enterprise-grade, though when I say “heavyweight,” I mean that in the enterprise sense. In practice, it’s actually a lightweight and easy-to-use platform. I think you’re really going to like it.

Today is all about data ingest.

On one hand, I’m going to dive a bit deeper and help you build some core skills. We’ll be doing data transformation using a node called Edit Fields, which used to be called Set. This is such a crucial node, and mastering it is an important foundational skill.

On the other hand, I’m also taking a bit of a shortcut. The source of data we’re going to use is a Google Sheet. We’ve done this a few times already, and you might be thinking, “That’s a bit boring.” I thought the same thing initially.

Originally, I thought it would be really cool if we used a Google Drive folder, where every time a document was dropped in, it would automatically kick off our data ingest pipeline. And that would be cooler. However, there’s a bit of overhead involved—especially around credentials and OAuth 2—which would become a big distraction right now.

We are going to do that next week. We’ll set it up properly so you can absolutely come back and upgrade this workflow. You’ll be able to move from updating a Google Sheet to simply dropping a document into a Google Drive folder and having everything trigger automatically. That will be very cool.

But for now, it would be too much of a detour. It would turn into a one-hour sidebar just to get authentication working, and I’d rather we stay focused on RAG and get a complete system built.

So the shortcut is this: our source data will be a Google Sheet, which we already know how to integrate with. That part will be easy. We’ll spend most of our time focusing on the agentic RAG build-out.

Tomorrow, we’ll add the question-answering part. Yesterday we did data ingest, tomorrow we add QA, and that completes a fully agentic RAG commercial build.

We’re also definitely going to add a voice agent tomorrow. At that point, I think it will really click why I’ve been talking about building business logic in n8n and then using ElevenLabs as the voice agent that drives the workflow. We’re following that second integration approach, where n8n handles the logic and ElevenLabs acts as the voice interface.

Alright—enough talk. Time to do.

Let’s set up our data ingest.

Let me start by introducing you to the data.

This is a Google Sheet that your client has shared with you in this example. It’s a sheet full of products sold in their online store, which specializes in computer accessories. You can see product names, categories, SKUs, prices, and descriptions.

There are quite a few entries. Not a huge number—there are 60 rows—because I don’t want this example to be expensive or overwhelming. But the key point is scalability. This could just as easily be 60,000 rows. Everything we’re building will scale to that level and beyond.

If you want to generate more data or use your own dataset, I would strongly encourage that.

We’re using Google Sheets here because it’s something we’ve already integrated with, making it easy to pull in the data. A more advanced challenge would be to solve the next step—having any document dropped into a shared Google Drive trigger ingestion automatically. That would be great, and you’re welcome to try it later.

But this scenario is very realistic. Clients really do hand over spreadsheets like this and say, “Here’s our data.” So this is what we’re working with, and we’ll get used to it because we’ll be reading it, analyzing it, and transforming it.

Let’s get started.

We go over to n8n and sign in. We enter our workspace and land on the home screen. From there, we create a brand-new workflow.

The first question is: what triggers this workflow?

There are lots of interesting options—when a sheet changes, when a file is dropped into Google Drive—but I’m keeping it simple and triggering it manually. We’ll add more interesting triggers later. For now, the focus is on RAG and the data pipeline.

This workflow will load data from the Google Sheet and put it into Supabase as a vector store.

At this point, I’m rolling up my sleeves because we’re about to get deep into some data—and that’s something I always enjoy, and I think you should too.

We start by adding a Google Sheets node. We want to get rows from the sheet. I’ll configure it to pull in all the rows from the product sheet.

I select the spreadsheet, choose the only sheet—Sheet1—and that’s it.

Now we execute the workflow. With a manual trigger, we just click the Execute Workflow button and let it run.

Once it runs, we can double-click the node and see the output. You’ll see all 60 items loaded in. For example, one row might be the “NovaKey Tactile Keyboard,” complete with its category, SKU, price, and description.

Great. Our data is now loaded.

This is the Extract stage of ETL—Extract, Transform, Load. The next step is Transform.

We now want to massage this data into a format that makes the most sense for our knowledge base.

To do that, we’ll use a node that used to be called Set and is now called Edit Fields. You’ll even see “Set” in brackets because that’s what it used to be named.

On the left, we have the incoming data. On the right, we define what we want the transformed output to look like. This mapping step is where we shape the data into the best possible format for our RAG system.

And that’s what we’re going to do next.

# **Q) Day 4 - Building RAG Pipeline with n8n and Supabase Vector Store Setup**

So here we are in the Edit Fields node.

This is such a core node. This node used to be called the Set node, and this is really where a data engineer lives. This node is all about mapping data from one format to another. It has nothing directly to do with AI—although of course, working with data is fundamental to AI—but it is absolutely crucial for this kind of data engineering work.

You can see that there are some clever, advanced ways to do this using JSON, but we’re going to do manual mapping, which is by far the most common approach. What we’re going to do here is create a dataset with two fields.

The first field will be called content. This is going to contain text that is best suited for our knowledge base—text that can be vectorized and used effectively by a language model. The second field will be called category, which will tell us what kind of thing this record represents.

The reason we’re adding category is because it’s useful metadata. Even though we may not make heavy use of it right now, it’s a great practice to tag data like this. Later, it could be used for filtering or refining results in different ways.

So I press Add Field, and this lets me define new fields. As mentioned, we’re going to add two fields: one called content, and another called category. Every single record coming in from the left-hand side will end up with these two fields built on the right-hand side.

Let’s start with content.

Here, we’re defining what should go into the content field. We also need to choose its type. Is it a string, a number, a boolean, an array, an object, or something else like a file? In our case, it’s going to be a string.

What goes into this string is entirely up to us. This text will eventually be sent to an LLM, so we want it to be as useful and descriptive as possible. We’re essentially turning structured product data into a natural-language description that provides good context.

So we might start with something like:
Product name:

But we don’t want this to be fixed text—we want it to be an expression. So after “Product name:” we insert the product name dynamically using an expression:
$json.name

You can see in the result preview below that it evaluates correctly and shows the product name, such as the keyboard.

Next, we add a new line and write Category:. For this, we insert the category using $json.Category, with a capital “C,” because that’s how it appears in the incoming data.

Then we continue. Why not include everything? We add the SKU, using $json.SKU, and then the Price, using $json.Price.

If you look at the result preview, you’ll notice that the price comes through exactly as it appears in the table, without a dollar sign. Since this text is meant for an LLM, it might actually be helpful to include a dollar sign so the model understands this is a monetary value. So we manually add the dollar sign before the price.

Finally—and most importantly—we add the Description. We make sure it’s spelled correctly, and we pull it directly from the input using $json.Description.

At this point, we have a field called content, and you can see exactly what it evaluates to in the preview: a nicely formatted block of text containing the product name, category, SKU, price, and description.

Now, if you’re used to very strict data engineering practices, this might feel a bit janky. We’re essentially crafting a descriptive text block by hand. But this is where data engineering meets data science and AI.

The goal here isn’t just clean structure—it’s usefulness for an LLM. We’re trying to create a string that provides rich, relevant context so that when a question is asked about this product, the model has everything it needs to respond accurately.

There’s no single right or wrong way to do this. You should experiment. Try different formats, different phrasing, and see what produces the best results. Trial and error is the best way to get good outcomes. That said, this is a perfectly reasonable starting point.

Next, we move on to the category field.

This one is simple. We want it to contain the category value from the original data. Again, the field is called Category with a capital “C” in the input.

We click on Expression and enter:
$json.Category

You could drag and drop it from the input panel, but by now we’re familiar enough to type it directly. The preview shows “Keyboard,” which means it’s working correctly.

At this point, we’ve successfully mapped the incoming data to a new structure with two fields: content and category. Content is rich, descriptive text meant for the LLM, and category is useful metadata indicating what type of product this is.

Now let’s test it.

We click Execute Step, and immediately on the right-hand side we see the result of the mapping. The output is clean, readable, and perfectly structured for our use case. This is exactly what we’ll provide to the LLM as context, along with the extra category metadata.

Excellent. We’ve just completed a data mapping step.

If we now go back to the main workflow view and click Execute Workflow, the same transformation is applied to all rows. All 60 records are mapped into this new format. This is the beginning of our data pipeline.

At this point, I’d like you to go to Supabase, at supabase.com.

If you haven’t already, please create an account. You’ll need to create an organization. As I mentioned yesterday, mine is called Donna Research. Feel free to call yours whatever you like—just don’t call it Donna Research.

Once you’ve set that up, you should land on the Projects page. You won’t have any projects yet. If you don’t see this page, click on Projects in the left navigation or click on your organization name to get there.

From here, click New Project.

Let’s call the project rag. The compute size can be tiny. You should be able to stay on the free plan. If you’re on a paid plan, this will cost about a cent an hour, and you can always delete the project once we’re done.

You’ll then need to choose a database password and select a region. Pick the region closest to you. Normally I’d choose the Americas, but I’m in the UK today, so Europe would make sense—though I might still choose the Americas to stay closer to where my n8n instance is running.

Once you’re ready, click Create New Project.

I’ve entered a strong password and stored it securely in a password manager, as you should. After clicking Create New Project, Supabase lets me know about potential costs, but this should be free for you on a trial account.

Now the project is being set up. It’s called rag, and it will take a few minutes to provision everything, including your API endpoints—terminology you’re already very familiar with.

Once that’s done, we’ll be the proud owners of our first Supabase project.

# **R) Day 4 - How to Set Up Supabase Vector Database for RAG with n8n**

I came back, and it’s done. It takes a few minutes for Supabase to finish setting everything up.

Now let’s take a moment to look around the navigation inside our project. If you ever get lost, the way to get back here is simple: go to your Projects page, click on rag, and you’ll land right back here again.

Looking at the navigation on the left, we can see several sections. There are Tables, there’s SQL, which you probably already know is how you query and interact with your database, and then a bunch of other options.

If we click into the database tables, we’ll see that there are currently no tables at all. This is an empty database, which is exactly what we expect at this stage.

So we go back to the project overview. Here we are again.

Now this is the one point where I told you we’d have to do something that’s a little bit “code-y.” For those of you who are engineers, this will feel completely routine. For those of you who aren’t, this might feel like, “Wow, what is all of this?” But I want to reassure you: this is very cookie-cutter, very standard stuff.

Yes, we do need to run some code at this point. I’ll give you that code in the course resources, and you’ll simply paste it in and run it. This is a very pedestrian activity. In fact, you could ask ChatGPT to write this for you with a short prompt, and it would do it instantly.

And here’s a little secret, just between you and me: I actually had ChatGPT generate this SQL for me. I hate writing SQL because I always make mistakes, so I didn’t even bother doing it by hand. I just told ChatGPT exactly what I needed, and it produced this for me right away.

You can use ChatGPT, Claude, Cursor, or whatever tool you like. The important thing is that when you’re ready, you need to go to the SQL Editor. This is where you can paste in SQL code and press the Run button to execute it.

The reason we’re doing this is because the default tables that come with Supabase are not set up in exactly the way that n8n expects for vector-based retrieval. There isn’t an out-of-the-box button that says, “Create the exact tables that n8n wants,” so we need to create them ourselves.

So we paste in this SQL script, which you can copy directly from the course resources.

At a high level, here’s what this SQL does. It creates a table called knowledgebase, which is where our data will live. Each row in this table will have an id, content, metadata, and an embedding vector. The content is the text we just created in the Edit Fields node. The metadata includes things like category. The embedding is the vector representation of that content.

There’s also one slightly janky thing here, which I’ll explain in a moment.

In addition to the table, this SQL creates a database function called match_documents. This function is what n8n will call when it wants to retrieve relevant context from the vector database. In other database setups, this function might be created automatically for you, but because we’re using Supabase directly, we need to define it ourselves.

The function returns the results in exactly the format that n8n expects. There’s also a small adjustment inside the function—specifically taking one minus a similarity calculation—because Postgres returns similarity scores slightly differently than what n8n expects. This detail isn’t something you need to memorize or deeply understand right now.

If you wanted to generate this yourself, you could simply ask ChatGPT something like: “Can you generate Postgres SQL for Supabase that creates a knowledge base table with content, metadata, and embeddings in the format that n8n expects?” That’s exactly how this was created.

Now let me explain the one janky thing I mentioned earlier.

People who are new to RAG sometimes think that all the magic happens inside the vector database itself. While the vectors are important, the real magic happens in the embedding model. That’s the model that takes text and turns it into a vector.

Postgres and Supabase do not create embeddings. They only store them and perform similarity searches on them. The embedding itself is generated outside the database—in our case, inside n8n using an embedding model.

Because of that, the database needs to know how many dimensions each embedding vector will have. Different embedding models produce vectors of different sizes. Some might produce just a few numbers, while others produce thousands.

So when we create the table, we must specify the number of dimensions in advance.

In our case, we’re going to use a very standard, popular choice: OpenAI’s text-embedding-small model. This model produces vectors with 1,536 dimensions. That number—1536—must appear in two places in the SQL script.

And yes, this is a bit janky. If you accidentally put 1537 or 1535, you’ll get a confusing error later about mismatched dimensions. That’s why this number has to be exactly right.

OpenAI’s embedding model does have a cost, but it’s extremely small. For 60 documents, it’s effectively negligible. Even for thousands of documents, it’s still very inexpensive.

That said, you’re not locked into OpenAI. Gemini has a great embedding model, and there are excellent open-source embedding models as well. If you choose a different model, the only thing you need to do is rerun this SQL with the correct number of dimensions for that model.

You can rerun this script as many times as you like. It recreates the table if needed. Just make sure that the final version matches the embedding model you’re actually using. If you stick with OpenAI, then 1536 is the correct number.

I know this feels a bit like voodoo, but it’s simply a requirement of how vector databases work. The database needs to know the vector size ahead of time.

Alright, that was a long explanation. Now it’s time to actually run this.

But wait—there’s one more thing we need to do first.

Before running the SQL script, we need to enable vector support inside Supabase. To do that, go to the Database section on the left, then click on Extensions.

You’ll see a list of available extensions. Search for vector. When it appears, enable it.

Supabase will think for a moment, and then it will confirm that the vector extension is enabled.

Now we can go back to the SQL Editor.

You might see an arrow or warning there if you tried to run the script earlier. That’s fine. Now we press Run.

The query runs, and we see “Success. No rows returned.” That’s exactly what we want—we weren’t trying to retrieve data, just create structures.

To confirm everything worked, we go back to the Database section. A moment ago, it said there were no tables. Now we see it: the knowledgebase table.

It has an id, content, metadata, and embedding field. This is exactly what we need. This is where our vectorized data will live.

So we’ve done it. We’ve successfully set up a Supabase vector database.

Now it’s time to head back to n8n, hook everything together, and perform the load step of ETL by populating this database with our data.

# **S) Day 4 - Supabase Vector Store Integration with n8n Using OpenAI Embeddings**

Okay, it’s time to connect n8n to Supabase. This is the big integration for today.

First, here we are in Supabase, looking at our database table. On the left, you can see the knowledgebase table listed. You’ll also notice a database side navigation with a Settings option there—but that is not the one we want to use right now.

Instead, go further left to the main project navigation. Make sure you’re looking at the Project Overview screen. From there, you’ll see a list of sections. What you want to click on is Project Settings.

This is an easy place to get confused, because Supabase has several places called “settings.” The one you want right now is specifically Project Settings, not database settings or table settings.

Once you’re inside Project Settings, look at the left-hand menu and click on Data API. That’s the section we need.

The first thing you’ll see on the Data API page is a URL. This URL is the endpoint for your Supabase database. You need to copy this URL and put it somewhere safe, because we’re about to use it in n8n.

Be careful when you copy it. Don’t paste it into a tool that might reformat or alter characters. I know you’re already good at this, but it’s worth saying—this URL must remain exactly as it is.

This URL is the first credential we’ll need.

Next, look a little further down the page at API keys. Supabase recently introduced a newer API key system using publishable and secret keys, but n8n does not yet support that newer approach.

Instead, n8n uses the legacy API keys, which are still extremely common and widely used.

So on the API Keys section, switch over to the Legacy keys tab. You’ll see a key there with a little orange “secret” label next to it. This is the key we need.

Click Reveal, then copy that key. Keep it safe. Just like before, you can leave this Supabase tab open and switch back and forth between tabs to copy and paste directly. That’s often the safest approach so you’re not storing keys anywhere unnecessarily.

Alright, now let’s go do the integration.

We switch back to n8n.

Click the plus (+) button to add a new node. The node we’re going to add is called Supabase Vector Store. Be careful here—there is also a normal Supabase node, but that’s for standard databases. We want the vector store version because we’re working with embeddings.

Once the node is added, look at the available operations. The one we want is Add documents to vector store. Select that.

Now it’s time to set up credentials—everyone’s favorite part.

Click Create new credential.

The first field is Host. This is where you paste the Supabase URL you copied earlier from the Data API section.

Next is the Service Role Secret. This is where you paste the legacy API key you revealed and copied from Supabase.

If you’ve still got Supabase open in another tab, now’s a great time to switch back, copy it again, and paste it directly so you know it’s exact.

Once both fields are filled in, click Save.

And now… drumroll please.

Green. Green is what we want. If you see green, the connection worked.

If you don’t see green, don’t panic. This is usually just a copy-paste issue. Go back, check the URL, check the key, paste them again carefully. Be the API-key pro that I know you are.

Once you’ve got green, congratulations—you now have a working Supabase vector store connection in n8n.

Now, this node needs two things in order to do its job.

The first one is probably exactly what you’re expecting: an embedding model. As I mentioned earlier, the smart part of RAG does not live in Supabase. Supabase only stores vectors. The embedding model is what actually converts text into vectors.

So here, we add an Embedding node.

We’re going to use OpenAI Embeddings. You’ll see that there are several embedding providers available—some free, some paid, some from Gemini, some open source.

If you want to avoid cost entirely, you can absolutely explore those options. Many Gemini models are free within limits, and open-source embeddings are completely free if you know how to run them.

However, if you choose to follow along with me, we’ll use OpenAI text-embedding-3-small, which is extremely common and very inexpensive. OpenAI does require a small upfront balance, but the per-use cost is tiny—especially for something like 60 documents.

This model produces embeddings with 1,536 dimensions, which is exactly the number we specified earlier when we created the Supabase table. This alignment is critical.

OpenAI also offers text-embedding-3-large, which produces around 3,000 dimensions and is more powerful but slightly more expensive. We don’t need that here.

So we select text-embedding-3-small.

If you ever choose a different embedding model, just remember: you must go back to Supabase and rerun the SQL script with the correct number of dimensions so everything matches.

Next, we need to configure the second required piece: the Document Loader.

This is the component that tells n8n how to take incoming data, chunk it if necessary, and load it into Supabase.

We’ll use the default document loader, which is perfectly fine. This is where we describe how our data should be passed into the vector store.

By default, it’s set to Load all input data, which technically works—but it’s not ideal. That would push a lot of unnecessary fields into Supabase.

Instead, we switch it to Load specific data.

Now we tell it exactly what data we want to load. And the answer is simple: we want the content field that we carefully constructed earlier.

So we add an expression and set it to:

{{$json.content}}

You already know this syntax by now. You could drag and drop, but we’re being pros.

This ensures that only the meaningful text goes into the knowledge base, not all the surrounding fields.

Next, we configure text splitting.

We can leave this at Simple, which is the default. This means it will try to split text every 1,000 characters with a small overlap.

In our case, none of the product descriptions are anywhere near 1,000 characters, so this effectively means one chunk per product, which is exactly what we want.

If you were dealing with long documents, this same configuration would automatically break them into sensible chunks. This is a great default and works very well in practice.

Finally, we move to Options.

There’s only one option we can add here, and that’s Metadata.

Metadata lets us attach extra information to each chunk—information that might be useful later for filtering, reasoning, or giving the agent more context.

We click Add property.

The name of the property will be category.

The value should be an expression, and of course, it comes from the input:

{{$json.category}}

At this point, n8n practically auto-completes it for us because it knows what we’re doing.

This means that when something like “Keyboard” is loaded into the knowledge base, it will be tagged in metadata as category = Keyboard.

We could add more metadata fields if we wanted—SKU, price, or anything else—but this is perfect for now.

And that’s it.

We’ve now fully configured the Supabase vector store node with:

Proper credentials

An embedding model

A document loader

Chunking

Metadata

This is exactly what we want, and we’re ready to load our data into the vector database.

# **T) Day 4 - Build a RAG Pipeline Using n8n and Supabase Vector Database**

Okay, now we have our full workflow, so let’s tidy things up.

You’ll notice there’s a red box on one of the nodes saying “Parameter table name is required.” That’s a fair complaint—we haven’t told Supabase which table to write to yet. So let’s fix that.

We head back to the Supabase Vector Store node. It’s asking which table name we want to use. If you right-click and look at the available tables, you’ll see Knowledge Base, which is the table we created earlier using our SQL script. That’s exactly the one we want. Once we select that, the error disappears.

Now we have a complete workflow that’s ready for data ingest. Let’s rename it to something meaningful—Data Ingest—and get ready to run it.

Before we do that, let’s quickly jump back to Supabase and confirm the current state of the database. On the left, go to Database, load the tables, and open Knowledge Base. As expected, it’s empty. There’s no data in it yet.

Back in our workflow, we’ll hide the logs for a cleaner view and press Execute Workflow.

Here we go.

You can see it loading the 60 items, embedding them, and writing them to Supabase. The workflow completes successfully, and the count shows 60 documents loaded. Each row has been vectorized and stored. Everything is green.

What might have taken days—or even a week—to wire together manually has been done in minutes using n8n and Supabase.

But let’s verify it.

Back in Supabase, the table initially looks empty, but that’s just because it hasn’t refreshed yet. Once it refreshes, there it is—the Knowledge Base table populated with 60 rows.

Each entry has:

an id

content (the text)

metadata

and embedding, which is the vector

If we open one row, we can see the content text—exactly what we want to feed into an LLM during retrieval. In the metadata, we can see the category field we added, along with some automatically generated metadata. And the embedding column contains the 1,536-dimensional vector produced by the OpenAI embedding model.

At this point, I realized I made a small typo.

Some of the content includes things like “$1.59 .99” or descriptions starting with a stray dollar sign, like “$vertical design” or “$compact.” That’s clearly not intentional—but this actually gives us a great opportunity to demonstrate how easy it is to fix and rerun the pipeline.

First, we’ll delete the existing data. In Supabase, select the table and delete all 60 rows. Yes, it can’t be undone—but that’s fine.

Now we go back to our workflow, open the field mapping node, and there it is—the extra dollar sign in the description field. We remove it, save the node, and return to the workflow.

You’ll notice a yellow warning triangle on the node, indicating that it has changed since the last execution. That’s expected.

We run the workflow again.

Once again, all 60 items are embedded and stored successfully. Back in Supabase, the table refreshes automatically, and when we open a row, the typo is gone. Clean data, correct content, everything exactly as we want it.

This is a great example of how powerful and forgiving these pipelines are. Small mistakes are easy to fix, and rerunning the entire ingest process is trivial.

So let’s recap what we’ve built.

We talked about the two major phases of a RAG system:

Data ingest (what we did today)

Question answering (what we’ll do tomorrow)

In the ingest phase:

Data Source: We pulled product data from a Google Sheet.

Extract: An n8n node fetched that data.

Transform: A field mapping node created structured content and categories.

Chunking & Vectorization: Handled by the document loader and embedding model.

Load: Data was stored in Supabase as vectors in a Postgres database.

You may have noticed that several of these steps were grouped into node clusters. The AI-related logic is one cluster, and the Supabase integration is another. That modularity is intentional and very powerful.

Right now, this workflow is triggered manually by pressing a button. But you could just as easily:

run it on a schedule

trigger it when the spreadsheet changes

or connect it to another event

And that’s a wrap for data ingest—connecting to Supabase and working with vectors.

Tomorrow is where everything really comes together. We’ll complete the project for Week Two, focusing on accelerating development with voice agents and RAG.

You’ve now reached the 60% mark. Tomorrow, we close out Week Two.

You definitely don’t want to miss it.
